<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tool monkey]]></title>
  <link href="http://erikerlandson.github.com/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2013-01-02T09:16:04-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Mean of the Modulus Does Not Equal the Modulus of the Mean]]></title>
    <link href="http://erikerlandson.github.com/blog/2013/01/02/the-mean-of-the-modulus-does-not-equal-the-modulus-of-the-mean/"/>
    <updated>2013-01-02T08:55:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2013/01/02/the-mean-of-the-modulus-does-not-equal-the-modulus-of-the-mean</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been considering models for the effects of HTCondor negotiation cycle cadence on pool loading and accounting group starvation, which led me to thinking about the effects of taking the modulus of a random variable, for reasons I plan to discuss in future posts.</p>

<p>When you take the modulus of a random variable, X, the corresponding expected value E[X mod m] is not equal to E[X] mod m.  Consider the following example:</p>

<p><img src="http://erikerlandson.github.com/assets/images/rv_modulus_mean.png" title="An example demonstrating that E[X mod m] != E[X] mod m" alt="Random Variable Images" /></p>

<p>As we see from the example above, the random variables X and Y have the same mean:  E[X] = E[Y] = 0.75, however E[X mod 1] = 0.75 while E[Y mod 1] = 0.5.  One implication is that computing the moments of the modulus of random variables must be on a per-distribution basis, perhaps via monte carlo methods.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Demonstration of Negotiator-Side Resource Consumption]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/12/03/a-demonstration-of-negotiator-side-resource-consumption/"/>
    <updated>2012-12-03T08:25:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/12/03/a-demonstration-of-negotiator-side-resource-consumption</id>
    <content type="html"><![CDATA[<p>HTCondor supports a notion of aggregate compute resources known as partitionable slots (p-slots), which may be consumed by multiple jobs.   Historically, at most one job could be matched against such a slot in a single negotiation cycle, which limited the rate at which partitionable slot resources could be utilized.  More recently, the scheduler has been enhanced with logic to allow it to acquire multiple claims against a partitionable slot, which increases the p-slot utilization rate. However, as this potentially bypasses the negotiator&#8217;s accounting of global pool resources such as accounting group quotas and concurrency limits, it places some contraints on what jobs can can safely acquire multiple claims against any particular p-slot: for example, only other jobs on the same scheduler can be considered.  Additionally, candidate job requirements must match the requirements of the job that originally matched in the negotiator.  Another significant impact is that the negotiator is still forced to match an entire p-slot, which may have a large match cost (weight): these large match costs cause <a href="https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3013">accounting difficulties</a> when submitter shares and/or group quotas drop below the cost of a slot.  This particular problem is growing steadily larger, as machines with ever-larger numbers of cores and other resources appear in HTCondor pools.</p>

<p>An alternative approach to scheduler-side resource consumption is to enhance the negotiator with the ability to match multiple jobs against a resource (p-slot) &#8211; negotiator-side resource consumption.   The advantages of negotiator-side consumption are that it places fewer limitations on what jobs can consume a given resource.  The negotiator already handles global resource accounting, and so jobs are not required to adhere to the same requirements expression to safely consume assets from the same resource.  Furthermore, jobs from any scheduler may be considered.  Each match is only charged the cost of resources consumed, and so p-slots with large amounts of resources do not cause difficulties with large match costs.   Another considerable benefit of this approach is that it facilitates the support of <a href="http://spinningmatt.wordpress.com/2012/11/13/no-longer-thinking-in-slots-thinking-in-aggregate-resources-and-consumption-policies/">configurable resource consumption policies</a></p>

<p>I have developed a working draft of negotiator-side resource consumption on my HTCondor github fork, topic branch <a href="https://github.com/erikerlandson/htcondor/tree/V7_9-prototype-negside-pslot-splits">V7_9-prototype-negside-pslot-splits</a> which also implements support for configurable resource consumption policies.   I will briefly demonstrate this implementation and some of its advantages below.</p>

<p>First I will demonstrate an example with a consumption policy that is essentially equivalent to HTCondor&#8217;s current default policies.  Consider this configuration:</p>

<pre><code># spoof some cores
NUM_CPUS = 10

# configure an aggregate resource (p-slot) to consume
SLOT_TYPE_1 = 100%
SLOT_TYPE_1_PARTITIONABLE = True
# declare multiple claims for negotiator to use
# may also use global: NUM_CLAIMS
SLOT_TYPE_1_NUM_CLAIMS = 20
NUM_SLOTS_TYPE_1 = 1

# turn off schedd-side resource splitting since we're demonstrating neg-side alternative
CLAIM_PARTITIONABLE_LEFTOVERS = False

# turn this off to demonstrate that consumption policy will handle this kind of logic
MUST_MODIFY_REQUEST_EXPRS = False

# configure a consumption policy.   This policy is modeled on
# current 'modify-request-exprs' defaults:
# "my" is resource ad, "target" is job ad
STARTD_EXPRS = ConsumptionCpus, ConsumptionMemory, ConsumptionDisk
ConsumptionCpus = quantize(target.RequestCpus, {1})
ConsumptionMemory = quantize(target.RequestMemory, {128})
ConsumptionDisk = quantize(target.RequestDisk, {1024})
# swap doesn't seem to be actually supported in resource accounting

# keep slot weights enabled for match costing
NEGOTIATOR_USE_SLOT_WEIGHTS = True

# weight used to derive match cost: W(before-consumption) - W(after-consumption)
SlotWeight = Cpus

# for simplicity, turn off preemption, caching, worklife
CLAIM_WORKLIFE=0
MAXJOBRETIREMENTTIME = 3600
PREEMPT = False
RANK = 0
PREEMPTION_REQUIREMENTS = False
NEGOTIATOR_CONSIDER_PREEMPTION = False
NEGOTIATOR_MATCHLIST_CACHING = False

# verbose logging
ALL_DEBUG = D_FULLDEBUG

# reduce daemon update latencies
NEGOTIATOR_INTERVAL = 30
SCHEDD_INTERVAL = 15
</code></pre>

<p>In the above configuration, we declare a typical aggregate (that is, partitionable) resource <code>SLOT_TYPE_1</code>, but then we also configure a <em>consumption policy</em>, by advertising <code>ConsumptionCpus</code>, <code>ConsumptionMemory</code> and <code>ConsumptionDisk</code>.  Note that these are defined with quantizing expressions currently used as default values for the <code>MODIFY_REQUEST_EXPRS</code> behavior.  The startd and the negotiatior will <em>both</em> use these expressions by examining the slot ads.</p>

<p>Next, we submit 15 jobs.  Note that this more than the 10 cores advertised by the p-slot:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 60
should_transfer_files = if_needed
when_to_transfer_output = on_exit
queue 15
</code></pre>

<p>If we watch the negotiator log, we will see that negotiator matches the 10 jobs supported by the p-slot on the next cycle (note that it uses slot1 each time):</p>

<pre><code>$ tail -f NegotiatorLog | grep -e '\-\-\-\-\-'  -e 'matched
12/03/12 11:53:10 ---------- Finished Negotiation Cycle ----------
12/03/12 11:53:25 ---------- Started Negotiation Cycle ----------
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25 ---------- Finished Negotiation Cycle ----------
</code></pre>

<p>You can use <code>condor_q</code> to verify that the 10 jobs subsequently run.   The jobs run against 10 dynamic slots (d-slots) in the standard way:</p>

<pre><code>$ ccdump condor_status Name TotalSlotCpus
slot1@rorschach | 10
slot1_10@rorschach | 1
slot1_1@rorschach | 1
slot1_2@rorschach | 1
slot1_3@rorschach | 1
slot1_4@rorschach | 1
slot1_5@rorschach | 1
slot1_6@rorschach | 1
slot1_7@rorschach | 1
slot1_8@rorschach | 1
slot1_9@rorschach | 1
</code></pre>

<p>Next we consider altering the resource consumption policy.  As a simple example, suppose we wish to allocate memory more coarsely.  We could alter the configuration above by changing <code>ConsumptionMemory</code> to:</p>

<pre><code>ConsumptionMemory = quantize(target.RequestMemory, {512})
</code></pre>

<p>Perhaps we then also want to express match cost in a memory-centric way, instead of the usual cpu-centric way:</p>

<pre><code>SlotWeight = floor(TotalSlotMemory / 512)
</code></pre>

<p>Here it is worth noting that in this implementation of negotiator-side consumption, the cost of a match is defined as W(S) - W(S&#8217;), where W(S) is the weight of the slot <em>prior</em> to consuming resources from the match and consumption policy, and W(S`) is the weight evaluated for the slot <em>after</em> those resources are subtracted.  This modification enables multiple matches to be made against a single p-slot, and furthermore it paves the way to possible avenues for a <a href="http://erikerlandson.github.com/blog/2012/11/26/rethinking-the-semantics-of-group-quotas-and-slot-weights-computing-claim-capacity-from-consumption-policy/">better unit analysis of slot weights and accounting groups</a>.</p>

<p>Continuing the example, if we re-run the example with this new consumption policy, we should see that memory limits reduce the number of jobs matched against <code>slot1</code> to 3:</p>

<pre><code>$ tail -f NegotiatorLog | grep -e '\-\-\-\-\-'  -e 'matched'
12/03/12 12:58:22 ---------- Finished Negotiation Cycle ----------
12/03/12 12:58:37 ---------- Started Negotiation Cycle ----------
12/03/12 12:58:37       Successfully matched with slot1@rorschach
12/03/12 12:58:37       Successfully matched with slot1@rorschach
12/03/12 12:58:37       Successfully matched with slot1@rorschach
12/03/12 12:58:37 ---------- Finished Negotiation Cycle ----------
</code></pre>

<p>Examining the slot memory assets, we see that there is insufficient memory for a fourth match when our consumption policy sets the minimum at 512:</p>

<pre><code>$ ccdump condor_status Name TotalSlotMemory
slot1@rorschach | 1903
slot1_1@rorschach | 512
slot1_2@rorschach | 512
slot1_3@rorschach | 512
</code></pre>

<p>As a final example, I&#8217;ll demonstrate the positive impact of negotiator side matching on interactions with accounting groups (or submitter shares).  Again returning to my original example, modify the configuration with a simple accounting group policy:</p>

<pre><code>GROUP_NAMES = a
GROUP_QUOTA_a = 1
GROUP_ACCEPT_SURPLUS = False
GROUP_AUTOREGROUP = False
</code></pre>

<p>Now submit 2 jobs against accounting group <code>a</code>:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 60
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="a.u"
queue 2
</code></pre>

<p>We see that accounting groups are respected: one job runs, and it does not suffer from insufficient share to acquire resources from <code>slot1</code> <a href="https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3013">(GT3013)</a>, because match cost is computed using only the individual job&#8217;s impact on slot weight, instead of being required to match the entire p-slot:</p>

<pre><code>$ tail -f ~/condor/local/log/NegotiatorLog | grep -e '\-\-\-\-\-' -e matched
12/03/12 14:57:50 ---------- Finished Negotiation Cycle ----------
12/03/12 14:58:08 ---------- Started Negotiation Cycle ----------
12/03/12 14:58:08       Successfully matched with slot1@rorschach
12/03/12 14:58:09 ---------- Finished Negotiation Cycle ----------

$ ccdump condor_status Name TotalSlotCpus
slot1@rorschach | 10
slot1_1@rorschach | 1
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Semantics of Group Quotas and Slot Weights: Computing Claim Capacity from Consumption Policy]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/26/rethinking-the-semantics-of-group-quotas-and-slot-weights-computing-claim-capacity-from-consumption-policy/"/>
    <updated>2012-11-26T13:52:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/26/rethinking-the-semantics-of-group-quotas-and-slot-weights-computing-claim-capacity-from-consumption-policy</id>
    <content type="html"><![CDATA[<p>In two previous posts, I made a case to motivate the need for a better definition of slot weights and group quotas that could accommodate use cases involving aggregate resources (partitionable slots) with heterogeneous consumption policies and also provide a principled unit analysis for weights and quotas.  These previous posts can be viewed here:</p>

<ul>
<li><a href="http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources/">Rethinking the Semantics of Group Quotas and Slot Weights for Heterogeneous and Multidimensional Compute Resources</a></li>
<li><a href="http://erikerlandson.github.com/blog/2012/11/15/rethinking-the-semantics-of-group-quotas-and-slot-weights-claim-capacity-model/">Rethinking the Semantics of Group Quotas and Slot Weights: Claim Capacity Model</a></li>
</ul>


<p>As previously mentioned, a Claim Capacity Model of accounting group quotas and slot weights (or &#8220;resource consumption costs&#8221;) requires a resource claiming model that assigns a well defined finite value for the maximum number of claims that each resource and its consumption policy can support.  It must also be re-evaluatable on a resource as its assets are consumed, so that the cost of a proposed claim (or match, in negotiation-speak) can be defined as W(R) - W(R&#8217;), were R&#8217; embodies the amounts of all assets remaining after the claim has taken its share.  (Here, I will be using the term &#8216;assets&#8217; to refer to quantities such as cpus, memory, disk, swap or any <a href="http://spinningmatt.wordpress.com/2012/11/19/extensible-machine-resources/">extensible resources</a> defined, to clarify the difference between an aggregate resource (i.e. a partitionable slot) versus a single resource dimension such as cpus, memory, etc).</p>

<p>This almost immediately raises the question of how best to define such a resource claiming model.  In this post I will briefly describe a few possible approaches, focusing on models which are easy reason about, easy to configure and additionally allow claim capacity for a resource - W(R) - to be computed automatically for the user, thus making a sane relationship between consumption policies and consumption costs possible to enforce.</p>

<h3>Approach 1: fixed claim consumption model</h3>

<p>The simplest-possible approach is arguably to just directly configure a fixed number, M, of claims attached to a resource.  In this model, each match of a job against a resource consumes one of the M claims.   Here, match cost W(R) - W(R&#8217;) = 1 in all cases, and is independent of the &#8216;size&#8217; of assets consumed from a resource.</p>

<p>A possible use case for such a model is that one might wish to declare that a resource can run up to a certain number of jobs, without assigning any particular cost to consuming individual assets.  If the pool users&#8217; workload consists of large numbers of resource-cheap jobs that can effectively share cpu, memory, etc, then such a model might be a good fit.</p>

<h3>Approach 2: configure asset quantization levels</h3>

<p>Another approach that makes the relation between consumption policy and claim capacity easy to think about is to configure a quantization level for each resource asset.  For example, here we might quantize memory into 20 levels, i.e. Q(memory) = 20.  Similarly we might define Q(cpus) = 10 (note that HTCondor does not currently handle fractional cpus on resources, but this kind of model would benefit if floating point asset fractions were supported).  At any time, a resource R has some number q(a) left of the original Q(a).  A job requests an amount r(a) for asset (a).   Here, a claim gets a quantized approximation of any requested asset = V(a)(n(a)/Q(a)), where V(a) is the total original value available for asset (a), and n(a) = ceiling(r(a)Q(a)/V(a)).   Here there are two possible sub-policies.  If we demand that each claim consume >= 1 quantum of every asset (i.e. n(a) >= 1), then the claim capacity W(R) is the minimum of q(a), for (a) over all assets.  However, if a claim is allowed to consume a zero quantity of some individual assets (n(a)=0), then the claim capacity is the <em>maximum</em> of the q(a).   In this case, one must address the corner case of a claim attempting to consume (n(a)=0) over all assets.  The resulting resource R&#8217; has q&#8217;(a) = q(a)-n(a), and W(R&#8217;) is the minium (or maximum) over the new q&#8217;(a).</p>

<h3>Approach 3: configure minimum asset charges</h3>

<p>A third approach is to configure a <em>minimum</em> amount of each asset that any claim must be charged.   For example, we might define a minimum amount of memory C(memory) to charge any claim.   If a job requests an amount r(a), it will always receive max(r(a), C(a)).  As above, q(a) is the number of quanta currently available for asset (a).  Let v(a) be the amount of (a) currently available.  Here we define q(a) for an asset (a) to be floor(v(a)/C(a)).   If we adhere to a reasonable restriction that C(a) must be strictly > 0 for all (a), we are guaranteed a well defined W(R) = min over the q(a).</p>

<p>It is an open question which of these models (or some other completely different options) should be supported.  Conceivably all of them could be provided as options.</p>

<p>Currently my personal preference leans toward Approach 3.  It is easy to reason about and configure.  It yields a well defined W(R) in all circumstances, with no corner cases, that is straightforward to compute and enforce automatically.  It is easy to configure heterogeneous consumption policies that cost different resource assets in different ways, simply by tuning minimum charge C(a) appropriately for each asset.  This includes claim capacity models where jobs are assumed to use very small amounts of any resource, including fractional shares of cpu assets.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Semantics of Group Quotas and Slot Weights: Claim Capacity Model]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/15/rethinking-the-semantics-of-group-quotas-and-slot-weights-claim-capacity-model/"/>
    <updated>2012-11-15T17:22:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/15/rethinking-the-semantics-of-group-quotas-and-slot-weights-claim-capacity-model</id>
    <content type="html"><![CDATA[<p>In my previous post about <a href="http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources">Rethinking the Semantics of Group Quotas and Slot Weights</a>, I proposed a concept for unifying the semantics of accounting group quotas and slot weights across arbitrary resource allocation strategies.</p>

<p>My initial terminology was that the weight of a slot (i.e. resource ad) is a measure of the <em>maximum</em> number of jobs that might match against that ad, given the currently available resource quantities and the allocation policy.  The cost of a match becomes the amount by which that measure is reduced, after the match&#8217;s resources are removed from the ad.</p>

<p>In the HTCondor vocabulary, a job acquires a <em>claim</em> on resources to actually run after it has been matched.  It has been proposed that it may be beneficial for HTCondor to evolve toward a model where there are (aggregate) resource ads, and claims against those ads, as a simplification of the current model which involves static, partitionable and dynamic slots, with claims.  With this in mind, a preferable terminology for group quota and weight semantics might be that a resource ad (or slot) has a measure of the maximum number of claims it could dispense: a <em>claim capacity</em> measure.  The cost of a claim (or match) is the corresponding reduction of the resource&#8217;s claim capacity.</p>

<p>So, this semantic model could be referred to as the Claim Capacity Model of group quotas and slot weights.  With this terminology, the shared &#8216;unit&#8217; for group quotas and slot weights would be <em>claims</em> instead of <em>jobs</em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Semantics of Group Quotas and Slot Weights for Heterogeneous and Multidimensional Compute Resources]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources/"/>
    <updated>2012-11-13T15:31:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources</id>
    <content type="html"><![CDATA[<p>The HTCondor semantic for accounting group quotas and slot weights is currently cpu-centric.  This is an artifact of the historic primacy of cpu as the most commonly-considered limiting resource in computations.  For example the <code>SlotWeight</code> attribute is currently defaulted to <code>Cpus</code>, and when slot weights are disabled, there is logic activated in matchmaking to sum the available cpus on slots to avoid &#8216;undercounting&#8217; total pool quotas.</p>

<p>However, HTCondor slots &#8211; the core model of computational resources in an HTCondor pool &#8211; manage four resources by default: cpu, memory, disk and swap.  Furthermore, slots may now be configured with arbitrary custom resources.  As recently mentioned by <a href="http://spinningmatt.wordpress.com/2012/11/13/no-longer-thinking-in-slots-thinking-in-aggregate-resources-and-consumption-policies">Matthew Farrellee</a>, there is a growing pressure to provide robust support not just for traditional cpu-centric resource presentation, usage and allocation, but also seamlessly mediated with memory-centric, gpu-centric or &#8216;*&#8217;-centric resource allocation policies and more generally allocation policies that are simultaneously aware of all resource dimensions.</p>

<p>This goal immediately raises some questions for the semantics of accounting groups and slot weights when matching jobs against slots during matchmaking.</p>

<p>Consider a pool where 50% of the slots are &#8216;weighted&#8217; in a traditional cpu-centric way, but the other 50% are intended to be allocated in a memory-centric way.  This is currently possible, as the <code>SlotWeight</code> attribute can be configured appropriately to be a function of either <code>Cpus</code> or <code>Memory</code>.</p>

<p>But in a scenario where slots are weighted as functions of heterogeneous resource dimensions, it raises a semantic question:  when we sum these weights to obtain the pool-wide available quota, what &#8216;real world&#8217; quantity does this total represent &#8211; if any?   Is it a purely heuristic numeric value with no well defined unit attached?</p>

<p>This question has import.  Understanding what the answer is, or should be, impacts what story we tell to users about what their accounting group configuration actually means.  When I assign a quota to an accounting group in such a heterogeneous environment, what is that quota regulating?   When a job matches a cpu-centric slot, does the cost of that match have a different meaning than when matching against a memory-centric slot?   When the slots are partitionable, a match implies a certain multi-dimensional slice of resources allocated from that slot.  What is the cost of that slice?  Does the sum of costs add up to the original weight on the partitionable slot?  If not, how does that affect our understanding of quota semantics?</p>

<p>It may be possible unify all of these ideas by adopting the perspective that a slot&#8217;s weight is a measure of the maximum number of jobs that can be matched against it.  The cost of a match is W(S)-W(S&#8217;), where W(S) is the weight function evaluated on the slot prior to match, and W(S&#8217;) is the corresponding weight after the match has extracted its requested resources.  The pool&#8217;s total quota is just the sum of W(S), over all slots S in the pool.  Note, this implies that the &#8216;unit&#8217; attached to both slot weights and accounting group quotas is &#8216;jobs&#8217;.</p>

<p>Consider a simple example from the traditional cpu-centric configuration:   A partitionable slot is configured with 8 cpus, and <code>SlotWeight</code> is just its default <code>Cpus</code>.  Using this model, the allocation policy is: &#8216;each match must use >= 1 cpu&#8221;, and that other resource requests are assumed to be not limiting.  The maximum number of matches is 8 jobs, each requesting 1 cpu.   However, a job might also request 2 cpus.  In this case, note that the cost of the match is 2, since the remaining slot has 6 slots, and so W(S&#8217;) now evaluates to 6.   So, the cost of the match is how many fewer possible jobs the original slot can support after the match takes its requested resources.</p>

<p>This approach can be applied equally well to a memory-centric strategy, or a disk centric strategy, or a gpu-based strategy, or any combination simultaneously.  All weights evaluate to a value with unit &#8216;jobs&#8217;.   All match costs are differences between weights (before and after match), and so their values are also in units of &#8216;jobs&#8217;.  Therefore, the semantics of the sum of weights over a pool is always well defined: it is a number of jobs, and spefically a measure of the maximum number of jobs that might match against all the slots in the pool.  When a match acquires resources that reduce this maximum by more than 1 job, that is not in any way inconsistent.  It means the job used resources that might have supported two or more &#8216;smaller&#8217; jobs.   This means that accounting group quotas (and submitter shares) also have a well defined unit and semantic, which is &#8216;how many (or what fraction of) the maximum possible jobs is this group guaranteed by my accounting policy&#8217;</p>

<p>One implication of this proposed semantic for quotas and weights is that the measure for the maximum number of jobs that may match against any given slot must be some finite number.   It implies that all resource dimensions are quantized in some way by the allocation policy.   This scheme would not support a real-valued resource dimension that had no minimum quantization.  I do not think that this is a very heavy-weight requirement, and in fact we have already been moving in that direction with features such as MODIFY_REQUEST_EXPRS_xxx.</p>

<p>When a slot&#8217;s resource allocation policy is defined over all its resources, what bounds this measure of maximum possible matches?  In a case where each job match <em>must</em> use at least one non-zero quantum of each resource dimension, then the limit is the resource with the mimimum quantized levels.   In a case where jobs may request a zero amount of resources, then the limit is the resource with the maximum quantized levels.  (note, it is required that each match use at least one quantum of at least one resource, otherwise the maximum is not properly bounded).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Accounting Groups With Wallaby]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/01/using-accounting-groups-with-wallaby/"/>
    <updated>2012-11-01T07:41:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/01/using-accounting-groups-with-wallaby</id>
    <content type="html"><![CDATA[<p>In this post I will describe how to use HTCondor accounting groups with <a href="http://getwallaby.com">Wallaby</a>.  I will begin by walking through an accounting group configuration on a pool managed by wallaby.  Following, I will demonstrate the configuration in action.</p>

<p>The gist of this demo will be to create a simple accounting group hierarchy:  A top-level group called <code>Demo</code>, and three child groups <code>Demo.A, Demo.B, Demo.C</code>.  <code>Demo</code> will be given a <em>static</em> quota to simulate the behavior of a pool with a particular number of slots available.  The child groups will use <em>dynamic</em> quotas to express their quota shares from the parent as ratios.</p>

<p>First, it is good practice to snapshot current wallaby configuration for reference:</p>

<pre><code>$ wallaby make-snapshot "pre demo state"
</code></pre>

<p>We will be constructing a wallaby feature called <code>AccountingGroups</code> to hold our accounting group configurations.  This creates the feature:</p>

<pre><code>$ wallaby add-feature AccountingGroups
</code></pre>

<p>Wallaby wants to know about features that are used in configurations, so begin by declaring them to the wallaby store:</p>

<pre><code>$ wallaby add-param GROUP_NAMES
$ wallaby add-param GROUP_QUOTA_Demo
$ wallaby add-param GROUP_QUOTA_DYNAMIC_Demo.A
$ wallaby add-param GROUP_QUOTA_DYNAMIC_Demo.B
$ wallaby add-param GROUP_QUOTA_DYNAMIC_Demo.C
$ wallaby add-param GROUP_ACCEPT_SURPLUS_Demo
$ wallaby add-param NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION
$ wallaby add-param NEGOTIATOR_CONSIDER_PREEMPTION
$ wallaby add-param CLAIM_WORKLIFE
</code></pre>

<p>Here we disable the &#8220;claim worklife&#8221; feature by setting claims to expire immediately.   This prevents jobs under one accounting group from acquiring surplus quota and holding on to it when new jobs arrive under a different group:</p>

<pre><code>$ wallaby add-params-to-feature ExecuteNode CLAIM_WORKLIFE=0
$ wallaby add-params-to-subsystem startd CLAIM_WORKLIFE
$ wallaby add-params-to-feature Scheduler CLAIM_WORKLIFE=0
$ wallaby add-params-to-subsystem scheduler CLAIM_WORKLIFE
</code></pre>

<p>If you alter the configuration parameters, you will want the negotiator to reconfigure itself when you activate.  Here we declare the accounting group features as part of the negotiator subsystem:</p>

<pre><code>$ wallaby add-params-to-subsystem negotiator \
GROUP_NAMES \
GROUP_QUOTA_Demo \
GROUP_QUOTA_DYNAMIC_Demo.A \
GROUP_QUOTA_DYNAMIC_Demo.B \
GROUP_QUOTA_DYNAMIC_Demo.C \
NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION \
NEGOTIATOR_CONSIDER_PREEMPTION
</code></pre>

<p>Activate the configuration so far to tell subsystems about new parameters for reconfig</p>

<pre><code>$ wallaby activate
</code></pre>

<p>Now we construct the actual configuration as the <code>AccountingGroups</code> wallaby feature.  Here we are constructing a group <code>Demo</code> with three subgroups <code>Demo.{A|B|C}</code>.  In a multi-node pool with several cores, it is often easiest to play with group behavior by creating a sub-hierarchy such as this <code>Demo</code> sub-hierarchy, and configuring <code>GROUP_ACCEPT_SURPLUS_Demo=False</code>, so that the sub-hierarchy behaves with a well-defined total slot quota (in this case 15).  The sub-groups A,B and C each take 1/3 of the parent&#8217;s quota, so in this example each will receive 5 slots.</p>

<pre><code>$ wallaby add-params-to-feature AccountingGroups \
NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION=False \
NEGOTIATOR_CONSIDER_PREEMPTION=False \
GROUP_NAMES='Demo, Demo.A, Demo.B, Demo.C' \
GROUP_ACCEPT_SURPLUS=True \
GROUP_QUOTA_Demo=15 \
GROUP_ACCEPT_SURPLUS_Demo=False \
GROUP_QUOTA_DYNAMIC_Demo.A=0.333 \
GROUP_QUOTA_DYNAMIC_Demo.B=0.333 \
GROUP_QUOTA_DYNAMIC_Demo.C=0.333
</code></pre>

<p>With our accounting group feature created, we can apply it to the machine our negotiator daemon is running on.  Then snapshot our configuration modifications for reference, and activate the new configuration:</p>

<pre><code>$ wallaby add-features-to-node negotiator.node.com AccountingGroups
$ wallaby make-snapshot 'new acct group config'
$ wallaby activate
</code></pre>

<p>Now we will demonstrate the new feature in action.  Submit the following file to your pool, which submits 100 jobs each to groups <code>Demo.A</code> with durations randomly chosen between 25 and 35 seconds:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = $$([25 + random(11)])
transfer_executable = false
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="Demo.A.user1"
queue 100
</code></pre>

<p>Once you make this submission, allow the jobs to negotiate, and you can check to see what accounting groups are running on slots by inspecting the value of <code>RemoteNegotiatingGroup</code> on slot ads.   You should see that subgroup <code>Demo.A</code> has acquired surplus and is running 15 jobs, as there are no jobs under groups <code>Demo.B</code> or <code>Demo.C</code> that need slots.  Note, due to jobs completing between negotiation cycles, these numbers can be less than the maximum possible at certain times.  If you have any other slots in the pool, they will show up in the output below as having either <code>undefined</code> negotiating group or possibly <code>&lt;none&gt;</code> if any other jobs are running.</p>

<pre><code>$ condor_status -format "%s\n" 'ifThenElse(RemoteNegotiatingGroup isnt undefined, string(RemoteNegotiatingGroup), "undefined")' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
 15 Demo.A
 50 &lt;none&gt;
 50 undefined
115 total
</code></pre>

<p>Now submit some jobs against <code>Demo.B</code> and <code>Demo.C</code>, like so:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = $$([25 + random(11)])
transfer_executable = false
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="Demo.B.user1"
queue 100
+AccountingGroup="Demo.C.user1"
queue 100
</code></pre>

<p>Once these jobs begin to negotiate, we expect to see the jobs balanced between the three groups evenly, as we gave each group 1/3 of the quota:</p>

<pre><code>$ condor_status -format "%s\n" 'ifThenElse(RemoteNegotiatingGroup isnt undefined, string(RemoteNegotiatingGroup), "undefined")' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
  5 Demo.A
  5 Demo.B
  5 Demo.C
 50 &lt;none&gt;
 50 undefined
115 total
</code></pre>

<p>Finally, we see what happens if we remove jobs under <code>Demo.B</code>:</p>

<pre><code>$ condor_rm -constraint 'AccountingGroup =?= "Demo.B.user1"'
</code></pre>

<p>Now we should see quota start to share between <code>Demo.A</code> and <code>Demo.C</code>:</p>

<pre><code>$ condor_status -format "%s\n" 'ifThenElse(RemoteNegotiatingGroup isnt undefined, string(RemoteNegotiatingGroup), "undefined")' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
  7 Demo.A
  8 Demo.C
 50 &lt;none&gt;
 50 undefined
115 total
</code></pre>

<p>With this accounting group configuration in place, you can play with changing quotas for the accounting groups and observe the numbers of running jobs change in response.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Randomized Sleep Jobs in HTCondor Using Delayed Evaluation]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/10/31/randomized-sleep-jobs-in-htcondor-using-delayed-evaluation/"/>
    <updated>2012-10-31T14:17:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/10/31/randomized-sleep-jobs-in-htcondor-using-delayed-evaluation</id>
    <content type="html"><![CDATA[<p>In some cases, when testing or demonstrating the performance of an HTCondor pool, it is useful to submit a plug of jobs with randomized running times.  The standard technique for controlling run times is to submit a classic &#8216;sleep&#8217; job.  However, randomizing the argument to sleep is another matter.  Luckily there is an easy way to do this with a single submit file, using delayed evaluation syntax.</p>

<p>A classad expression placed inside of a special enclosure, like this: <code>$$([ &lt;expr&gt; ])</code>, causes <code>&lt;expr&gt;</code> to be evaluated at the time the job ad is matched with a slot.  You can read more about delayed evaluation <a href="http://research.cs.wisc.edu/condor/manual/v7.8/condor_submit.html#78367">here</a>.  Consider the following example submit file:</p>

<pre><code>universe = vanilla
executable = /bin/sleep

# generate a random sleep duration when job is matched
args = $$([25 + random(11)])

# boilerplate to avoid file transfers and notifications
transfer_executable = false
should_transfer_files = no
when_to_transfer_output = on_exit
notification = never

# generate 100 copies of this job - each will evaluate the
# randomizing expression independently
queue 100
</code></pre>

<p>As you can see in the example above, the value of <code>args</code> is set to the delayed evaluation expression <code>$$([25 + random(11)])</code>, which will evaluate the classad expression <code>25 + random(11)</code> when each job ad matches a slot to run.  The <code>queue 100</code> command generates 100 separate job ads, and so the net effect is 100 jobs, which will each run a sleep job with a duration <em>randomly chosen</em> between 25 and 35.</p>

<p>If we submit this file to a condor pool, and let the jobs run to completion, we can check the pool history file to see how the <code>Args</code> attribute was set on the job ad using the special generative attribute <code>MATCH_EXP_Args</code>, and the <a href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/">cchist tool</a>:</p>

<pre><code>$ cchist condor_history 'MATCH_EXP_Args'
     11 25
      7 26
     10 27
      9 28
      7 29
     13 30
      8 31
      7 32
      8 33
      9 34
     11 35
    100 total
</code></pre>

<p>We can also sanity check our measure of actual run time, to see that those values are close to our values of <code>Args</code>:</p>

<pre><code>$ cchist condor_history 'CompletionDate-JobCurrentStartDate'
      1 25
     11 26
      9 27
      8 28
      9 29
      9 30
     12 31
      4 32
      8 33
     10 34
     12 35
      6 36
      1 37
    100 total
</code></pre>

<p>Have fun with easy random sleep jobs!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hosting a Blog Feed Aggregator With Octopress]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/10/05/hosting-a-blog-feed-aggregator-with-octopress/"/>
    <updated>2012-10-05T12:52:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/10/05/hosting-a-blog-feed-aggregator-with-octopress</id>
    <content type="html"><![CDATA[<p>I have written an Octopress plugin to allow turnkey support for hosting a blog feed aggregator, in Octopress idiomatic style.  I will describe the steps to install it and use it below.  Some of its current features are:</p>

<ul>
<li>Easy configuration and deployment, providing all feed aggregator parameters as yaml front-matter</li>
<li>Turn-key generation of feed aggregator pages, in the configured site style</li>
<li>Optional generation of a &#8216;meta-feed&#8217; in atom.xml format, from aggregated feed entries</li>
<li>Automatic removal of duplicate feed list urls, and automatic removal of duplicate posts (e.g. if multiple category feeds from the same author are listed)</li>
<li>Automatic generation of feed author list as an Octopress &#8216;aside&#8217;</li>
<li>Inclusion/exclusion of posts based on number of posts and/or post age</li>
<li>Display of full or summary content based on number of posts and/or post age</li>
</ul>


<h3>Install the feed_aggregator.rb plugin</h3>

<p>Currently, you can obtain a copy of &#8220;feed_aggregator.rb&#8221; here:</p>

<p><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/plugins/feed_aggregator.rb">feed_aggregator.rb</a></p>

<p>Simply copy this file into the plugins directory for your octopress repo:</p>

<pre><code>$ cp feed_aggregator.rb /path/to/your/octopress/repo/plugins
</code></pre>

<h3>Install feed aggregator layout files</h3>

<p>You can obtain a copy of the layout files here:</p>

<ul>
<li><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/.themes/classic/source/_layouts/feed_aggregator.html">feed_aggregator.html</a></li>
<li><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/.themes/classic/source/_layouts/feed_aggregator_page.html">feed_aggregator_page.html</a></li>
<li><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/.themes/classic/source/_layouts/feed_aggregator_meta.xml">feed_aggregator_meta.xml</a></li>
</ul>


<p>Copy the layouts files to your &#8216;_layouts&#8217; directory:</p>

<pre><code>$ cp feed_aggregator.html /path/to/your/octopress/repo/source/_layouts
$ cp feed_aggregator_page.html /path/to/your/octopress/repo/source/_layouts
$ cp feed_aggregator_meta.xml /path/to/your/octopress/repo/source/_layouts
</code></pre>

<h3>Add feedzirra dependency to the Octopress Gemfile</h3>

<p>Octopress wants its dependencies bundled, so you will want to add this dependency to /path/to/your/octopress/repo/Gemfile:</p>

<pre><code>gem 'feedzirra', '~&gt; 0.1.3'
</code></pre>

<p>Then update the bundles:</p>

<pre><code>$ bundle update
</code></pre>

<h3>Create a page for your feed aggregator</h3>

<p>Here is an example feed aggregator:</p>

<pre><code>---
# use the 'feed_aggregator' layout to generate a feed aggregator page
layout: feed_aggregator

# Title to display for the feed
title: My Blog Feed Aggregator

# maximum number of entries from each feed url to display (defaults to 5)
# use '0' for 'no limit'
post_limit: 5

# limit on total posts for feed (defaults to 100)
# use 0 for 'no limit'
post_total_limit: 50

# maximum post age to include: &lt;N&gt; { seconds | minutes | hours | days | weeks | months | years }
# abbreviations and plurals are supported, e.g.  w, week, weeks
# defaults to '1 year'
# use '0 &lt;any-unit&gt;' for 'no limit'
post_age_limit: 6 months

# only render full content for the first &lt;N&gt; posts 
# (default is 'full content for all posts')
# use a limit of 0 to use all summaries
full_post_limit: 10

# use summaries for all posts older than this 
# (default is 'no maximum age')
# works like post_age_limit
full_post_age_limit: 1 month

# generate a 'meta-feed' atom file, with the given name 'atom.xml' (meta feeds are optional)
# (with no directory, generates in same directory as the feed aggregator page)
meta_feed: atom.xml

# list all urls to aggregate here
# You can either specify a single feed url, or explicitly specify 'url', 'author' 
# and/or 'author_url' params for the feed aggregator to use.
# feed_aggregator does its best to supply these values automatically otherwise.
feed_list:
  - http://blog_site_1.com/atom.xml
  - http://blog_site_2.com/atom.xml
  - url: http://www.john_doe.com/feed/feed.rss
    author: John Doe
    author_url: http://www.john_doe.com
---
</code></pre>

<p>As you can see, you only need to supply some yaml front-matter.  Page formatting/rendering is performed automatically from the information in the header.  You must use <code>layout: feed_aggregator</code>, and include the standard <code>title</code> to use for the aggregator title, and the <code>feed_list</code> to supply the individual feeds to aggregate.  Other parameters have default values and behaviors, which are described above.  Various <code>meta_feed</code> path behaviors are described in their own section below.</p>

<p>Once you&#8217;ve created the page, you can publish as usual:</p>

<pre><code>$ rake generate
$ rake deploy
</code></pre>

<p>If you want to update your feed automatically, you can set up a cron job:</p>

<pre><code>cd /path/to/octopress/repo
rake generate
rake deploy
</code></pre>

<h3>Screen Shot</h3>

<p>Here is a screen shot of a feed aggregator.  It respects whatever style theme is configured for the site.  The aggregator title is at the top, and a list of contributing authors is automatically generated as an &#8216;aside&#8217;.  Each author name links to the parent blog of the author&#8217;s feed.  In addition to the standard date, the author&#8217;s name is also included.  Post titles link back to the original post url.</p>

<p><img src="http://erikerlandson.github.com/assets/feed_aggregator/screen1.png" alt="Aggregator Screen Shot" /></p>

<h3>Meta feed generation</h3>

<p>You may optionally request that a meta feed, created from the aggregated posts, be generated.  The meta feed is created in atom format.  Following are some examples of specifying meta feed files</p>

<pre><code># Generate a meta feed called 'atom.xml' in the same directory as the feed aggregator page
# e.g. if the url for the feed aggregator page is  http://blog.site.com/aggregator/index.html, 
# then the path to the meta-feed will be: http://blog.site.com/aggregator/atom.xml
meta_feed: atom.xml

# Generate a meta feed called 'wilma.xml' in subdirectory 'flintstones' of the website.
# the url for this file will be:   http://blog.site.com/flintstones/wilma.xml
meta_feed: /flintstones/wilma.xml

# url for this will be http://blog.site.com/metafeed.xml
meta_feed: /metafeed.xml

# Supplying no file name is equivalent to 'meta_feed: atom.xml'
meta_feed:
</code></pre>

<h3>To Do</h3>

<ul>
<li>It might be nice to support the display of an avatar/icon for authors</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improved Parse Checking for ClassAd Log Files in Condor]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/09/26/improved-parse-checking-for-classad-log-files-in-condor/"/>
    <updated>2012-09-26T10:06:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/09/26/improved-parse-checking-for-classad-log-files-in-condor</id>
    <content type="html"><![CDATA[<p>Condor maintains certain key transactional information using the ClassAd Log system.  For example, both the negotiator&#8217;s accountant log (&#8220;Accountantnew.log&#8221;) and the scheduler&#8217;s job queue log (&#8220;job_queue.log&#8221;) are maintained in ClassAd Log format.</p>

<p>As of <a href="http://www.redhat.com/products/mrg/grid/">Red Hat Grid 2.2</a> (upstream: <a href="http://research.cs.wisc.edu/condor/">condor 7.9.0</a>), the ClassAd Log system provides significantly improved parse checking.  This upgraded format checking allows a much wider variety of log corruptions to be detected, and also provides detailed information on the location of corruptions encountered.</p>

<h3>ClassAd Log Format</h3>

<p>A bit of familiarity with ClassAd Log format will aid in understanding subsequent discussion.  The ClassAd Log system serializes a ClassAd collection history as a sequence of tuples:  <code>opcode, [key, [args]]</code>.  For example, here is an annotated ClassAd log excerpt (NOTE: annotations or comments are illegal in an actual file):</p>

<pre><code>105                               &lt;- open a transaction
103 1.0 LastSuspensionTime 0      &lt;- for classad '1.0', set LastSuspentionTime to 0
103 1.0 CurrentHosts 1            &lt;- for classad '1.0', set CurrentHosts to 1
106                               &lt;- close the transaction
</code></pre>

<p>ClassAd Log parse checking works by detecting any occurrence of an invalid op-code, or any invalid ClassAd expression in the RHS of an attribute update operation (opcode 103, as in the example above)</p>

<h3>Examples of Parse Failure Detection</h3>

<p>Consider a ClassAd Log with a corrupted op-code &#8216;ZMG&#8217; (in this case, not even a proper integer):</p>

<pre><code>107 1 CreationTimestamp 1334245749
101 0.0 Job Machine
103 0.0 NextClusterNum 1
105
ZMG 1.0 JobStatus 2                        &lt;- Oh no, a bad opcode!
103 1.0 EnteredCurrentStatus 1334245771
103 1.0 LastSuspensionTime 0
103 1.0 CurrentHosts 1
106
105
103 1.1 LastJobStatus 1
103 1.1 JobStatus 2
</code></pre>

<p>Parse checking will result in the following log message in the scheduler, which provides its assessment of what operation line/tuple it found the corruption, and the following 3 lines for additional context:</p>

<pre><code>09/12/12 15:30:35 WARNING: Encountered corrupt log record 5 (byte offset 89)
09/12/12 15:30:35 Lines following corrupt log record 5 (up to 3):
09/12/12 15:30:35     103 1.0 EnteredCurrentStatus 1334245771
09/12/12 15:30:35     103 1.0 LastSuspensionTime 0
09/12/12 15:30:35     103 1.0 CurrentHosts 1
09/12/12 15:30:35 ERROR "Error: corrupt log record 5 (byte offset 89) occurred inside closed transaction, recovery failed" at line 1136 in file /home/eje/git/grid/src/condor_utils/classad_log.cpp
</code></pre>

<p>Note that here the scheduler halted with an exception, as strict parsing was enabled, and the error was inside a completed transaction.</p>

<p>Here is a second example that contains a badly-formed ClassAd expression:</p>

<pre><code>107 1 CreationTimestamp 1334245749
101 0.0 Job Machine
103 0.0 NextClusterNum 1
105
103 1.0 JobStatus 2
103 1.0 EnteredCurrentStatus 1334245749
103 1.0 LastSuspensionTime 0
103 1.0 CurrentHosts 1
106
105
103 1.1 LastJobStatus 1 + eek!             &lt;- bad ClassAd expr!
103 1.1 JobStatus 2
</code></pre>

<p>Note that parse errors detected in unterminated transactions (the last transaction in a file may be uncompleted) are considered non-fatal:</p>

<pre><code>09/12/12 15:43:29 WARNING: Encountered corrupt log record 11 (byte offset 211)
09/12/12 15:43:29 Lines following corrupt log record 11 (up to 3):
09/12/12 15:43:29     103 1.1 JobStatus 2
09/12/12 15:43:29 Detected unterminated log entry in ClassAd Log /home/eje/condor/local/spool/job_queue.log. Forcing rotation.
</code></pre>

<h3>Disabling Strict Parse Checking</h3>

<p>Strict parse checking means that detected errors are fatal (unless in an unterminated transaction).  One consequence of the former lax error checking for Classad Log files is that some log file output was generated that was not properly formed.  Most such instances have been identified and corrected.  However, in order to accomodate legacy ClassAd Log files and any hidden bugs in log output generation, a condor configuration variable has been provided to disable strict checking:</p>

<pre><code># Disable strict parsing: parse errors will not be fatal
CLASSAD_LOG_STRICT_PARSING = False
</code></pre>

<p>In Red Hat Grid 2.2, <code>CLASSAD_LOG_STRICT_PARSING</code> defaults to <code>False</code>.  In the upstream condor repository, the default value has been set to <code>True</code>, in order to allow strict parsing failures to capture any remaining infrequent bugs in ClassAd log generation.</p>

<p>Note that strict checking can also be disabled or enabled <em>selectively</em>.  For example, this configuration disables strict checking only on the negotiator:</p>

<pre><code>CLASSAD_LOG_STRICT_PARSING = True
NEGOTIATOR.CLASSAD_LOG_STRICT_PARSING = False
</code></pre>

<h3>Categories of Undetectable Corruption</h3>

<p>In the ClassAd Log format, the key is considered an arbitrary string.  Therefore, any corruption that alters a key value is not detectable:</p>

<pre><code>103 1.rats! LastSuspensionTime 0   &lt;- weird key '1.rats!' will go undetected
</code></pre>

<p>Similarly, ClassAd attribute names are by nature arbitrary, and so corruptions to a name can go undetected:</p>

<pre><code>103 1.0 LastOopsie 0   &lt;- LastOopsie is a valid attribute name
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Driving a Condor Job Renice Policy with Accounting Groups]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/27/driving-a-condor-job-renice-policy-with-accounting-groups/"/>
    <updated>2012-07-27T13:50:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/27/driving-a-condor-job-renice-policy-with-accounting-groups</id>
    <content type="html"><![CDATA[<p>Condor can run its jobs with a renice priority level specified by <code>JOB_RENICE_INCREMENT</code>, which defaults simply to 10, but can in fact be any ClassAd expression, and is evaluated in the context of the job ad corresponding to the job being run.</p>

<p>This opens up an opportunity to create a renice <em>policy</em>, driven by accounting groups.  Consider a <a href="http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool/">scenario I discussed previously</a>, where a condor pool caters to mission critical (MC) jobs and regular (R) jobs.</p>

<p>An additional configuration trick we could apply is to add a renice policy that gives a higher renice value (that is, a lower priority) to any jobs that aren&#8217;t run under the mission-critical (MC) rubric, as in this example configuration:</p>

<pre><code># A convenience expression that extracts group, e.g. "mc.user@domain.com" --&gt; "mc"
SUBMIT_EXPRS = AcctGroupName
AcctGroupName = ifThenElse(my.AccountingGroup =!= undefined, \
                           regexps("^([^@]+)\.[^.]+", my.AccountingGroup, "\1"), "&lt;none&gt;")

NUM_CPUS = 3

# Groups representing mission critical and regular jobs:
GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 2
GROUP_QUOTA_R = 1

# Any group not MC gets a renice increment of 10:
JOB_RENICE_INCREMENT = 10 * (AcctGroupName =!= "MC")
</code></pre>

<p>To demonstrate this policy in action, I wrote a little shell script I called <code>burn</code>, whose only function is to burn cycles for a given number of seconds:</p>

<pre><code>#!/bin/sh

# usage: burn [n]
# where n is number of seconds to burn cycles
s="$1"
if [ -z "$s" ]; then s=60; fi

t0=`date +%s`
while [ 1 ]; do
    x=0
    # burn some cycles:
    while [ $x -lt 10000 ]; do let x=$x+1; done
    t=`date +%s`
    let e=$t-$t0
    # halt when the requested time is up:
    if [ $e -gt $s ]; then exit ; fi
done
</code></pre>

<p>Begin by standing up a condor pool including the configuration above.   Make sure the <code>burn</code> script is readable.  Also, it is preferable to make sure your system is unloaded (load average should be as close to zero as reasonably possible).  Then submit the following, which instantiates two <code>burn</code> jobs running under accounting group <code>MC</code> and a third under group <code>R</code>:</p>

<pre><code>universe = vanilla
cmd = /path/to/burn
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup = "MC.user"
queue 2
+AccountingGroup = "R.user"
queue 1
</code></pre>

<p>Allow the jobs to negotiate and then run for a couple minutes.  You should then see something similar to the following load-average information from the slot ads:</p>

<pre><code>$ condor_status -format "%s" SlotID -format " | %.2f" LoadAvg -format " | %.2f" CondorLoadAvg -format " | %.2f" TotalLoadAvg -format " | %.2f" TotalCondorLoadAvg -format " | %s\n" AccountingGroup | sort
1 | 1.33 | 1.33 | 2.75 | 2.70 | MC.user@localdomain
2 | 1.28 | 1.24 | 2.75 | 2.70 | MC.user@localdomain
3 | 0.13 | 0.13 | 2.77 | 2.72 | R.user@localdomain
</code></pre>

<p>Note, which particular <code>SlotID</code> runs which job may vary.  However, we expect to see that the load averages for the slot running group <code>R</code> are much lower than the load averages for slots running jobs under group <code>MC</code>, as seen above.</p>

<p>We can explicitly verify the renice numbers from our policy to see that our one <code>R</code> job has a nice value of 10 (and is using only a fraction of the cpu):</p>

<pre><code># tell 'ps' to give us (pid, %cpu, nice, cmd+args):
$ ps -eo "%p %C %n %a" | grep 'burn 600'
22403 10.2  10 /bin/sh /home/eje/bin/burn 600
22406 93.2   0 /bin/sh /home/eje/bin/burn 600
22411 90.6   0 /bin/sh /home/eje/bin/burn 600
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LIFO and FIFO Preemption Policies for a Condor Pool]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/19/lifo-and-fifo-preemption-policies-for-a-condor-pool/"/>
    <updated>2012-07-19T13:57:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/19/lifo-and-fifo-preemption-policies-for-a-condor-pool</id>
    <content type="html"><![CDATA[<p>On a Condor pool, a Last In First Out (LIFO) preemption policy favors choosing the longest-running job from the available preemption options.  Correspondingly, a First In First Out (FIFO) policy favors the most-recent job for preemption.</p>

<p>Configuring a LIFO or FIFO policy is easy, using the <code>PREEMPTION_RANK</code> configuration variable.  <code>PREEMPTION_RANK</code> defines a ClassAd expression that is evaluated for all slots that are candidates for claim preemption, and causes those candidates to be sorted so that the candidates with the highest rank value are considered first.   Therefore, to implement a LIFO (or FIFO) preemption policy, one needs reference an expression that represents the claiming job&#8217;s running time:</p>

<pre><code># LIFO preemption: favor preempting jobs that have been running the longest
PREEMPTION_RANK = TotalJobRunTime
# turn this into FIFO by using (-TotalJobRunTime)
</code></pre>

<p>The attribute <code>TotalJobRunTime</code> represents the amount of time a job has been running on its claim (generally, this is effectively equivalent to total running time, unless your job supports some form of checkpointing), and so ranking preemption candidates by this attribute results in LIFO preemption, and ranking by its negative provides FIFO preemption.</p>

<p>Note that <code>PREEMPTION_RANK</code> applies <em>only</em> to candidates that have already met the requirements defined on <code>PREEMPTION_REQUIREMENTS</code>, or the slot-centric preemption policy defined by <code>RANK</code>.  <code>PREEMPTION_RANK</code> does not itself determine what claimed slots are considered by a job for preemption.</p>

<p>To demonstrate LIFO and FIFO preemption in action, consider the following configuration:</p>

<pre><code># turn off scheduler optimizations, as they can sometimes obscure the
# negotiator/matchmaker behavior
CLAIM_WORKLIFE = 0
CLAIM_PARTITIONABLE_LEFTOVERS = False

# reduce update latencies for faster testing response
UPDATE_INTERVAL = 15
NEGOTIATOR_INTERVAL = 20
SCHEDD_INTERVAL = 15

# for demonstration purposes, make sure basic preemption knobs are 'on'
MAXJOBRETIREMENTTIME = 0
PREEMPTION_REQUIREMENTS = True
NEGOTIATOR_CONSIDER_PREEMPTION = True
RANK = 0.0

# LIFO preemption: favor preempting jobs that have been running the longest
PREEMPTION_RANK = TotalJobRunTime
# turn this into FIFO by using (-TotalJobRunTime)

# define 3 cpus to provide fodder for preemption
NUM_CPUS = 3
</code></pre>

<p>Begin by spinning up a condor pool with the configuration above.  When the pool is operating, fill the three slots with jobs for &#8216;user1&#8217;, with a delay to ensure that jobs have easily distinguishable values for <code>TotalJobRunTime</code>:</p>

<pre><code>$ cat /tmp/user1.jsub 
universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="user1"
queue 1

$ condor_submit /tmp/user1.jsub ; sleep 30 ; condor_submit /tmp/user1.jsub ; sleep 30 ; condor_submit /tmp/user1.jsub
</code></pre>

<p>Once these jobs have all started running, verify their run times using <a href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/">ccsort</a>:</p>

<pre><code>$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
1.0 | 78 | user1@localdomain
2.0 | 36 | user1@localdomain
3.0 | 16 | user1@localdomain
</code></pre>

<p>to make preemption easy, give user1 a low priority:</p>

<pre><code>$ condor_userprio -setprio user1@localdomain 10
</code></pre>

<p>Now, we will submit some jobs for &#8216;user2&#8217;: which will be allowed to preempt jobs for &#8216;user1&#8217;.  We should see that the longest-running job for user1 is chosen each time:</p>

<pre><code>$ condor_submit /tmp/user2.jsub
Submitting job(s).
1 job(s) submitted to cluster 4.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
2.0 | 81 | user1@localdomain
3.0 | 61 | user1@localdomain
4.0 | 2 | user2@localdomain

$ condor_submit /tmp/user2.jsub
Submitting job(s).
1 job(s) submitted to cluster 5.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 91 | user1@localdomain
4.0 | 32 | user2@localdomain
5.0 | 3 | user2@localdomain
</code></pre>

<p>Now we change LIFO to FIFO and demonstrate.  Switch the sign of <code>TotalJobRunTime</code>:</p>

<pre><code># Now I am FIFO!
PREEMPTION_RANK = -TotalJobRunTime
</code></pre>

<p>And restart the negotiator, and check on our currently running jobs:</p>

<pre><code>$ condor_restart -negotiator

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 151 | user1@localdomain
4.0 | 92 | user2@localdomain
5.0 | 49 | user2@localdomain
</code></pre>

<p>Now, set up &#8216;user2&#8217; for easy preemption like user1:</p>

<pre><code>$ condor_userprio -setprio user2@localdomain 10
</code></pre>

<p>And submit some jobs for user3.  Since we reconfigured for FIFO preemption, we should now see the <em>most recent</em> job preempted each time (in this case, these should both be the &#8216;user2&#8217; jobs):</p>

<pre><code>$ condor_submit /tmp/user3.jsub
Submitting job(s).
1 job(s) submitted to cluster 6.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 241 | user1@localdomain
4.0 | 182 | user2@localdomain
6.0 | 15 | user3@localdomain

$ condor_submit /tmp/user3.jsub
Submitting job(s).
1 job(s) submitted to cluster 7.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 301 | user1@localdomain
6.0 | 75 | user3@localdomain
7.0 | 17 | user3@localdomain
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Configuring Minimum and Maximum Resources for Mission Critical Jobs in a Condor Pool]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool/"/>
    <updated>2012-07-10T15:49:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool</id>
    <content type="html"><![CDATA[<p>Suppose you are administering a Condor pool for a company or organization where you want to support both &#8220;mission critical&#8221; (MC) jobs and &#8220;regular&#8221; (R) jobs.  Mission critical jobs might include IT functions such as backups, or payroll, or experiment submissions from high profile internal customers.  Regular jobs encompass any jobs that can be delayed, or preempted, with little or no consequence.</p>

<p>As part of your Condor policy for supporting MC jobs, you may want to ensure that these jobs always have access to a minimum set of resources on the pool.  In order to maintain the peace, you may also wish to set a pool-wide maximum on MC jobs, to leave some number of resources available for R jobs as well.  The following configuration, which I will discuss and demonstrate below, configures a pool-wide minimum <em>and maximum</em> for resources allocated to MC jobs.  Additionally, it shows how to dedicate MC resources on specific nodes in the pool.</p>

<pre><code># turn off scheduler optimizations, as they can sometimes obscure the
# negotiator/matchmaker behavior
CLAIM_WORKLIFE = 0

# turn off adaptive loops in negotiation - these give a single
# 'traditional' one-pass negotiation cycle
GROUP_QUOTA_MAX_ALLOCATION_ROUNDS = 1
GROUP_QUOTA_ROUND_ROBIN_RATE = 1e100

# for demonstration purposes, make sure basic preemption knobs are 'on'
MAXJOBRETIREMENTTIME = 0
PREEMPTION_REQUIREMENTS = True
NEGOTIATOR_CONSIDER_PREEMPTION = True
RANK = 0.0

# extracts the acct group name, e.g. "MC.user@localdomain" --&gt; "MC"
SUBMIT_EXPRS = AcctGroupName CCLimits
AcctGroupName = ifThenElse(my.AccountingGroup =!= undefined, \
                           regexps("^([^@]+)\.[^.]+", my.AccountingGroup, "\1"), "&lt;none&gt;")
CCLimits = ifThenElse(my.ConcurrencyLimits isnt undefined, \
                      my.ConcurrencyLimits, "***")
# note - the "my." scoping in the above is important - 
# these attribute names may also occur in a machine ad

# oversubscribe the machine to simulate 20 nodes on a single box
NUM_CPUS = 20

# accounting groups, each with equal quota
# Mission Critical jobs are associated with group 'MC'
# Regular jobs are associated with group 'R'
GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 10
GROUP_QUOTA_R = 10

# enable 'autoregroup' for groups, which gives all grps
# a chance to compete for resources above their quota
GROUP_AUTOREGROUP = TRUE
GROUP_ACCEPT_SURPLUS = FALSE

# a pool-wide limit on MC job resources
# note this is a "hard" limit - with this example config, MC jobs cannot exceed this
# limit even if there are free resources
MC_JOB_LIMIT = 15

# special slot for MC jobs, effectively reserves
# specific resources for MC jobs on a particular node.
SLOT_TYPE_1 = cpus=1
SLOT_TYPE_1_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_1 = 5

# Allocate any "non-MC" remainders here:
SLOT_TYPE_2 = cpus=1
SLOT_TYPE_2_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_2 = 15

# note - in the above, I declared static slots for the purposes of 
# demonstration, because partitionable slots interfere with clarity of
# APPEND_RANK expr behavior, due to being peeled off 1 slot at a time
# in the negotiation cycle

# A job counts against MC_JOB_LIMIT if and only if it is of the "MC" 
# accounting group, otherwise it won't be run
START = ($(START)) &amp;&amp; (((AcctGroupName =?= "MC") &amp;&amp; (stringListIMember("mc_job", CCLimits))) \
              || ((AcctGroupName =!= "MC") &amp;&amp; !stringListIMember("mc_job", CCLimits)))

# rank from the slot's POV:
# "MC-reserved" slots (slot type 1) prefer MC jobs,
# while other slots have no preference
RANK = ($(RANK)) + 10.0*ifThenElse((SlotTypeID=?=1) || (SlotTypeID=?=-1), \
                                   1.0 * (AcctGroupName =?= "MC"), 0.0)

# rank from the job's POV:
# "MC" jobs prefer any specially allocated per-node resources
# any other jobs prefer other jobs
APPEND_RANK = 10.0*ifThenElse(AcctGroupName =?= "MC", \
              1.0*((SlotTypeID=?=1) || (SlotTypeID=?=-1)), \
              1.0*((SlotTypeID=!=1) &amp;&amp; (SlotTypeID=!=-1)))

# If a job negotiated under "MC", it may not be preempted by a job that did not.
PREEMPTION_REQUIREMENTS = ($(PREEMPTION_REQUIREMENTS)) &amp;&amp; \
                          ((SubmitterNegotiatingGroup =?= "MC") || \
                           (RemoteNegotiatingGroup =!= "MC"))
</code></pre>

<p>Next I will discuss some of the components from this configuration and their purpose.  The first goal of a pool-wide resource minimum is accomplished by declaring accounting groups for MC and R jobs to run against:</p>

<pre><code>GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 10
GROUP_QUOTA_R = 10
</code></pre>

<p>We will enable the autoregroup feature, which allows jobs to also compete for any unused resources <em>without</em> regard for accounting groups, after all jobs have had an opportunity to match under their group.  This is a good way to allow opportunistic resource usage, and also will facilitate demonstration.</p>

<pre><code>GROUP_AUTOREGROUP = TRUE
</code></pre>

<p>A pool-wide maximum on resource usage by MC jobs can be accomplished with a concurrency limit.  Note that this limit is larger than the group quota for MC jobs:</p>

<pre><code>MC_JOB_LIMIT = 15
</code></pre>

<p>It is also desirable to enforce the semantic that MC jobs <em>must</em> &#8216;charge&#8217; against the MC_JOB concurrency limit, and conversely that any non-MC jobs are not allowed to charge against that limit.   Adding the following clause to the START expression enforces this semantic by preventing any jobs not following this rule from running:</p>

<pre><code>START = ($(START)) &amp;&amp; (((AcctGroupName =?= "MC") &amp;&amp; (stringListIMember("mc_job", CCLimits))) \
                    || ((AcctGroupName =!= "MC") &amp;&amp; !stringListIMember("mc_job", CCLimits)))
</code></pre>

<p>The final resource related goal for MC jobs is to reserve a certain number of resources on specific machines in the pool.  In the configuration above that is accomplished by declaring a special slot type, as here where we declare 5 slots of slot type 1 (the remaining 15 slots are declared via slot type 2, above):</p>

<pre><code>SLOT_TYPE_1 = cpus=1
SLOT_TYPE_1_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_1 = 5
</code></pre>

<p>Then we add a term to the slot rank expression that will cause any slot of type 1 to preempt a non-MC job in favor of an MC job (the factor of 10.0 is an optional tuning factor to allow this term to either take priority over other terms, or cede priority):</p>

<pre><code>RANK = ($(RANK)) + 10.0*ifThenElse((SlotTypeID=?=1) || (SlotTypeID=?=-1), \
                                   1.0 * (AcctGroupName =?= "MC"), 0.0)
</code></pre>

<p>(Note, slot type -1 would represent a dynamic slot derived from a partitionable slot of type 1.  In this example, all slots are static)</p>

<p>An additional &#8220;job side&#8221; rank term can also be helpful, to allow MC jobs to try and match special MC reserved slots first, and to allow non-MC jobs to avoid reserved slots if possible:</p>

<pre><code>APPEND_RANK = 10.0*ifThenElse(AcctGroupName =?= "MC", \
              1.0*((SlotTypeID=?=1) || (SlotTypeID=?=-1)), \
              1.0*((SlotTypeID=!=1) &amp;&amp; (SlotTypeID=!=-1)))
</code></pre>

<p>Lastly, preemption policy can be configured to help enforce resource allocations for MC jobs.  Here, a preemption clause is added to prevent any non-MC job from preempting a MC job, and specifically one that <em>negotiated</em> under its group quota (that is, it refers to RemoteNegotiatingGroup):</p>

<pre><code>PREEMPTION_REQUIREMENTS = ($(PREEMPTION_REQUIREMENTS)) &amp;&amp; \
                          ((SubmitterNegotiatingGroup =?= "MC") || \
                           (RemoteNegotiatingGroup =!= "MC"))
</code></pre>

<p>With the example policy configuration unpacked, we can demonstrate its behavior.  Begin by spinning up a pool with the above configuration.  Verify that we have the expected slots (You can refer <a href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/">here to learn more about cchist</a>):</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 undefined | undefined | 1
     15 undefined | undefined | 2
     20 total
</code></pre>

<p>Next, submit 20 Mission Critical jobs (getting enough sleep is critical):</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
concurrency_limits = mc_job
+AccountingGroup="MC.user"
queue 20
</code></pre>

<p>Since we configured a pool-wide maximum of 15 cores, we want to verify that we did not exceed that limit.  Note that 5 slots were negotiated under &#8220;&lt;none>&#8221;, via the autoregroup feature (denoted by the value in RemoteNegotiatingGroup), as the group quota for MC is 10, and the MC jobs were able to match their pool limit of 15:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 MC | MC | 1
      5 MC | MC | 2
      5 MC | &lt;none&gt; | 2
      5 undefined | undefined | 2
     20 total
</code></pre>

<p>Next we set the MC submitter to a lower priority (i.e. higher prio value):</p>

<pre><code>$ condor_userprio -setprio MC.user@localdomain 10
The priority of MC.user@localdomain was set to 10.000000
</code></pre>

<p>Now we submit 15 &#8220;regular&#8221; R jobs:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="R.user"
queue 15
</code></pre>

<p>The submitter &#8220;R.user&#8221; currently has higher priority than &#8220;MC.user&#8221;, however our preemption policy will only allow preemption of MC jobs that negotiated under &#8220;&lt;none>&#8221;, as those were matched outside the accounting group&#8217;s quota.  So we see that jobs with RemoteNegotiatingGroup == &#8220;MC&#8221; remain un-preempted:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 MC | MC | 1
      5 MC | MC | 2
     10 R | R | 2
     20 total
</code></pre>

<p>The above demonstrates the pool-wide quota and concurrentcy limits for MC jobs.  To demonstrate per-machine resources, we start by clearing all jobs:</p>

<pre><code>$ condor_rm -all
</code></pre>

<p>Submit 20 &#8220;R&#8221; jobs (similar to above), and verify that they occupy all slots, including the slots with SlotTypeID == 1, which are reserved for MC jobs (but not currently being used):</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 R | &lt;none&gt; | 1
      5 R | &lt;none&gt; | 2
     10 R | R | 2
     20 total
</code></pre>

<p>Submit 10 MC jobs.  &#8220;MC.user&#8221; does not have sufficient priority to preempt &#8220;R.user&#8221;, however the slot rank expression <em>will</em> preempt non-MC jobs for an MC job on slots of type 1, and so we see that MC jobs <em>do</em> acquire the 5 type-1 slots reserved on this node:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 MC | MC | 1
      5 R | &lt;none&gt; | 2
     10 R | R | 2
     20 total
</code></pre>

<p>Finally, as an encore you can verify that jobs run against the MC accounting group must also charge against the MC_JOB concurrency limit, and non-MC jobs may not charge against it.  Again, start with an empty queue:</p>

<pre><code>$ condor_rm -all
</code></pre>

<p>Now, submit &#8216;bad&#8217; jobs that use accounting group &#8220;MC&#8221; but does not use the &#8220;mc_job&#8221; concurrency limits:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="MC.user"
queue 10
</code></pre>

<p>And likewise some &#8216;bad&#8217; regular jobs that attempt to use the &#8220;mc_job&#8221; concurrency limits:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
concurrency_limits = mc_job
+AccountingGroup="R.user"
queue 10
</code></pre>

<p>You should see that <em>none</em> of these jobs are allowed to run:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 undefined | undefined | 1
     15 undefined | undefined | 2
     20 total
$ cchist condor_q JobStatus
     20 1
     20 total
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deriving an Incremental Form of the Polynomial Regression Equations]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations/"/>
    <updated>2012-07-05T19:46:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations</id>
    <content type="html"><![CDATA[<p>Incremental, or on-line, algorithms are increasingly popular as data set sizes explode and web enabled applications create environments where new data arrive continuously (that is, incrementally) from clients out on the internet.</p>

<p>Recently I have been doing some <a href="https://github.com/erikerlandson/ratorade">experiments</a> with applying one of the <em>oldest</em> incremental algorithms to the task of rating predictions: computing a linear regression with a coefficient of correlation.  The incremental formulae look like this:</p>

<div markdown="0">
To find coefficients \( a_0, a_1 \) of the linear predictor \( y = a_0 + a_1 x \):
&#92;[
a_1 = &#92;frac {n &#92;Sigma x y - &#92;Sigma x &#92;Sigma y} {n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^2 }
&#92;hspace{1 cm}
a_0 = &#92;frac { &#92;Sigma y - a_1 &#92;Sigma x } {n}
&#92;]
The correlation coefficient of this predictor is given by:
&#92;[
&#92;rho (x,y) = &#92;frac {n &#92;Sigma x y - &#92;Sigma x &#92;Sigma y} {&#92;sqrt {n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^ 2 } &#92;sqrt {n &#92;Sigma y^2 - &#92;left( &#92;Sigma y &#92;right) ^ 2 } }
&#92;]
</div>


<p>As you can see from the formulae above, it is sufficient to maintain running sums</p>

<div markdown="0"> &#92;[ n, &#92;Sigma x, &#92;Sigma y, &#92;Sigma x^2, &#92;Sigma y^2, &#92;Sigma x y &#92;] </div>


<p>and so any new data can be included incrementally - that is, the model can be updated without revisiting any previous data.</p>

<p>Working with these models caused me to wonder if there was a way to generalize them to obtain incremental formulae for a quadratic predictor, or generalized polynomials.  As it happens, there is.  To show how, I&#8217;ll derive an incremental formula for the coefficients of the quadratic predictor:</p>

<div markdown="0">
&#92;[
y = a_0 + a_1 x + a_2 x^2
&#92;]
</div>


<p>Recall the <a href="http://en.wikipedia.org/wiki/Polynomial_regression#Matrix_form_and_calculation_of_estimates">matrix formula</a> for polynomial regression:</p>

<div markdown="0">
&#92;[ &#92;vec{a} = &#92;left( X^T X &#92;right) ^ {-1} X^T &#92;vec{y} &#92;]

where, in the quadratic case:

&#92;[
&#92;vec{a} = &#92;left( &#92;begin{array} {c}
a_0 &#92;&#92;
a_1 &#92;&#92;
a_2 &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
X = &#92;left( &#92;begin{array} {ccc}
1 & x_1 & x_1^2 &#92;&#92;
1 & x_2 & x_2^2 &#92;&#92;
  &  \vdots  & &#92;&#92;
1 & x_n & x_n^2 &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
&#92;vec{y} = &#92;left( &#92;begin{array} {c}
y_1 &#92;&#92;
y_2 &#92;&#92;
\vdots &#92;&#92;
y_n &#92;&#92;
&#92;end{array} &#92;right)
&#92;]

Note that we can apply the definition of matrix multiplication and express the two products &#92;( X^T X &#92;) and &#92;( X^T &#92;vec{y} &#92;) from the above formula like so:
&#92;[
X^T X = 
&#92;left( &#92;begin{array} {ccc}
n & &#92;Sigma x & &#92;Sigma x^2 &#92;&#92;
&#92;Sigma x & &#92;Sigma x^2 & &#92;Sigma x^3 &#92;&#92;
&#92;Sigma x^2 & &#92;Sigma x^3 & &#92;Sigma x^4 &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
X^T &#92;vec{y} =
&#92;left( &#92;begin{array} {c}
&#92;Sigma y &#92;&#92;
&#92;Sigma x y &#92;&#92;
&#92;Sigma x^2 y &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>And so now we can express the formula for our quadratic coefficients in this way:</p>

<div markdown="0">
&#92;[
&#92;left( &#92;begin{array} {c}
a_0 &#92;&#92;
a_1 &#92;&#92;
a_2 &#92;&#92;
&#92;end{array} &#92;right)
=
&#92;left( &#92;begin{array} {ccc}
n & &#92;Sigma x & &#92;Sigma x^2 &#92;&#92;
&#92;Sigma x & &#92;Sigma x^2 & &#92;Sigma x^3 &#92;&#92;
&#92;Sigma x^2 & &#92;Sigma x^3 & &#92;Sigma x^4 &#92;&#92;
&#92;end{array} &#92;right)
^ {-1}
&#92;left( &#92;begin{array} {c}
&#92;Sigma y &#92;&#92;
&#92;Sigma x y &#92;&#92;
&#92;Sigma x^2 y &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>Note that we now have a matrix formula that is expressed entirely in sums of various terms in x and y, which means that it can be maintained incrementally, as we desired.  If you have access to a matrix math package, you might very well declare victory right here, as you can easily construct these matrices and do the matrix arithmetic at will to obtain the model coefficients.  However, as an additional step I applied <a href="http://www.sagemath.org/">sage</a> to do the symbolic matrix inversion and multiplication to give:</p>

<div markdown="0">
&#92;[
&#92;small
a_0 =
&#92;frac {1} {Z}
&#92;left( 
- &#92;left( &#92;Sigma x^3 &#92;Sigma x - &#92;left( &#92;Sigma x^2 &#92;right)^2 &#92;right) &#92;Sigma x^2 y  +  &#92;left( &#92;Sigma x^4  &#92;Sigma x - &#92;Sigma x^3 &#92;Sigma x^2 &#92;right) &#92;Sigma x y  -  &#92;left( &#92;Sigma x^4 &#92;Sigma x^2 - &#92;left( &#92;Sigma x^3 &#92;right)^2 &#92;right) &#92;Sigma y 
&#92;right)
&#92;normalsize
&#92;]
&#92;[
&#92;small
a_1 =
&#92;frac {1} {Z}
&#92;left( 
&#92;left( n &#92;Sigma x^3  - &#92;Sigma x^2 &#92;Sigma x &#92;right) &#92;Sigma x^2 y  -  &#92;left( n &#92;Sigma x^4 - &#92;left( &#92;Sigma x^2 &#92;right) ^2 &#92;right) &#92;Sigma x y  +  &#92;left( &#92;Sigma x^4 &#92;Sigma x - &#92;Sigma x^3 &#92;Sigma x^2 &#92;right) &#92;Sigma y
&#92;right)
&#92;normalsize
&#92;]
&#92;[
&#92;small
a_2 =
&#92;frac {1} {Z}
&#92;left( 
- &#92;left( n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^2 &#92;right) &#92;Sigma x^2 y  +  &#92;left( n &#92;Sigma x^3 - &#92;Sigma x^2 &#92;Sigma x &#92;right) &#92;Sigma x y  -  &#92;left( &#92;Sigma x^3 &#92;Sigma x - &#92;left( &#92;Sigma x^2 &#92;right) ^2 &#92;right) &#92;Sigma y 
&#92;right)
&#92;normalsize
&#92;]
where:
&#92;[
Z = n &#92;left( &#92;Sigma x^3 &#92;right) ^ 2 - 2 &#92;Sigma x^3 &#92;Sigma x^2 &#92;Sigma x + &#92;left( &#92;Sigma x^2 &#92;right) ^3 - &#92;left( n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^2  &#92;right) &#92;Sigma x^4
&#92;]
</div>


<p>Inspecting the quadratic derivation above, it is now fairly easy to see that the general form of the incremental matrix formula for the coefficients of a degree-m polynomial looks like this:</p>

<div markdown="0">
&#92;[
&#92;left( &#92;begin{array} {c}
a_0 &#92;&#92;
a_1 &#92;&#92;
\vdots &#92;&#92;
a_m &#92;&#92;
&#92;end{array} &#92;right)
=
&#92;left( &#92;begin{array} {cccc}
n & &#92;Sigma x & &#92;cdots & &#92;Sigma x^m &#92;&#92;
&#92;Sigma x & &#92;Sigma x^2 & &#92;cdots & &#92;Sigma x^{m+1} &#92;&#92;
&#92;vdots & & &#92;ddots & &#92;vdots &#92;&#92;
&#92;Sigma x^m & &#92;Sigma x^{m+1} & &#92;cdots & &#92;Sigma x^{2 m} &#92;&#92;
&#92;end{array} &#92;right)
^ {-1}
&#92;left( &#92;begin{array} {c}
&#92;Sigma y &#92;&#92;
&#92;Sigma x y &#92;&#92;
&#92;vdots &#92;&#92;
&#92;Sigma x^m y &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>Having an incremental formula for generalized polynomial regression leaves open the question of how one might generalize the correlation coefficient.  There is such a generalization, called the <a href="http://en.wikipedia.org/wiki/Multiple_correlation">coefficient of multiple determination</a>, which is defined:</p>

<div markdown="0">
&#92;[
r = &#92;sqrt { &#92;vec{c} ^ T  R^{-1}  &#92;vec{c} }
&#92;]
Where
&#92;[
&#92;vec{c} = 
&#92;left ( &#92;begin{array} {c}
&#92;rho (x,y) &#92;&#92;
&#92;rho (x^2,y) &#92;&#92;
&#92;vdots &#92;&#92;
&#92;rho (x^m,y) &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
R =
&#92;left( &#92;begin{array} {cccc}
1 & &#92;rho (x,x^2) & &#92;cdots & &#92;rho(x,x^m) &#92;&#92;
&#92;rho (x^2,x) & 1 & &#92;cdots & &#92;rho(x^2,x^m) &#92;&#92;
&#92;vdots & & &#92;ddots & &#92;vdots &#92;&#92;
&#92;rho (x^m,x) & &#92;rho (x^m,x^2) & &#92;cdots & 1 &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
and &#92;( &#92;rho (x,y) &#92;) is the traditional pairwise correlation coefficient.
</div>


<p>But we already have an incremental formula for any pairwise correlation coefficient, which is defined above.  And so we can maintain the running sums needed to fill the matrix entries, and compute the coefficient of multiple determination for our polynomial model at any time.</p>

<p>So we now have incremental formulae to maintain any polynomial model in an on-line environment where we either can&#8217;t or prefer not to store the data history, and also incrementally evaluate the &#8216;generalized correlation coefficient&#8217; for that model.</p>

<p>Readers familiar with linear regression may notice that there is also nothing special about polynomial regression, in the sense that powers of x may also be replaced with arbitrary functions of x, and the same regression equations hold.  And so we might generalize the incremental matrix formulae further to replace products of powers of x with products of functions of x:</p>

<div markdown="0">
for a linear regression model &#92;( y = a_1 f_1 (x) + a_2 f_2 (x) + &#92;cdots + a_m f_m(x) &#92;) :
&#92;[
&#92;left( &#92;begin{array} {c}
a_1 &#92;&#92;
a_2 &#92;&#92;
\vdots &#92;&#92;
a_m &#92;&#92;
&#92;end{array} &#92;right)
=
&#92;left( &#92;begin{array} {cccc}
&#92;Sigma f_1 (x) f_1 (x) & &#92;Sigma f_1 (x) f_2 (x) & &#92;cdots & &#92;Sigma f_1 (x) f_m (x) &#92;&#92;
&#92;Sigma f_2 (x) f_1 (x) & &#92;Sigma f_2 (x) f_2 (x) & &#92;cdots & &#92;Sigma f_2 (x) f_m (x) &#92;&#92;
&#92;vdots & & &#92;ddots & &#92;vdots &#92;&#92;
&#92;Sigma f_m (x) f_1 (x) & &#92;Sigma f_m (x) f_2 (x) & &#92;cdots & &#92;Sigma f_m (x) f_m (x) &#92;&#92;
&#92;end{array} &#92;right)
^ {-1}
&#92;left( &#92;begin{array} {c}
&#92;Sigma y f_1 (x) &#92;&#92;
&#92;Sigma y f_2 (x) &#92;&#92;
&#92;vdots &#92;&#92;
&#92;Sigma y f_m (x) &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>The coefficient of multiple determination generalizes in the analogous way.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Easy Histograms and Tables from Condor Jobs and Slots]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/"/>
    <updated>2012-06-29T09:46:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots</id>
    <content type="html"><![CDATA[<p>Several <a href="http://research.cs.wisc.edu/condor/">Condor</a> commands, including condor_status, condor_q and condor_history, provide a nice feature for outputting formatted subsets of classad attributes: the <code>-format &lt;format&gt; &lt;attr&gt;</code> option.  In this post, I assume basic familiarity with <code>-format</code>.  You can read more <a href="http://research.cs.wisc.edu/condor/manual/v7.8/condor_status.html#SECTION0011453000000000000000">here</a></p>

<p>The <code>-format</code> option can be used to generate tables and histograms of attributes, in a classic &#8216;unix one-liner&#8217; fashion.  For example, supposing I wanted to use condor_status to create a nice histogram of the values for slot type, state, activity and accounting group.  I might issue a one-liner like this:</p>

<pre><code>$ condor_status -format "%s" 'ifThenElse(SlotType =!= undefined, string(SlotType), "undefined")' \
&gt; -format " | %s" 'ifThenElse(State =!= undefined, string(State), "undefined")' \
&gt; -format " | %s" 'ifThenElse(Activity =!= undefined, string(Activity), "undefined")' \
&gt; -format " | %s\n" 'ifThenElse(AccountingGroup =!= undefined, string(AccountingGroup), "undefined")' \
&gt; | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
      3 Static | Claimed | Busy | A.user@localdomain
      2 Static | Claimed | Busy | B.user@localdomain
     10 Static | Unclaimed | Idle | undefined
     15 total
</code></pre>

<p>Note that in this command I was extra pedantic and careful about converting expressions to strings, and using the ClassAd ifThenElse to trap and handle possible undefined values (which do indeed occur for AccountingGroup, when a slot is not in use).</p>

<p>We can see that a lot of this would benefit from some programmatic automation.  To that end I wrote some <a href="https://github.com/erikerlandson/bash_condor_tools">convenience bash functions</a> for automating the tedious portions of this process: <code>cchist</code>, <code>ccsort</code> and <code>ccdump</code>.  For example I could use <code>cchist</code> to generate the histogram from the example above much more cleanly:</p>

<pre><code>$ cchist condor_status SlotType State Activity AccountingGroup
      3 Static | Claimed | Busy | A.user@localdomain
      2 Static | Claimed | Busy | B.user@localdomain
     10 Static | Unclaimed | Idle | undefined
     15 total
</code></pre>

<p>The <code>ccdump</code> command simply dumps the table of values, uncollated, while <code>ccsort</code> outputs the table of values, but sorted:</p>

<pre><code>$ ccdump condor_status SlotType State Activity AccountingGroup
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | B.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Claimed | Busy | A.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Claimed | Busy | B.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
$ ccsort condor_status SlotType State Activity AccountingGroup
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | B.user@localdomain
Static | Claimed | Busy | B.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
</code></pre>

<p>If you are interested in providing the actual raw unix command that was executed, you can use the <code>-cmd</code> option (note, this currently must appear <em>first</em>)</p>

<pre><code>$ cchist -cmd condor_status SlotType State Activity AccountingGroup
condor_status -format "%s" 'ifThenElse(SlotType isnt undefined, string(SlotType), "undefined")' -format " | %s" 'ifThenElse(State isnt undefined, string(State), "undefined")' -format " | %s" 'ifThenElse(Activity isnt undefined, string(Activity), "undefined")' -format " | %s\n" 'ifThenElse(AccountingGroup isnt undefined, string(AccountingGroup), "undefined")' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
</code></pre>

<p>As you can see, the command condor_status is a parameter.  You can also use the same commands with condor_q and condor_history:</p>

<pre><code>$ cchist condor_q AccountingGroup LastJobStatus
      3 A.user | 1
      2 B.user | 1
      5 total
$ cchist condor_history AccountingGroup LastJobStatus
     18 A.user | 2
     26 B.user | 2
     20 C.user | 2
     64 total
</code></pre>

<p>You can obtain cchist and friends at the <a href="https://github.com/erikerlandson/bash_condor_tools">bash_condor_tools github repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maintaining Accounting Group Quotas With Preemption Policy]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/06/27/maintaining-accounting-group-quotas-with-preemption-policy/"/>
    <updated>2012-06-27T20:33:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/06/27/maintaining-accounting-group-quotas-with-preemption-policy</id>
    <content type="html"><![CDATA[<p>There is a straightforward technique to leverage a Condor <a href="http://research.cs.wisc.edu/condor/manual/v7.8/3_3Configuration.html#20480">preemption policy</a> to direct preemptions in a way that helps maintain resource usages as close as possible to the defined <a href="http://research.cs.wisc.edu/condor/manual/v7.8/3_4User_Priorities.html#SECTION00447000000000000000">accounting group quotas</a>.</p>

<p>I will begin by simply giving the configuration and then describe how it works, with a short demonstration.  The actual configuration is simply a clause that can be added to the preemption policy defined by <code>PREEMPTION_REQUIREMENTS</code>:</p>

<pre><code>PREEMPTION_REQUIREMENTS = $(PREEMPTION_REQUIREMENTS) &amp;&amp; (((SubmitterGroupResourcesInUse &lt; SubmitterGroupQuota) &amp;&amp; (RemoteGroupResourcesInUse &gt; RemoteGroupQuota)) || (SubmitterGroup =?= RemoteGroup))
</code></pre>

<p>Unpacking the above logic: the term <code>(SubmitterGroupResourcesInUse &lt; SubmitterGroupQuota)</code> captures the idea that to best maintain quota-driven resource usage, we only want to allow preemption if the submitting accounting group has not yet reached its quota, as acquiring more resources moves the usage closer to the group&#8217;s quota.  Conversely, if the accounting group&#8217;s resource usage is <em>already</em> at or above its quota, acquiring more resources via preemption will only drive the usage <em>farther</em> from the configured quota.</p>

<p>The term <code>(RemoteGroupResourcesInUse &gt; RemoteGroupQuota)</code> captures a similar idea from the &#8216;remote&#8217; side (the candidate for preemption).  Provided the remote&#8217;s resource usage is greater than its quota, allowing preemption will move its usage closer to the configured quota.</p>

<p>The last term <code>(SubmitterGroup =?= RemoteGroup)</code> (a disjunction) ensures that with an accounting group preemption may always occur, deferring to any other clauses in the expression.</p>

<p>A brief aside: in the following example, I use the &#8216;svhist&#8217; bash function for ease and clarity.  For example, the command <code>svhist AccountingGroup State Activity</code> is shorthand for: <code>condor_status -format "%s" 'AccountingGroup' -format " | %s" 'State' -format " | %s\n" 'Activity' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'</code>  The svhist command is available <a href="https://github.com/erikerlandson/bash_condor_tools">here</a>.</p>

<p>To demonstrate this preemption policy, consider the following example configuration:</p>

<pre><code># turn off scheduler optimizations, as they can sometimes obscure the
# negotiator/matchmaker behavior
CLAIM_WORKLIFE = 0

# for demonstration purposes, make sure basic preemption knobs are 'on'
MAXJOBRETIREMENTTIME = 0
PREEMPTION_REQUIREMENTS = True
NEGOTIATOR_CONSIDER_PREEMPTION = True

NUM_CPUS = 15

# 3 accounting groups, each with equal quota
GROUP_NAMES = A, B, C
GROUP_QUOTA_A = 5
GROUP_QUOTA_B = 5
GROUP_QUOTA_C = 5

# groups may use each others' surplus
GROUP_ACCEPT_SURPLUS = TRUE
# (an alternative way for groups to acquire surplus is to enable GROUP_AUTOREGROUP)
# GROUP_AUTOREGROUP = TRUE

# A preepmption policy clause that only allows preemptions that move usages closer to configured quotas
PREEMPTION_REQUIREMENTS = $(PREEMPTION_REQUIREMENTS) &amp;&amp; (((SubmitterGroupResourcesInUse &lt; SubmitterGroupQuota) &amp;&amp; (RemoteGroupResourcesInUse &gt; RemoteGroupQuota)) || (SubmitterGroup =?= RemoteGroup))
</code></pre>

<p>Begin by submitting 5 jobs to accounting group A, and 10 jobs to group B:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 3600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="A.user"
queue 5
+AccountingGroup="B.user"
queue 10
</code></pre>

<p>Confirm that group B&#8217;s resource usage is 10 (note, this is over its quota of 5):</p>

<pre><code>$ svhist AccountingGroup State Activity
      5 A.user@localdomain | Claimed | Busy
     10 B.user@localdomain | Claimed | Busy
     15 total
</code></pre>

<p>Now set submitter priorities to allow preemption, provided preemption policy supports it</p>

<pre><code>$ condor_userprio -setprio A.user@localdomain 10
The priority of A.user@localdomain was set to 10.000000
$ condor_userprio -setprio B.user@localdomain 10
The priority of B.user@localdomain was set to 10.000000
</code></pre>

<p>Now submit 10 jobs to group C.</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 3600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="C.user"
queue 10
</code></pre>

<p>Finally, we verify that our preemption policy drove the resource usages to quota:</p>

<pre><code>$ svhist AccountingGroup State Activity
      5 A.user@localdomain | Claimed | Busy
      5 B.user@localdomain | Claimed | Busy
      5 C.user@localdomain | Claimed | Busy
     15 total
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Joy of Anonymized Data]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/05/20/the-joy-of-anonymized-data/"/>
    <updated>2012-05-20T11:31:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/05/20/the-joy-of-anonymized-data</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been fooling around with an <a href="https://github.com/erikerlandson/ratorade/tree/master/data">anonymized data set</a> on the side.  Although this can be frustrating in its own way, it occurred to me that it <em>does</em> have the advantage of forcing me to see the data in the same way my algorithms see it: that is, the data is just some anonymous strings, values and identifiers.  To the code, strings like &#8220;120 minute IPA&#8221; or &#8220;Dogfish Head Brewery&#8221; have no more significance than &#8220;Beer-12&#8221; or &#8220;Brewer-5317&#8221;, and the anonymous identifiers remove any subconscious or conscious tendencies of mine to impart more meaning to an identifier string than is present to the algorithms.</p>

<p>On the other hand, having anonymous identifiers prevents me from drawing any actual inspirations for utilizing semantics that <em>might</em> genuinely be leveragable by an algorithm.  However, my current goal is to produce tools that are generically useful across data domains.  In that respect, I think developing on anonymized data could actually be helping.  Time will tell.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pretty Good Random Sampling from Database Queries]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/05/16/pretty-good-random-sampling-from-database-queries/"/>
    <updated>2012-05-16T07:05:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/05/16/pretty-good-random-sampling-from-database-queries</id>
    <content type="html"><![CDATA[<p>Suppose you want to add random sampling to a database query, but your database does not support it.  One known technique is to add a field, say &#8220;rk&#8221;, that contains a random key value in [0,1), index on that field, and add a clause to the query:  <code>("rk" &gt;= x  &amp;&amp;  "rk" &lt; x+p)</code>, where p is your desired random sampling probability and x is randomly chosen from [0,1-p).</p>

<p>This is not bad, but we can see it is not <em>truly</em> randomized, as the sliding window [x,x+p) over the &#8220;rk&#8221; random key field generates overlap in the samplings.  The larger the value of p, the more significant the overlapping effect will be.</p>

<p>Eliminating this effect absolutely (and maintaining query efficiency) is difficult without direct database support, however we can take steps to significantly reduce it.  Suppose we generated <em>two</em> independently randomized keys &#8220;rk0&#8221; and &#8220;rk1&#8221;.  We could sample using a slightly more complex clause: <code>(("rk0" &gt;= x0  &amp;&amp; "rk0" &lt; x0+d) || ("rk1" &gt;= x1  &amp;&amp;  "rk1" &lt; x1+d))</code>, where x0 and x1 are randomly selected from [0,1-d).</p>

<p>What value do we use for d to maintain a random sampling factor of p?  As &#8220;rk0&#8221; and &#8220;rk1&#8221; are independent random variables, the effective sampling factor p is given by p = d + d - d<sup>2,</sup> where the d<sup>2</sup> accounts for query results present in both the &#8220;rk0&#8221; and &#8220;rk1&#8221; subqueries.  Applying the quadratic formula to solve for d gives us: d = 1-sqrt(1-p).</p>

<p>This approach should be useable with any database.  Here is example code I wrote for generating the random sampling portion of a mongodb query in pymongo:</p>

<pre><code>def random_sampling_query(p, rk0="rk0", rk1="rk1", pad = 0):
    d = (1.0 - sqrt(1.0-p)) * (1.0 + pad)
    if d &gt; 1.0: d = 1.0
    if d &lt; 0.0: d = 0.0
    s0 = random.random()*(1.0 - d)
    s1 = random.random()*(1.0 - d)
    return {"$or":[{rk0:{"$gte":s0, "$lt":s0+d}}, {rk1:{"$gte":s1, "$lt":s1+d}}]}
</code></pre>

<p>I included an optional &#8216;pad&#8217; parameter to support a case where one might want a particular (integer) sample size s, and so set p = s/(db-table-size), and use padding to mitigate the probability of getting less than s records due to random sampling jitter.  In mongodb one could then append <code>limit(s)</code> to the query return, and get exactly s returns in most cases, with the correct padding.</p>

<p>Here is a pymongo example of using the <code>random_sampling_query()</code> above:</p>

<pre><code># get a query that does random sampling of 1% of the results:
query = random_sampling_query(0.01)
# other query clauses can be added if desired:
query[user] = "eje"
# issue the final query to get results with random sampling:
qres = data.find(query)
</code></pre>

<p>One could extend the logic above by using 3 independent random fields rk0,rk1,rk2 and applying the cubic formula, or four fields and the quartic formula, but I suspect that is passing the point of diminishing returns on storage cost, query cost and algebra.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interaction between mktime() and tm_isdst - a compute cycle landmine]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/03/19/interaction-between-mktime-and-tm-isdst-a-compute-cycle-landmine/"/>
    <updated>2012-03-19T13:18:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/03/19/interaction-between-mktime-and-tm-isdst-a-compute-cycle-landmine</id>
    <content type="html"><![CDATA[<p>I was recently profiling the <a href="http://research.cs.wisc.edu/condor/">Condor</a> collector, and was a bit stunned to discover that the standard C library function <a href="http://www.cplusplus.com/reference/clibrary/ctime/mktime/">mktime</a>() was burning <em>60% of the collector&#8217;s cycles</em>.</p>

<p><a href="http://spinningmatt.wordpress.com/">Matt</a> helpfully attempted to reproduce, but his profile showed <code>mktime()</code> using almost none of the cycles, which is exactly the sane result one would expect.</p>

<p>In the code, I noticed that <code>tm_isdst</code> was set to 1, in other words &#8220;assert that DST is in effect.&#8221;  This made my eye twitch, because I live in Arizona, where we boldy do not observe DST.  I created a little test rig to help confirm my suspicion that time zone might have something to do with it:</p>

<pre><code>#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;
#include &lt;iostream&gt;

using std::cout;

time_t mktA(struct tm* tmp) {
    tmp-&gt;tm_isdst = -1;
    return mktime(tmp);
}

time_t mktB(struct tm* tmp) {
    tmp-&gt;tm_isdst = 0;
    return mktime(tmp);
}

time_t mktC(struct tm* tmp) {
    tmp-&gt;tm_isdst = 1;
    return mktime(tmp);
}

int main(int argc, char** argv) {
    struct tm stm;
    stm.tm_year = 2012 - 1900;
    stm.tm_mon = 3-1;
    stm.tm_mday = 17-1;
    stm.tm_hour = 0;
    stm.tm_min = 0;
    stm.tm_sec = 0;

    // this gets altered for each testing function:
    stm.tm_isdst = 0;

    cout &lt;&lt; mktA(&amp;stm) &lt;&lt; "\n";
    cout &lt;&lt; mktB(&amp;stm) &lt;&lt; "\n";
    cout &lt;&lt; mktC(&amp;stm) &lt;&lt; "\n";

    return 0;
}
</code></pre>

<p>Then I built the test rig, which I expertly named <code>test_mktime</code>, and profiled it using valgrind/callgrind:</p>

<pre><code># build the test rig
$ make test_mktime
g++     test_mktime.cpp   -o test_mktime

# profile using valgrind/callgrind:
$ valgrind --tool=callgrind ./test_mktime
==2671== Callgrind, a call-graph generating cache profiler
==2671== Copyright (C) 2002-2009, and GNU GPL'd, by Josef Weidendorfer et al.
==2671== Using Valgrind-3.5.0 and LibVEX; rerun with -h for copyright info
==2671== Command: ./test_mktime
==2671== 
==2671== For interactive control, run 'callgrind_control -h'.
1331881200
1331881200
1331881200
==2671== 
==2671== Events    : Ir
==2671== Collected : 4125723
==2671== 
==2671== I   refs:      4,125,723

# massage the raw output into something (more or less) human readable:
$ callgrind_annotate --inclusive=yes --tree=calling callgrind.out.2671 &gt; mktprof.txt
</code></pre>

<p>Examining the massaged output in <code>mktprof.txt</code>, I observed that calling <code>mktime()</code> with <code>tm_isdst = {-1|0}</code> (<code>mktA()</code> and <code>mktB()</code>) takes the small amount of time one would expect, calling with <code>tm_isdst = 1</code> (<code>mktC()</code>) uses a completely insane number of cycles, and clearly nearly all of the cycles burned by the test rig:</p>

<pre><code>2,749,933  *  ???:main [/home/eje/mktime/test_mktime]
2,655,457  &gt;   ???:mktC(tm*) (1x) [/home/eje/mktime/test_mktime]
    4,428  &gt;   ???:std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*) (3x) [/usr/lib64/libstdc++.so.6.0.13]
   11,260  &gt;   ???:std::ostream::operator&lt;&lt;(long) (3x) [/usr/lib64/libstdc++.so.6.0.13]
    4,064  &gt;   ???:mktB(tm*) (1x) [/home/eje/mktime/test_mktime]
    3,989  &gt;   ???:_dl_runtime_resolve (2x) [/lib64/ld-2.11.2.so]
   74,212  &gt;   ???:mktA(tm*) (1x) [/home/eje/mktime/test_mktime]
</code></pre>

<p>Again, Matt verified that he could reproduce the weird behavior if he set <em>his</em> timezone to &#8220;Arizona&#8221;.</p>

<p>The bottom line appears to be that invoking <code>mktime()</code> with <code>tm_isdst = 1</code>, in a time zone that does not observe DST, can set off a nuclear cycle-stealing land mine of inefficiency and horror.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Don't try to stop me. I'm on a rampage.]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/03/19/dont-try-to-stop-me/"/>
    <updated>2012-03-19T12:08:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/03/19/dont-try-to-stop-me</id>
    <content type="html"><![CDATA[<p>In which I join the 21st century, kicking and screaming.</p>
]]></content>
  </entry>
  
</feed>
