<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tool monkey]]></title>
  <link href="http://erikerlandson.github.com/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2016-12-05T19:31:09-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Encoding Map-Reduce As A Monoid With Left Folding]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid/"/>
    <updated>2016-09-05T10:31:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid</id>
    <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/">previous post</a> I discussed some scenarios where traditional map-reduce (directly applying a map function, followed by some monoidal reduction) could be inefficient.
To review, the source of inefficiency is in situations where the <code>map</code> operation is creating some non-trivial monoid that represents a single element of the input type.
For example, if the monoidal type is <code>Set[Int]</code>, then the mapping function (&#8216;prepare&#8217; in algebird) maps every input integer <code>k</code> into <code>Set(k)</code>, which is somewhat expensive.</p>

<p>In that discussion, I was focusing on map-reduce as embodied by the algebird <code>Aggregator</code> type, where <code>map</code> appears as the <code>prepare</code> function.
However, it is easy to see that <em>any</em> map-reduce implementation may be vulnerable to the same inefficiency.</p>

<p>I wondered if there were a way to represent map-reduce using some alternative formulation that avoids this vulnerability.
There is such a formulation, which I will talk about in this post.</p>

<p>I&#8217;ll begin by reviewing a standard map-reduce implementation.
The following scala code sketches out the definition of a monoid over a type <code>B</code> and a map-reduce interface.
As this code suggests, the <code>map</code> function maps input data of some type <code>A</code> into some <em>monoidal</em> type <code>B</code>, which can be reduced (aka &#8220;aggregated&#8221;) in a way that is amenable to parallelization:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">Monoid</span><span class="o">[</span><span class="kt">B</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// aka &#39;combine&#39; aka &#39;++&#39;</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">plus</span><span class="k">:</span> <span class="o">(</span><span class="kt">B</span><span class="o">,</span> <span class="kt">B</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">B</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// aka &#39;empty&#39; aka &#39;identity&#39;</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">e</span><span class="k">:</span> <span class="kt">B</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">trait</span> <span class="nc">MapReduce</span><span class="o">[</span><span class="kt">A</span>, <span class="kt">B</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// monoid embodies the reducible type</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">monoid</span><span class="k">:</span> <span class="kt">Monoid</span><span class="o">[</span><span class="kt">B</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// mapping function from input type A to reducible type B</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">map</span><span class="k">:</span> <span class="kt">A</span> <span class="o">=&gt;</span> <span class="n">B</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// the basic map-reduce operation</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">map</span><span class="o">).</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// map-reduce parallelized over data partitions</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">ParSeq</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">]])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span>
</span><span class='line'>    <span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">part</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="n">part</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">map</span><span class="o">).</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>In the parallel version of map-reduce above, you can see that map and reduce are executed on each data partition (which may occur in parallel) to produce a monoidal <code>B</code> value, followed by a final reduction of those intermediate results.
This is the classic form of map-reduce popularized by tools such as Hadoop and Apache Spark, where inidividual data partitions may reside across highly parallel commodity clusters.</p>

<p>Next I will present an alternative definition of map-reduce.
In this implementation, the <code>map</code> function is replaced by a <code>foldL</code> function, which executes a single &#8220;left-fold&#8221; of an input object with type <code>A</code> into the monoid object with type <code>B</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// a map reduce operation based on a monoid with left folding</span>
</span><span class='line'><span class="k">trait</span> <span class="nc">MapReduceLF</span><span class="o">[</span><span class="kt">A</span>, <span class="kt">B</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">MapReduce</span><span class="o">[</span><span class="kt">A</span>, <span class="kt">B</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">monoid</span><span class="k">:</span> <span class="kt">Monoid</span><span class="o">[</span><span class="kt">B</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// left-fold an object with type A into the monoid B</span>
</span><span class='line'>  <span class="c1">// obeys type law: foldL(b, a) = b ++ foldL(e, a)</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">foldL</span><span class="k">:</span> <span class="o">(</span><span class="kt">B</span><span class="o">,</span> <span class="kt">A</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">B</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// foldL(e, a) embodies the role of map(a) in standard map-reduce</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">map</span> <span class="k">=</span> <span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">A</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">foldL</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// map-reduce operation is now a single fold-left operation</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">foldL</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// map-reduce parallelized over data partitions</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">ParSeq</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">]])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span>
</span><span class='line'>    <span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">part</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="n">part</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">foldL</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As the comments above indicate, the left-folding function <code>foldL</code> is assumed to obey the law <code>foldL(b, a) = b ++ foldL(e, a)</code>.
This law captures the idea that folding <code>a</code> into <code>b</code> should be the analog of reducing <code>b</code> with a monoid corresponding to the single element <code>a</code>.
Referring to my earlier example, if type <code>A</code> is <code>Int</code> and <code>B</code> is <code>Set[Int]</code>, then <code>foldL(b, a) =&gt; b + a</code>.
Note that <code>b + a</code> is directly inserting single element <code>a</code> into <code>b</code>, which is significantly more efficient than <code>b ++ Set(a)</code>, which is how a typical map-reduce implementation would be required to operate.</p>

<p>This law also gives us the corresponding definition of <code>map(a)</code>, which is <code>foldL(e, a)</code>, or in my example: <code>Set.empty[Int] ++ a</code> or just: <code>Set(a)</code></p>

<p>In this formulation, the basic map-reduce operation is now a single <code>foldLeft</code> operation, instead of a mapping followed by a monoidal reduction.
The parallel version is analoglous.
Each partition uses the new <code>foldLeft</code> operation, and the final reduction of intermediate monoidal results remains the same as before.</p>

<p>The <code>foldLeft</code> function is potentially a much more general operation, and it raises the question of whether this new encoding is indeed parallelizable as before.
I will conclude with a proof that this encoding is also parallelizable;
Note that the law <code>foldL(b, a) = b ++ foldL(e, a)</code> is a significant component of this proof, as it represents the constraint that <code>foldL</code> behaves like an analog of reducing <code>b</code> with a monoidal representation of element <code>a</code>.</p>

<p>In the following proof I used a scala-like pseudo code, described in the introduction:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// given an object mr of type MapReduceFL[A, B]</span>
</span><span class='line'><span class="c1">// and using notation:</span>
</span><span class='line'><span class="c1">// f &lt;==&gt; mr.foldL</span>
</span><span class='line'><span class="c1">// for b1,b2 of type B: b1 ++ b2 &lt;==&gt; mr.plus(b1, b2)</span>
</span><span class='line'><span class="c1">// e &lt;==&gt; mr.e</span>
</span><span class='line'><span class="c1">// [...] &lt;==&gt; Seq(...)</span>
</span><span class='line'><span class="c1">// d1, d2 are of type Seq[A]</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Proof that map-reduce with left-folding is parallelizable</span>
</span><span class='line'><span class="c1">// i.e. mr(d1 ++ d2) == mr(d1) ++ mr(d2)</span>
</span><span class='line'><span class="n">mr</span><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of map-reduce operation</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// Lemma A</span>
</span><span class='line'><span class="o">==</span> <span class="n">mr</span><span class="o">(</span><span class="n">d1</span><span class="o">)</span> <span class="o">++</span> <span class="n">mr</span><span class="o">(</span><span class="n">d2</span><span class="o">)</span>  <span class="c1">// definition of map-reduce (QED)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Proof of Lemma A</span>
</span><span class='line'><span class="c1">// i.e. (d1 ++ d2).foldLeft(e)(f) == d1.foldLeft(e)(f) ++ d2.foldLeft(e)(f)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// proof is by induction on the length of data sequence d2</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// case d2 where length is zero, i.e. d2 == []</span>
</span><span class='line'><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="o">[]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of empty sequence []</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">e</span>  <span class="c1">// definition of identity e</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">[].</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of foldLeft</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// case d2 where length is 1, i.e. d2 == [a] for some a of type A</span>
</span><span class='line'><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="n">f</span><span class="o">(</span><span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">),</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// definition of foldLeft and f</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">f</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// the type-law f(b, a) == b ++ f(e, a)</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">].</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of foldLeft</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// inductive step, assuming proof for d2&#39; of length &lt;= n</span>
</span><span class='line'><span class="c1">// consider d2 of length n+1, i.e. d2 == d2&#39; ++ [a], where d2&#39; has length n</span>
</span><span class='line'><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of d2, d2&#39;, [a]</span>
</span><span class='line'><span class="o">==</span> <span class="n">f</span><span class="o">((</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">),</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// definition of foldLeft and f</span>
</span><span class='line'><span class="o">==</span> <span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">f</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// type-law f(b, a) == b ++ f(e, a)</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">f</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// induction</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">].</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// def&#39;n of foldLeft</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">(</span><span class="n">d2</span><span class="err">&#39;</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// induction</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of d2 (QED)</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Supporting Competing APIs in Scala -- Can Better Package Factoring Help?]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/08/31/supporting-competing-apis-in-scala-can-better-package-factoring-help/"/>
    <updated>2016-08-31T17:55:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/08/31/supporting-competing-apis-in-scala-can-better-package-factoring-help</id>
    <content type="html"><![CDATA[<p> On and off over the last year, I&#8217;ve been working on a <a href="http://erikerlandson.github.io/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/">library</a> of tree and map classes in Scala that happen to make use of some algebraic structures (mostly monoids or related concepts).
 In my initial implementations, I made use of the popular <a href="https://github.com/twitter/algebird">algebird</a> variations on monoid and friends.
 In their incarnation as an <a href="https://github.com/twitter/algebird/pull/496">algebird PR</a> this was uncontroversial to say the least, but lately I have been re-thinking them as a <a href="https://github.com/isarn/isarn/pull/1">third-party Scala package</a>.</p>

<p>This immediately raised some interesting and thorny questions:
in an ecosystem that contains not just <a href="https://github.com/twitter/algebird">algebird</a>, but other popular alternatives such as <a href="https://github.com/typelevel/cats">cats</a> and <a href="https://github.com/scalaz/scalaz">scalaz</a>, what algebra API should I use in my code?
How best to allow the library user to interoperate with the algebra libray of their choice?
Can I accomplish these things while also avoiding any problematic package dependencies in my library code?</p>

<p>In Scala, the second question is relatively straightforward to answer.
I can write my interface using <a href="http://docs.scala-lang.org/tutorials/tour/implicit-conversions">implicit conversions</a>, and provide sub-packages that provide such conversions from popular algebra libraries into the library I actually use in my code.
A library user can import the predefined implicit conversions of their choice, or if necessary provide their own.</p>

<p>So far so good, but that leads immediately back to the first question &#8211; what API should <strong><em>I</em></strong> choose to use internally in my own library?</p>

<p>One obvious approach is to just pick one of the popular options (I might favor <code>cats</code>, for example) and write my library code using that.
If a library user also prefers <code>cats</code>, great.
Otherwise, they can import the appropritate implicit conversions from their favorite alternative into <code>cats</code> and be on their way.</p>

<p>But this solution is not without drawbacks.
Anybody using my library will now be including <code>cats</code> as a transitive dependency in their project, even if they are already using some other alternative.
Although <code>cats</code> is not an enormous library, that represents a fair amount of code sucked into my users&#8217; projects, most of which isn&#8217;t going to be used at all.
More insidiously, I have now introduced the possiblity that the <code>cats</code> version I package with is out of sync with the version my library users are building against.
Version misalignment in transitive dependencies is a land-mine in project builds and very difficult to resolve.</p>

<p>A second approach I might use is to define some abstract algebraic traits of my own.
I can write my libraries in terms of this new API, and then provide implicit conversions from popular APIs into mine.</p>

<p>This approach has some real advantages over the previous.  Being entirely abstract, my internal API will be lightweight.  I have the option of including only the algebraic concepts I need.  It does not introduce any possibly problematic 3rd-party dependencies that might cause code bloat or versioning problems for my library users.</p>

<p>Although this is an effective solution, I find it dissatisfying for a couple reasons.
Firstly, my new internal API effectively represents <em>yet another competing algebra API</em>, and so I am essentially contributing to the proliferating-standards antipattern.</p>

<p><img src="https://imgs.xkcd.com/comics/standards.png" alt="standards" /></p>

<p>Secondly, it means that I am not taking advantage of community knowledge.
The <code>cats</code> library embodies a great deal of cumulative human expertise in both category theory and Scala library design.
What does a good algebra library API look like?
Well, <em>it&#8217;s likely to look a lot like <code>cats</code></em> of course!
The odds that I end up doing an inferior job designing my little internal vanity API are rather higher than the odds that I do as well or better.
The best I can hope for is to re-invent the wheel, with a real possibility that my wheel has corners.</p>

<p>Is there a way to resolve this unpalatable situation?
Can we design our projects to both remain flexible about interfacing with multiple 3rd-party alternatives, but avoid effectively writing <em>yet another alternative</em> for our own internal use?</p>

<p>I hardly have any authoritative answers to this problem, but I have one idea that might move toward a solution.
As I alluded to above, when I write my libraries, I am most frequently <em>only</em> interested in the API &#8211; the abstract interface.
If I did go with writing my own algebra API, I would seek to define purely abstract traits.
Since my intention is that my library users would supply their own favorite library alternative, I would have no need or desire to instantiate any of my APIs.
That function would be provided by the separate sub-projects that provide implicit conversions from community alternatives into my API.</p>

<p>On the other hand, what if <code>cats</code> and <code>algebird</code> factored <em>their</em> libraries in a similar way?
What if I could include a sub-package like <code>cats-kernel-api</code>, or <code>algebird-core-api</code>, which contained <em>only</em> pure abstract traits for monoid, semigroup, etc?
Then I could choose my favorite community API, and code against it, with much less code bloat, and a much reduced vulnerability to any versioning drift.
I would still be free to provide implicit conversions and allow <em>my</em> users to make their own choice of library in their projects.</p>

<p>Although I find this idea attractive, it is certainly not foolproof.
For example, there is never a way to <em>guarantee</em> that versioning drift won&#8217;t break an API.
APIs such as <code>cats</code> and <code>algebird</code> are likely to be unusually amenable to this kind of approach.
After all, their interfaces are primarily driven by underlying mathematical definitions, which are generally as stable as such things ever get.
However, APIs in general tend to be significantly more stable than underlying code.
And the most-stable subsets of APIs might be encoded as traits and exposed this way, allowing other more experimental API components to change at a higher frequency.
Perhaps library packages could even be factored in some way such as <code>library-stable-api</code> and <code>library-unstable-api</code>.
That would clearly add a bit of complication to library trait hierarchies, but the payoff in terms of increased 3rd-party usability might be worth it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Minimum Description Length to Optimize the 'K' in K-Medoids]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/08/03/x-medoids-using-minimum-description-length-to-identify-the-k-in-k-medoids/"/>
    <updated>2016-08-03T20:00:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/08/03/x-medoids-using-minimum-description-length-to-identify-the-k-in-k-medoids</id>
    <content type="html"><![CDATA[<p>Applying many popular clustering models, for example <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-Means</a>, <a href="https://en.wikipedia.org/wiki/K-medoids">K-Medoids</a> and <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture">Gaussian Mixtures</a>, requires an up-front choice of the number of clusters &#8211; the &#8216;K&#8217; in K-Means, as it were.
Anybody who has ever applied these models is familiar with the inconvenient task of guessing what an appropriate value for K might actually be.
As the size and dimensionality of data grows, estimating a good value for K rapidly becomes an exercise in wild guessing and multiple iterations through the free-parameter space of possible K values.</p>

<p>There are some varied approaches in the community for addressing the task of identifying a good number of clusters in a data set.  In this post I want to focus on an approach that I think deserves more attention than it gets: <a href="https://en.wikipedia.org/wiki/Minimum_description_length">Minimum Description Length</a>.</p>

<p>Many years ago I ran across a <a href="#cite1">superb paper</a> by Stephen J. Roberts on anomaly detection that described a method for <em>automatically</em> choosing a good value for the number of clusters based on the principle of Minimum Description Length.
Minimum Description Length (MDL) is an elegant framework for evaluating the parsimony of a model.
The Description Length of a model is defined as the amount of information needed to encode that model, plus the encoding-length of some data, <em>given</em> that model.
Therefore, in an MDL framework, a good model is one that allows an efficient (i.e. short) encoding of the data, but whose <em>own</em> description is <em>also</em> efficient
(This suggests connections between MDL and the idea of <a href="https://en.wikipedia.org/wiki/Data_compression#Machine_learning">learning as a form of data compression</a>).</p>

<p>For example, a model that directly memorizes all the data may allow for a very short description of the data, but the model itself will cleary require at least the size of the raw data to encode, and so direct memorization models generaly stack up poorly with respect to MDL.
On the other hand, consider a model of some Gaussian data.  We can describe these data in a length proportional to their log-likelihood under the Gaussian density.  Furthermore, the description length of the Gaussian model itself is very short; just the encoding of its mean and standard deviation.  And so in this case a Gaussian distribution represents an efficient model with respect to MDL.</p>

<p><strong>In summary, an MDL framework allows us to mathematically capture the idea that we only wish to consider increasing the complexity of our models if that buys us a corresponding increase in descriptive power on our data.</strong></p>

<p>In the case of <a href="#cite1">Roberts&#8217; paper</a>, the clustering model in question is a <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture">Gaussian Mixture Model</a> (GMM), and the description length expression to be optimized can be written as:</p>

<p><img src="http://erikerlandson.github.com/assets/images/xmedoids/mdl_gm_eq.png" alt="EQ-1" /></p>

<p>In this expression, X represents the vector of data elements.
The first term is the (negative) log-likelihood of the data, with respect to a candidate GMM having some number (K) of Gaussians; p(x) is the GMM density at point (x).
This term represents the cost of encoding the data, given that GMM.
The second term is the cost of encoding the GMM itself.
The value P is the number of free parameters needed to describe that GMM.
Assuming a dimensionality D for the data, then <nobr>P = K(D + D(D+1)/2):</nobr> D values for each mean vector, and <nobr>D(D+1)/2</nobr> values for each covariance matrix.</p>

<p>I wanted to apply this same MDL principle to identifying a good value for K, in the case of a <a href="https://en.wikipedia.org/wiki/K-medoids">K-Medoids</a> model.
How best to adapt MDL to K-Medoids poses some problems.
In the case of K-Medoids, the <em>only</em> structure given to the data is a distance metric.
There is no vector algebra defined on data elements, much less any ability to model the points as a Gaussian Mixture.</p>

<p>However, any candidate clustering of my data <em>does</em> give me a corresponding distribution of distances from each data element to it&#8217;s closest medoid.
I can evaluate an MDL measure on these distance values.
If adding more clusters (i.e. increasing K) does not sufficiently tighten this distribution, then its description length will start to increase at larger values of K, thus indicating that more clusters are not improving our model of the data.
Expressing this idea as an MDL formulation produces the following description length formula:</p>

<p><img src="http://erikerlandson.github.com/assets/images/xmedoids/mdl_km_eq.png" alt="EQ-2" /></p>

<p>Note that the first two terms are similar to the equation above; however, the underlying distribution <nobr>p(||x-c<sub>x</sub>||)</nobr> is now a distribution over the distances of each data element (x) to its closest medoid <nobr>c<sub>x</sub></nobr>, and P is the corresponding number of free parameters for this distribution (more on this below).
There is now an additional third term, representing the cost of encoding the K medoids.
Each medoid is a data element, and specifying each data element requires log|X| bits (or <a href="http://mathworld.wolfram.com/Nat.html">nats</a>, since I generally use natural logarithms), yielding an additional <nobr>(K)log|X|</nobr> in description length cost.</p>

<p>And so, an MDL-based algorithm for automatically identifying a good number of clusters (K) in a K-Medoids model is to run a K-Medoids clustering on my data, for some set of potential K values, and evaluate the MDL measure above for each, and choose the model whose description length L(X) is the smallest!</p>

<p>As I mentioned above, there is also an implied task of choosing a form (or a set of forms) for the distance distribution <nobr>p(||x-c<sub>x</sub>||)</nobr>.
At the time of this writing, I am fitting a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> to the distance data, and <a href="https://github.com/erikerlandson/silex/blob/blog/xmedoids/src/main/scala/com/redhat/et/silex/cluster/KMedoids.scala#L578">using this gamma distribution</a> to compute log-likelihood values.
A gamma distribution has two free parameters &#8211; a shape parameter and a location parameter &#8211; and so currently the value of P is always 2 in my implementations.
I elaborated on some back-story about how I arrived at the decision to use a gamma distribution <a href="http://erikerlandson.github.io/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution/">here</a> and <a href="http://erikerlandson.github.io/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/">here</a>.
An additional reason for my choice is that the gamma distribution does have a fairly good shape coverage, including two-tailed, single-tailed, and/or exponential-like shapes.</p>

<p>Another observation (based on my blog posts mentioned above) is that my use of the gamma distribution implies a bias toward cluster distributions that behave (more or less) like Gaussian clusters, and so in this respect its current behavior is probably somewhat analogous to the <a href="#cite2">G-Means algorithm</a>, which identifies clusterings that yield Gaussian disributions in each cluster.
Adding other candidates for distance distributions is a useful subject for future work, since there is no compelling reason to either favor or assume Gaussian-like cluster distributions over <em>all</em> kinds of metric spaces.
That said, I am seeing reasonable results even on data with clusters that I suspect are not well modeled as Gaussian distributions.
Perhaps the shape-coverage of the gamma distribution is helping to add some robustness.</p>

<p>To demonstrate the MDL-enhanced K-Medoids in action, I will illustrate its performance on some data sets that are amenable to graphic representation.  The code I used to generate these results is <a href="https://github.com/erikerlandson/silex/blob/blog/xmedoids/src/main/scala/com/redhat/et/silex/cluster/KMedoids.scala#L629">here</a>.</p>

<p>Consider this synthetic data set of points in 2D space.  You can see that I&#8217;ve generated the data to have two latent clusters:</p>

<p><img src="http://erikerlandson.github.com/assets/images/xmedoids/k2_raw.png" alt="K2-Raw" /></p>

<p>I collected the description-length values for candidate K-Medoids models having 1 up to 10 clusters, and plotted them.  This plot shows that the clustering with minimal description length had 2 clusters:</p>

<p><img src="http://erikerlandson.github.com/assets/images/xmedoids/k2_mdl.png" alt="K2-MDL" /></p>

<p>When I plot that optimal clustering at K=2 (with cluster medoids marked in black-and-yellow), the clustering looks good:</p>

<p><img src="http://erikerlandson.github.com/assets/images/xmedoids/k2_clusters.png" alt="K2-Clusters" /></p>

<p>To show the behavior for a different optimal value, the following plots demonstrate the MDL K-Medoids results on data where the number of latent clusters is 4:</p>

<p><img src="http://erikerlandson.github.com/assets/images/xmedoids/k4_raw.png" alt="K4-Raw" />
<img src="http://erikerlandson.github.com/assets/images/xmedoids/k4_mdl.png" alt="K4-MDL" />
<img src="http://erikerlandson.github.com/assets/images/xmedoids/k4_clusters.png" alt="K4-Clusters" /></p>

<p>A final comment on Minimum Description Length approaches to clustering &#8211; although I focused on K-Medoids models in this post, the basic approach (and I suspect even the same description length formulation) would apply equally well to K-Means, and possibly other clustering models.
Any clustering model that involves a distance function from elements to some kind of cluster center should be a good candidate.
I intend to keep an eye out for applications of MDL to <em>other</em> learning models, as well.</p>

<h5>References</h5>

<p><a name="cite1"</a>
[1] <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1338&amp;rep=rep1&amp;type=pdf">&#8220;Novelty Detection Using Extreme Value Statistics&#8221;</a>; Stephen J. Roberts; Feb 23, 1999
<a name="cite2"</a>
[2] <a href="http://papers.nips.cc/paper/2526-learning-the-k-in-k-means.pdf">&#8220;Learning the k in k-means. Advances in neural information processing systems&#8221;</a>; Hamerly, G., &amp; Elkan, C.; 2004</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Approximating a PDF of Distances With a Gamma Distribution]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution/"/>
    <updated>2016-07-09T11:25:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution</id>
    <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/">previous post</a> I discussed some unintuitive aspects of the distribution of distances as spatial dimension changes.  To help explain this to myself I derived a formula for this distribution, assuming a unit multivariate Gaussian.  For distance (aka radius) r, and spatial dimension d, the PDF of distances is:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dist_dist/gwwv5a5.png" alt="Figure 1" /></p>

<p>Recall that the form of this PDF is the <a href="https://en.wikipedia.org/wiki/Generalized_gamma_distribution">generalized gamma distribution</a>, with scale parameter <nobr>a=sqrt(2),</nobr> shape parameter p=2, and free shape parameter (d) representing the dimensionality.</p>

<p>I was interested in fitting parameters to such a distribution, using some distance data from a clustering algorithm.  <a href="https://www.scipy.org/">SciPy</a> comes with a predefined method for fitting generalized gamma parameters, however I wished to implement something similar using <a href="http://commons.apache.org/proper/commons-math/">Apache Commons Math</a>, which does not have native support for fitting a generalized gamma PDF.  I even went so far as to start working out <a href="http://erikerlandson.github.io/blog/2016/06/15/computing-derivatives-of-the-gamma-function/">some of the math</a> needed to augment the Commons Math <a href="http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/differentiation/package-summary.html">Automatic Differentiation libraries</a> with Gamma function differentiation needed to numerically fit my parameters.</p>

<p>Meanwhile, I have been fitting a <em>non generalized</em> <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> to the distance data, as a sort of rough cut, using a fast <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Maximum_likelihood_estimation">non-iterative approximation</a> to the parameter optimization.  Consistent with my habit of asking the obvious question last, I tried plotting this gamma approximation against distance data, to see how well it compared against the PDF that I derived.</p>

<p>Surprisingly (at least to me), my approximation using the gamma distribution is a very effective fit for spatial dimensionalities <nobr> >= 2 </nobr>:</p>

<p><img src="http://erikerlandson.github.com/assets/images/gamma_approx/approx_plot.png" alt="Figure 2" /></p>

<p>As the plot shows, only for the 1-dimension case is the gamma approximation substiantially deviating.  In fact, the fit appears to get better as dimensionality increases.  To address the 1D case, I can easily test the fit of a half-gaussian as a possible model.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing Derivatives of the Gamma Function]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/06/15/computing-derivatives-of-the-gamma-function/"/>
    <updated>2016-06-15T16:37:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/06/15/computing-derivatives-of-the-gamma-function</id>
    <content type="html"><![CDATA[<p>In this post I&#8217;ll describe a simple algorithm to compute the kth derivatives of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a>.</p>

<p>I&#8217;ll start by showing a simple recursion relation for these derivatives, and then gives its derivation.  The kth derivative of Gamma(x) can be computed as follows:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dgamma/hvqtl52.png" alt="Equation 1" /></p>

<p>The recursive formula for the D<sub>k</sub> functions has an easy inductive proof:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dgamma/h79ued9.png" alt="Equation 2" /></p>

<p>Computing the next value D<sub>k</sub> requires knowledge of D<sub>k-1</sub> but also derivative D&#8217;<sub>k-1</sub>.  If we start expanding terms, we see the following:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dgamma/hhvonpa.png" alt="Equation 3" /></p>

<p>Continuing the process above it is not hard to see that we can continue expanding until we are left only with terms of <nobr>D<sub>1</sub><sup>(*)</sup>(x);</nobr> that is, various derivatives of <nobr>D<sub>1</sub>(x)</nobr>.  Furthermore, each layer of substitutions adds an order to the derivatives, so that we will eventually be left with terms involving the derivatives of <nobr>D<sub>1</sub>(x)</nobr> up to the (k-1)th derivative. Note that these will all be successive orders of the <a href="https://en.wikipedia.org/wiki/Polygamma_function">polygamma function</a>.</p>

<p>What we want, to do these computations systematically, is a formula for computing the nth derivative of a term <nobr>D<sub>k</sub>(x)</nobr>.  Examining the first few such derivatives suggests a pattern:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dgamma/jqwqpzy.png" alt="Equation 4" /></p>

<p>Generalizing from the above, we see that the formula for the nth derivative is:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dgamma/jamccnh.png" alt="Equation 5" /></p>

<p>We are now in a position to fill in the triangular table of values, culminating in the value of <nobr>D<sub>k</sub>(x):</nobr></p>

<p><img src="http://erikerlandson.github.com/assets/images/dgamma/jj9ph5l.png" alt="Equation 6" /></p>

<p>As previously mentioned, the basis row of values <nobr>D<sub>1</sub><sup>(*)</sup>(x)</nobr> are the <a href="https://en.wikipedia.org/wiki/Polygamma_function">polygamma functions</a> where <nobr>D<sub>1</sub><sup>(n)</sup>(x) = polygamma<sup>(n)</sup>(x)</nobr>.  The first two polygammas, order 0 and 1, are simply the digamma and trigamma functions, respectively, and are available with most numeric libraries.  Computing the general polygamma is a project, and blog post, for another time, but the standard polynomial approximation for the digamma function can of course be differentiated&#8230;  Happy Computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploring the Effects of Dimensionality on a PDF of Distances]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/"/>
    <updated>2016-06-08T20:56:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances</id>
    <content type="html"><![CDATA[<p>Every so often I&#8217;m reminded that the effects of changing dimensionality on objects and processes can be surprisingly counterintuitive.  Recently I ran across a great example of this, while I working on a model for the distribution of distances in spaces of varying dimension.</p>

<p>Suppose that I draw some values from a classic one-dimensional Gaussian, with zero mean and unit variance, but that I am actually interested in their corresponding distances from center.  Knowing that my Gaussian is centered on the origin, I can rephrase that as: the distribution of magnitudes of values drawn from that Gaussian.  I can simulate this process by actually samping Gaussian values and taking their absolute value.  When I do, I get the following result:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dist_dist/figure1.png" alt="Figure 1" /></p>

<p>It&#8217;s easy to see &#8211; and intuitive &#8211; that the resulting distribution is a <a href="https://en.wikipedia.org/wiki/Half-normal_distribution">half-Gaussian</a>, as I confirmed by overlaying the histogrammed samples above with a half-Gaussian PDF (displayed in green).</p>

<p>I wanted to generalize this basic idea into some arbitrary dimensionality, (d), where I draw d-vectors from an <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">d-dimensional Gaussian</a> (again, centered on the origin with unit variances). When I take the magnitudes of these sampled d-vectors, what will the probability distribution of <em>their</em> magnitudes look like?</p>

<p>My intuitive assumption was that these magnitudes would <em>also</em> follow a half-Gaussian distribution.  After all, every multivariate Gaussian is densest at its mean, just like the univariate case I examined above.  In fact I was so confident in this assumption that I built my initial modeling around it.  Great confusion ensued, when I saw how poorly my models were working on my higher-dimensional data!</p>

<p>Eventually it occurred to me to do the obvious thing and generate some visualizations from higher dimensional data.  For example, here is the correponding plot generated from a bivariate Gaussian (d=2):</p>

<p><a name="figure2"></a>
<img src="http://erikerlandson.github.com/assets/images/dist_dist/figure2.png" alt="Figure 2" /></p>

<p>Surprise &#8211; the distribution at d=2 is <em>not even close to half-Gaussian!</em>.  My intuitions couldn&#8217;t have been more misleading!</p>

<p>Where did I go wrong?</p>

<p>I&#8217;ll start by observing what happens when I take a multi-dimensional PDF of vectors in (d) dimensions and project it down to a one-dimensional PDF of the corresponding vector magnitudes. To keep things simple, I will be assuming a multi-dimensional PDF <nobr>f<sub>r</sub>(<strong>x</strong><sub>d</sub>)</nobr> that is (1) centered on the origin, and (2) is radially symmetric; the pdf value is the same for all points at a given distance from the origin.  For example, any multivariate Gaussian with <strong>0</strong><sub>d</sub> mean and <strong>I</strong><sub>d</sub> for a covariance matrix satisfies these two assumptions.  With this in mind, you can see that the process of projecting from vectors in <strong>R</strong><sub>d</sub> to their distance from <strong>0</strong><sub>d</sub> (their magnitude) is equivalent to summing all densities <nobr>f<sub>r</sub>(<strong>x</strong><sub>d</sub>)</nobr> along the surface of &#8220;d-sphere&#8221; radius (r) to obtain a pdf f(r) in distance space.  With assumption (2) we can simplify that integration to just <nobr>f(r)=A<sub>d</sub>(r)f&#8217;(r)</nobr>, where f&#8217;(r) defines the value of <nobr>f<sub>r</sub>(<strong>x</strong>)</nobr> for all <strong>x</strong> with magnitude of (r), and A<sub>d</sub>(r) is the surface area of a d-sphere with radius (r):</p>

<p><img src="http://erikerlandson.github.com/assets/images/dist_dist/ztrlusa.png" alt="Figure 3" /></p>

<p>The key observation is that this term is a <em>polynomial</em> function of radius (r), with degree (d-1).  When d=1, it is simply a constant multiplier and so we get the half-Gaussian distribution we expect, but when <nobr>d >= 2</nobr>, the term is zero at r=0, and grows with radius.  Hence we see the in the <a href="#figure2">d=2 plot above</a> that the density begins at zero, then grows with radius until the decreasing gaussian density gradually drives it back toward zero again.</p>

<p>The above ideas can be expressed compactly as follows:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dist_dist/jukgy85.png" alt="Figure 4" /></p>

<p>In my experiments, I am using multivariate Gaussians of mean <strong>0</strong><sub>d</sub> and unit covariance matrix <strong>I</strong><sub>d</sub>, and so the form for f(r;d) becomes:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dist_dist/gwwv5a5.png" alt="Figure 4" /></p>

<p>This form is in fact the <a href="https://en.wikipedia.org/wiki/Generalized_gamma_distribution">generalized gamma distribution</a>, with scale parameter <nobr>a=2<sup>1/2</sup>,</nobr> shape parameter p=2, and free shape parameter (d) representing the dimensionality in this context.</p>

<p>I can verify that this PDF is correct by plotting it against randomly sampled data at differing dimensions:</p>

<p><img src="http://erikerlandson.github.com/assets/images/dist_dist/figure3.png" alt="Figure 5" /></p>

<p>This plot demonstrates both that the PDF expression is correct for varying dimensionalities and also illustrates how the shape of the PDF evolves as dimensionality changes.  For me, it was a great example of challenging my intuitions and learning something completely unexpected about the interplay of distances and dimension.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measuring Decision Tree Split Quality with Test Statistic P-Values]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/"/>
    <updated>2016-05-26T14:39:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values</id>
    <content type="html"><![CDATA[<p>When training a <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a> learning model (or an <a href="https://en.wikipedia.org/wiki/Random_forest">ensemble</a> of such models) it is often nice to have a policy for deciding when a tree node can no longer be usefully split.  There are a variety possibilities.  For example, halting when node population size becomes smaller than some threshold is a simple and effective policy.  Another approach is to halt when some measure of node purity fails to increase by some minimum threshold.  <strong>The underlying concept is to have some measure of split <em>quality</em>, and to halt when no candidate split has sufficient quality.</strong></p>

<p>In this post I am going to discuss some advantages to one of my favorite approaches to measuring split quality, which is to use a <a href="https://en.wikipedia.org/wiki/Statistical_significance">test statistic significance</a> &#8211; aka &#8220;p-value&#8221; &#8211; of the null hypothesis that the left and right sub-populations are the same after the split.  The idea is that if a split is of good quality, then it ought to have caused the sub-populations to the left and right of the split to be <em>meaningfully different</em>.  That is to say: the null hypothesis (that they are the same) should be <em>rejected</em> with high confidence, i.e. a small p-value.  What constitutes &#8220;small&#8221; is always context dependent, but popular p-values from applied statistics are 0.05, 0.01, 0.005, etc.</p>

<blockquote><p>update &#8211; there is now an Apache Spark <a href="https://issues.apache.org/jira/browse/SPARK-15699">JIRA</a> and a <a href="https://github.com/apache/spark/pull/13440">pull request</a> for this feature</p></blockquote>

<p>The remainder of this post is organized in the following sections:</p>

<p><a href="#consistency">Consistency</a> <br>
<a href="#awareness">Awareness of Sample Sizes</a> <br>
<a href="#results">Training Results</a> <br>
<a href="#conclusion">Conclusion</a> <br></p>

<p><a name="consistency"></a></p>

<h5>Consistency</h5>

<p>Test statistic p-values have some appealing properties as a split quality measure.  The test statistic methodology has the advantage of working essentially the same way regardless of the particular test being used.  We begin with two sample populations; in our case, these are the left and right sub-populations created by a candidate split.  We want to assess whether these two populations have the same distribution (the null hypothesis) or different distributions.  We measure some test statistic &#8216;S&#8217; (<a href="https://en.wikipedia.org/wiki/Student's_t-test">Student&#8217;s t</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a>, etc).  We then compute the probability that |S| >= the value we actually measured.  This probability is commonly referred to as the p-value.  The smaller the p-value, the less likely it is that our two populations are the same.  <strong>In our case, we can interpret this as: a smaller p-value indicates a better quality split.</strong></p>

<p>This consistent methodology has a couple advantages contributing to user experience (UX).  If all measures of split quality work in the same way, then there is a lower cognitive load to move between measures once the user understands the common pattern of use.  A second advantage is better &#8220;unit analysis.&#8221;  Since all such quality measures take the form of p-values, there is no risk of a chosen quality measure getting mis-aligned with a corresponding quality threshold.  They are all probabilities, on the interval [0,1], and &#8220;smaller threshold&#8221; always means &#8220;higher threshold of split quality.&#8221;   By way of comparison, if an application is measuring <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">entropy</a> and then switches to using <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini impurity</a>, these measures are in differing units and care has to be taken that the correct quality threshold is used in each case or the model training policy will be broken.  Switching between differing statistical tests does not come with the same risk.  <strong>A p-value quality threshold will have the same semantic regardless of which statistical test is being applied:</strong> probability that left and right sub-populations are the same, given the particular statistic being measured.</p>

<p><a name="awareness"></a></p>

<h5>Awareness of Sample Size</h5>

<p>Test statistics have another appealing property: many are &#8220;aware&#8221; of sample size in a way that captures the idea that the smaller the sample size, the larger the difference between populations should be to conclude a given significance.  For one example, consider <a href="https://en.wikipedia.org/wiki/Welch's_t-test#Statistical_test">Welch&#8217;s t-test</a>, the two-sample variation of the t distribution that applies well to comparing left and right sub populations of candidate decision tree splits:</p>

<p><img src="http://erikerlandson.github.com/assets/images/pval_halting/figure_1.png" alt="Figure 1" /></p>

<p>Visualizing the effects of sample sizes n1 and n2 on these equations directly is a bit tricky, but assuming equal sample sizes and variances allows the equations to be simplified quite a bit, so that we can observe the effect of sample size:</p>

<p><img src="http://erikerlandson.github.com/assets/images/pval_halting/figure_2.png" alt="Figure 2" /></p>

<p>These simplified equations show clearly that (all else remaining equal) as sample size grows smaller, the measured t-statistic correspondingly grows smaller (proportional to sqrt(n)), and furthermore the corresponding variance of the t distribution to be applied grows larger.  For any given shift in left and right sub-populations, each of these trends yields a larger (i.e. weaker) p-value.   This behavior is desirable for a split quality metric.  <strong>The less data there is at a given candidate split, the less confidence there <em>should</em> be in split quality.</strong>  Put another way: we would like to require a larger difference before a split is measured as being good quality when we have less data to work with, and that is exactly the behavior the t-test provides us.</p>

<p><a name="results"></a></p>

<h5>Training Results</h5>

<p>These propreties are pleasing, but it remains to show that test statistics can actually improve decision tree training in practice.  In the following sections I will compare the effects of training with test statstics with other split quality policies based on entropy and gini index.</p>

<p>To conduct these experiments, I modified a <a href="https://github.com/erikerlandson/spark/blob/pval_halting/mllib/src/main/scala/org/apache/spark/mllib/tree/impurity/ChiSquared.scala">local copy</a> of Apache Spark with the <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a> test statistic for comparing categorical distributions.  The demo script, which I ran in <code>spark-shell</code>, can be viewed <a href="https://github.com/erikerlandson/spark/blob/pval_halting/pval_demo">here</a>.</p>

<p>I generated an example data set that represents a two-class learning problem, where labels may be 0 or 1.  Each sample has 10 clean binary features, such that if the bit is 1, the probability of the label is 90% 1 and 10% 0.  There are 5 noise features, also binary, which are completely random.   There are 50 samples of each clean feature being on, for a total of 500 samples.   There are also 500 samples where all clean features are 0 and the corresponding labels are 90% 0 and 10% 1.  The total number of samples in the data set is 1000.  The shape of the data is illustrated by the following table:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>truth |     features 0 - 9 (one on at a time)     |   random noise
</span><span class='line'>------+-------------------------------------------+--------------
</span><span class='line'>90% 1 | 1   0   0   0   0   0   0   0   0   0   0 | 1   0   0   1   0
</span><span class='line'>90% 1 |  ... 50 samples with feature 0 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 | 0   1   0   0   0   0   0   0   0   0   0 | 0   1   1   0   0
</span><span class='line'>90% 1 |  ... 50 samples with feature 1 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 2 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 3 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 4 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 5 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 6 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 7 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 8 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 9 'on' ...   |   ... noise ...
</span><span class='line'>90% 0 | 0   0   0   0   0   0   0   0   0   0   0 | 1   1   0   0   1
</span><span class='line'>90% 0 |  ... 500 samples with all 'off  ...       |   ... noise ...</span></code></pre></td></tr></table></div></figure>


<p>For the first run I use my customized chi-squared statistic as the split quality measure.  I used a p-value threshold of 0.01 &#8211; that is, I would like my chi-squared test to conclude that the probability of left and right split populations are the same is &lt;= 0.01, or that split will not be used.  Note, this means I can expect that around 1% of the time, it will conclude a split was good, when it was just luck.  This is a reasonable false-positive rate; random forests are by nature robust to noise, including noise in their own split decisions:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; :load pval_demo
</span><span class='line'>Loading pval_demo...
</span><span class='line'>defined module demo
</span><span class='line'>
</span><span class='line'>scala&gt; val rf = demo.train("chisquared", 0.01, noise = 0.1)
</span><span class='line'>  pval= 1.578e-09
</span><span class='line'>gain= 20.2669
</span><span class='line'>  pval= 1.578e-09
</span><span class='line'>gain= 20.2669
</span><span class='line'>  pval= 1.578e-09
</span><span class='line'>gain= 20.2669
</span><span class='line'>  pval= 9.140e-09
</span><span class='line'>gain= 18.5106
</span><span class='line'>
</span><span class='line'>... more tree p-value demo output ...
</span><span class='line'>
</span><span class='line'>  pval= 0.7429
</span><span class='line'>gain= 0.2971
</span><span class='line'>  pval= 0.9287
</span><span class='line'>gain= 0.0740
</span><span class='line'>  pval= 0.2699
</span><span class='line'>gain= 1.3096
</span><span class='line'>rf: org.apache.spark.mllib.tree.model.RandomForestModel = 
</span><span class='line'>TreeEnsembleModel classifier with 1 trees
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>scala&gt; println(rf.trees(0).toDebugString)
</span><span class='line'>DecisionTreeModel classifier of depth 10 with 21 nodes
</span><span class='line'>  If (feature 5 in {1.0})
</span><span class='line'>   Predict: 1.0
</span><span class='line'>  Else (feature 5 not in {1.0})
</span><span class='line'>   If (feature 6 in {1.0})
</span><span class='line'>    Predict: 1.0
</span><span class='line'>   Else (feature 6 not in {1.0})
</span><span class='line'>    If (feature 0 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 0 not in {1.0})
</span><span class='line'>     If (feature 1 in {1.0})
</span><span class='line'>      Predict: 1.0
</span><span class='line'>     Else (feature 1 not in {1.0})
</span><span class='line'>      If (feature 2 in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 2 not in {1.0})
</span><span class='line'>       If (feature 8 in {1.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>       Else (feature 8 not in {1.0})
</span><span class='line'>        If (feature 3 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 3 not in {1.0})
</span><span class='line'>         If (feature 4 in {1.0})
</span><span class='line'>          Predict: 1.0
</span><span class='line'>         Else (feature 4 not in {1.0})
</span><span class='line'>          If (feature 7 in {1.0})
</span><span class='line'>           Predict: 1.0
</span><span class='line'>          Else (feature 7 not in {1.0})
</span><span class='line'>           If (feature 9 in {1.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>           Else (feature 9 not in {1.0})
</span><span class='line'>            Predict: 0.0
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p>The first thing to observe is that <strong>the resulting decision tree used exactly the 10 clean features 0 through 9, and none of the five noise features.</strong>   The tree splits off each of the clean features to obtain an optimally accurate leaf-node (one with 90% 1s and 10% 0s).  A second observation is that the p-values shown in the demo output are extremely small (i.e. strong) values &#8211; around 1e-9 (one part in a billion) &#8211; for good-quality splits.  We can also see &#8220;weak&#8221; p-values with magnitudes such as 0.7, 0.2, etc.  These are poor quality splits on the noise features that it rejects and does not use in the tree, exactly as we hope to see.</p>

<p>Next, I will show a similar run with the standard available &#8220;entropy&#8221; quality measure, and a minimum gain threshold of 0.035, which is a value I had to determine by trial and error, as what kind of entropy gains one can expect to see, and where to cut them off, is somewhat unintuitive and likely to be very data dependent.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val rf = demo.train("entropy", 0.035, noise = 0.1)
</span><span class='line'>  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
</span><span class='line'>gain= 0.0310
</span><span class='line'>  impurity parent= 0.9970, left= 0.1414 (  50), right= 0.9998 ( 950) weighted= 0.9569
</span><span class='line'>gain= 0.0402
</span><span class='line'>  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
</span><span class='line'>gain= 0.0310
</span><span class='line'>
</span><span class='line'>... more demo output ...
</span><span class='line'>
</span><span class='line'>rf: org.apache.spark.mllib.tree.model.RandomForestModel = 
</span><span class='line'>TreeEnsembleModel classifier with 1 trees
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>scala&gt; println(rf.trees(0).toDebugString)
</span><span class='line'>DecisionTreeModel classifier of depth 11 with 41 nodes
</span><span class='line'>  If (feature 4 in {1.0})
</span><span class='line'>   If (feature 12 in {1.0})
</span><span class='line'>    If (feature 11 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 11 not in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>   Else (feature 12 not in {1.0})
</span><span class='line'>    Predict: 1.0
</span><span class='line'>  Else (feature 4 not in {1.0})
</span><span class='line'>   If (feature 1 in {1.0})
</span><span class='line'>    If (feature 12 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 12 not in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>   Else (feature 1 not in {1.0})
</span><span class='line'>    If (feature 0 in {1.0})
</span><span class='line'>     If (feature 10 in {0.0})
</span><span class='line'>      If (feature 14 in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 14 not in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>     Else (feature 10 not in {0.0})
</span><span class='line'>      If (feature 14 in {0.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 14 not in {0.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>    Else (feature 0 not in {1.0})
</span><span class='line'>     If (feature 6 in {1.0})
</span><span class='line'>      Predict: 1.0
</span><span class='line'>     Else (feature 6 not in {1.0})
</span><span class='line'>      If (feature 3 in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 3 not in {1.0})
</span><span class='line'>       If (feature 7 in {1.0})
</span><span class='line'>        If (feature 13 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 13 not in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>       Else (feature 7 not in {1.0})
</span><span class='line'>        If (feature 2 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 2 not in {1.0})
</span><span class='line'>         If (feature 8 in {1.0})
</span><span class='line'>          Predict: 1.0
</span><span class='line'>         Else (feature 8 not in {1.0})
</span><span class='line'>          If (feature 9 in {1.0})
</span><span class='line'>           If (feature 11 in {1.0})
</span><span class='line'>            If (feature 13 in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>            Else (feature 13 not in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>           Else (feature 11 not in {1.0})
</span><span class='line'>            If (feature 12 in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>            Else (feature 12 not in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>          Else (feature 9 not in {1.0})
</span><span class='line'>           If (feature 5 in {1.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>           Else (feature 5 not in {1.0})
</span><span class='line'>            Predict: 0.0
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p>The first observation is that <strong>the resulting tree using entropy as a split quality measure is twice the size of the tree trained using the chi-squared policy.</strong>  Worse, it is using the noise features &#8211; its quality measure is yielding many more false positives.  The entropy-based model is less parsimonious and will also have performance problems since the model has included very noisy features.</p>

<p>Lastly, I ran a similar training using the &#8220;gini&#8221; impurity measure, and a 0.015 quality threshold (again, hopefully optimal value that I had to run multiple experiments to identify).  Its quality is better than the entropy-based measure, but this model is still substantially larger than the chi-squared model, and it still uses some noise features:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val rf = demo.train("gini", 0.015, noise = 0.1)
</span><span class='line'>  impurity parent= 0.4999, left= 0.2952 (  50), right= 0.4987 ( 950) weighted= 0.4885
</span><span class='line'>gain= 0.0113
</span><span class='line'>  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
</span><span class='line'>gain= 0.0158
</span><span class='line'>  impurity parent= 0.4999, left= 0.1472 (  50), right= 0.4981 ( 950) weighted= 0.4806
</span><span class='line'>gain= 0.0193
</span><span class='line'>  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
</span><span class='line'>gain= 0.0158
</span><span class='line'>
</span><span class='line'>... more demo output ...
</span><span class='line'>
</span><span class='line'>rf: org.apache.spark.mllib.tree.model.RandomForestModel = 
</span><span class='line'>TreeEnsembleModel classifier with 1 trees
</span><span class='line'>
</span><span class='line'>scala&gt; println(rf.trees(0).toDebugString)
</span><span class='line'>DecisionTreeModel classifier of depth 12 with 31 nodes
</span><span class='line'>  If (feature 6 in {1.0})
</span><span class='line'>   Predict: 1.0
</span><span class='line'>  Else (feature 6 not in {1.0})
</span><span class='line'>   If (feature 3 in {1.0})
</span><span class='line'>    Predict: 1.0
</span><span class='line'>   Else (feature 3 not in {1.0})
</span><span class='line'>    If (feature 1 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 1 not in {1.0})
</span><span class='line'>     If (feature 8 in {1.0})
</span><span class='line'>      Predict: 1.0
</span><span class='line'>     Else (feature 8 not in {1.0})
</span><span class='line'>      If (feature 2 in {1.0})
</span><span class='line'>       If (feature 14 in {0.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>       Else (feature 14 not in {0.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>      Else (feature 2 not in {1.0})
</span><span class='line'>       If (feature 5 in {1.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>       Else (feature 5 not in {1.0})
</span><span class='line'>        If (feature 7 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 7 not in {1.0})
</span><span class='line'>         If (feature 0 in {1.0})
</span><span class='line'>          If (feature 12 in {1.0})
</span><span class='line'>           If (feature 10 in {0.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>           Else (feature 10 not in {0.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>          Else (feature 12 not in {1.0})
</span><span class='line'>           Predict: 1.0
</span><span class='line'>         Else (feature 0 not in {1.0})
</span><span class='line'>          If (feature 9 in {1.0})
</span><span class='line'>           Predict: 1.0
</span><span class='line'>          Else (feature 9 not in {1.0})
</span><span class='line'>           If (feature 4 in {1.0})
</span><span class='line'>            If (feature 10 in {0.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>            Else (feature 10 not in {0.0})
</span><span class='line'>             If (feature 14 in {0.0})
</span><span class='line'>              Predict: 1.0
</span><span class='line'>             Else (feature 14 not in {0.0})
</span><span class='line'>              Predict: 1.0
</span><span class='line'>           Else (feature 4 not in {1.0})
</span><span class='line'>            Predict: 0.0
</span><span class='line'>
</span><span class='line'>scala&gt;</span></code></pre></td></tr></table></div></figure>


<p><a name="conclusion"></a></p>

<h5>Conclusion</h5>

<p>In this post I have discussed some advantages of using test statstics and p-values as split quality metrics for decision tree training:</p>

<ul>
<li>Consistency</li>
<li>Awareness of sample size</li>
<li>Higher quality model training</li>
</ul>


<p>I believe they are a useful tool for improved training of decision tree models!  Happy computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Forest Clustering of Machine Package Configurations in Apache Spark]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations/"/>
    <updated>2016-05-05T15:05:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations</id>
    <content type="html"><![CDATA[<p>In this post I am going to describe some results I obtained for <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a> machines by which <a href="https://en.wikipedia.org/wiki/RPM_Package_Manager">RPM packages</a> that were installed on them.  The clustering technique I used was <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup">Random Forest Clustering</a>.</p>

<p><a name="data"></a></p>

<h5>The Data</h5>

<p>The data I clustered consisted of 135 machines, each with a list of installed RPM packages.  The number of unique package names among all 135 machines was 4397.  Each machine was assigned a vector of Boolean values: a value of <code>1</code> indicates that the corresponding RPM was installed on that machine.  This means that the clustering data occupied a space of nearly 4400 dimensions.  I discuss the implications of this <a href="#payoff">later in the post</a>, and what it has to do with Random Forest Clustering in particular.</p>

<p>For ease of navigation and digestion, the remainder of this post is organized in sections:</p>

<p><a href="#clustering">Introduction to Random Forest Clustering</a> <br>
&nbsp; &nbsp; &nbsp; &nbsp;  (<a href="#payoff">The Pay-Off</a>) <br>
<a href="#code">Package Configuration Clustering Code</a> <br>
<a href="#results">Clustering Results</a> <br>
&nbsp; &nbsp; &nbsp; &nbsp;  (<a href="#outliers">Outliers</a>) <br></p>

<p><a name="clustering"></a></p>

<h5>Random Forests and Random Forest Clustering</h5>

<p>Full explainations of <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests</a> and <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup">Random Forest Clustering</a> could easily occupy blog posts of their own, but I will attempt to summarize them briefly here.  Random Forest learning models <em>per se</em> are well covered in the machine learning community, and available in most machine learning toolkits.  With that in mind, I will focus on their application to Random Forest Clustering, as it is less commonly used.</p>

<p>A Random Forest is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble learning model</a>, consisting of some number of individual <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a>, each trained on a random subset of the training data, and which choose from a random subset of candidate features when learning each internal decision node.</p>

<p>Random Forest Clustering begins by training a Random Forest to distinguish between the data to be clustered, and a corresponding <em>synthetic</em> data set created by sampling from the <a href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal</a> distributions of each <a href="https://en.wikipedia.org/wiki/Feature_vector">feature</a>.  If the data has well defined clusters in the <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint feature space</a> (a common scenario), then the model can identify these clusters as standing out from the more homogeneous distribution of synthetic data.  A simple example of what this looks like in 2 dimensional data is displayed in Figure 1, where the dark red dots are the data to be clustered, and the lighter pink dots represent synthetic data generated from the marginal distributions:</p>

<p><img src="http://erikerlandson.github.com/assets/images/rfc_machines/demo1_both.png" alt="Figure 1" /></p>

<p>Each interior decision node, in each tree of a Random Forest, typically divides the space of feature vectors in half: the half-space &lt;= some threshold, and the half-space > that threshold.  The result is that the model learned for our data can be visualized as rectilinear regions of space.  In this simple example, these regions can be plotted directly over the data, and show that the Random Forest did indeed learn the location of the data clusters against the background of synthetic data:</p>

<p><img src="http://erikerlandson.github.com/assets/images/rfc_machines/demo1_rules.png" alt="Figure 2" /></p>

<p>Once this model has been trained, the actual data to be clustered are evaluated against this model.  Each data element navigates the interior decision nodes and eventually arrives at a leaf-node of each tree in the Random Forest ensemble, as illustrated in the following schematic:</p>

<p><img src="http://erikerlandson.github.com/assets/images/rfc_machines/eval_leafs.png" alt="Figure 3" /></p>

<p>A key insight of Random Forest Clustering is that if two objects (or, their feature vectors) are similar, then they are likely to arrive at the same leaf nodes more often than not.  As the figure above suggests, it means we can cluster objects by their corresponding vectors of leaf nodes, <em>instead</em> of their raw feature vectors.</p>

<p>If we map the points in our toy example to leaf ids in this way, and then cluster the results, we obtain the following two clusters, which correspond well with the structure of the data:</p>

<p><img src="http://erikerlandson.github.com/assets/images/rfc_machines/demo1_clust.png" alt="Figure 4" /></p>

<p>A note on clustering leaf ids.  A leaf id is just that &#8211; an identifier &#8211; and in that respect a vector of leaf ids has no <em>algebra</em>; it is not meaningful to take an average of such identifiers, any more than it would be meaningful to take the average of people&#8217;s names.  Pragmatically, what this means is that the popular <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering algorithm</a> <em>cannot</em> be applied to this problem.</p>

<p>These vectors do, however, have <em>distance</em>: for any pair of vectors, add 1 for each corresponding pair of leaf ids that differ.  If two data elements arrived at all the same leafs in the Random Forest model, all their leaf ids are the same, and their distance is zero (with respect to the model, they are the same).  Therefore, we <em>can</em> apply <a href="https://en.wikipedia.org/wiki/K-medoids">k-medoids clustering</a>.</p>

<p><a name="payoff"></a></p>

<h5>The Pay-Off</h5>

<p>What does this somewhat indirect method of clustering buy us?  Why <em>not</em> just cluster objects by their raw feature vectors?</p>

<p>The problem is that in many real-world cases (unlike in our toy example above), feature vectors computed for objects have <em>many dimensions</em> &#8211; hundreds, thousands, perhaps millions &#8211; instead of the two dimensions in this example.  Computing distances on such objects, necessary for clustering, is often expensive, and worse yet the quality of these distances is frequently poor due to the fact that most features in large spaces will be poorly correlated with <em>any</em> structure in the data.  This problem is so common, and so important, it has a name: the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a>.</p>

<p>Random Forest Clustering, which clusters on vectors of leaf-node ids from the trees in the model, side-steps the curse of dimensionality because the Random Forest training process, by learning where the data is against the background of the synthetic data, has already identified the features that are useful for identifying the structure of the data!   If any particular feature was poorly correlated with that struture, it has already been ignored by the model.  In other words, a Random Forest Clustering model is implicitly examining <strong> <em>exactly those features that are most useful for clustering</em> </strong>, thus providing a cure for the Curse of Dimensionality.</p>

<p>The <a href="#data">machine package configurations</a> whose clustering I describe for this post are a good example of high dimensional data that is vulnerable to the Curse of Dimensionality.  The dimensionality of the feature space is nearly 4400, making distances between vectors potentially expensive to evaluate.  Any individual feature contributes little to the distance, having to contend with over 4000 other features.  Installed packages are also noisy.  Many packages, such as kernels, are installed everywhere.  Others may be installed but not used, making them potentially irrelevant to grouping machines.  Furthermore, there are only 135 machines, and so there are far more features than data examples, making this an underdetermined data set.</p>

<p>All of these factors make the machine package configuration data a good test of the strenghts of Random Forest Clustering.</p>

<p><a name="code"></a></p>

<h5>Package Configuration Clustering Code</h5>

<p>The implementation of Random Forest Clustering I used for the results in this post is a library available from the <a href="http://silex.freevariable.com/">silex project</a>, a package of analytics libraries and utilities for <a href="http://spark.apache.org/">Apache Spark</a>.</p>

<p>In this section I will describe three code fragments that load the machine configuration data, perform a Random Forest clustering, and format some of the output.  This is the code I ran to obtain the <a href="#results">results</a> described in the final section of this post.</p>

<p>The first fragment of code illustrates the logistics of loading the feature vectors from file <code>train.txt</code> that represent the installed-package configurations for each machine. A corresponding &#8220;parallel&#8221; file <code>nodesclean.txt</code> contains corresponding machine names for each vector.  A third companion file <code>rpms.txt</code> contains names of each installed package.  These are used to instantiate a specialized Scala function (<code>InvertibleIndexFunction</code>) between feature indexes and human-readable feature names (in this case, names of RPM packages).  Finally, another specialized function (<code>Extractor</code>) for instantiating Spark feature vectors is created.</p>

<p>Note: <code>Extractor</code> and <code>InvertibleIndexFunction</code> are also component libraries of <a href="http://silex.freevariable.com/">silex</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Load installed-package feature vectors</span>
</span><span class='line'><span class="k">val</span> <span class="n">fields</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$dataDir/train.txt&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Pair feature vectors with machine names</span>
</span><span class='line'><span class="k">val</span> <span class="n">nodes</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$dataDir/nodesclean.txt&quot;</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)(</span><span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="k">val</span> <span class="n">ids</span> <span class="k">=</span> <span class="n">fields</span><span class="o">.</span><span class="n">paste</span><span class="o">(</span><span class="n">nodes</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Load map from feature indexes to package names</span>
</span><span class='line'><span class="k">val</span> <span class="n">inp</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$dataDir/rpms.txt&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">r</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">toInt</span><span class="o">,</span> <span class="n">r</span><span class="o">(</span><span class="mi">1</span><span class="o">)))</span>
</span><span class='line'>  <span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">toVector</span><span class="o">.</span><span class="n">sorted</span>
</span><span class='line'><span class="k">val</span> <span class="n">nf</span> <span class="k">=</span> <span class="nc">InvertibleIndexFunction</span><span class="o">(</span><span class="n">inp</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// A feature extractor maps features into sequence of doubles</span>
</span><span class='line'><span class="k">val</span> <span class="n">m</span> <span class="k">=</span> <span class="n">fields</span><span class="o">.</span><span class="n">first</span><span class="o">.</span><span class="n">length</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class='line'><span class="k">val</span> <span class="n">ext</span> <span class="k">=</span> <span class="nc">Extractor</span><span class="o">(</span><span class="n">m</span><span class="o">,</span> <span class="o">(</span><span class="n">v</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">).</span><span class="n">tail</span> <span class="k">:</span><span class="kt">FeatureSeq</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">withNames</span><span class="o">(</span><span class="n">nf</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">withCategoryInfo</span><span class="o">(</span><span class="nc">IndexFunction</span><span class="o">.</span><span class="n">constant</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="n">m</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>The next section of code is where the work of Random Forest Clustering happens.  A <code>RandomForestCluster</code> object is instantiated, and configured.  Here, the configuration is for 7 clusters, 250 synthetic points (about twice as many synthetic points as true data), and a Random Forest of 20 trees.  Training against the input data is a simple call to the <code>run</code> method.</p>

<p>The <code>predictWithDistanceBy</code> method is then applied to the data paired with machine names, to yield tuples of cluster-id, distance to cluster center, and the associated machine name.  These tuples are split by distance into data with a cluster, and data considered to be &#8220;outliers&#8221; (i.e. elements far from any cluster center).  Lastly, the <code>histFeatures</code> method is applied, to examine the Random Forest Model and identify any commonly-used features.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Train a Random Forest Clustering Model</span>
</span><span class='line'><span class="k">val</span> <span class="n">rfcModel</span> <span class="k">=</span> <span class="nc">RandomForestCluster</span><span class="o">(</span><span class="n">ext</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setClusterK</span><span class="o">(</span><span class="mi">7</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setSyntheticSS</span><span class="o">(</span><span class="mi">250</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setRfNumTrees</span><span class="o">(</span><span class="mi">20</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setSeed</span><span class="o">(</span><span class="mi">37</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">run</span><span class="o">(</span><span class="n">fields</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Evaluate to get tuples: (cluster, distance, machine-name)</span>
</span><span class='line'><span class="k">val</span> <span class="n">cid</span> <span class="k">=</span> <span class="n">ids</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">rfcModel</span><span class="o">.</span><span class="n">predictWithDistanceBy</span><span class="o">(</span><span class="k">_</span><span class="o">)(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Split by closest distances into clusters and outliers  </span>
</span><span class='line'><span class="k">val</span> <span class="o">(</span><span class="n">clusters</span><span class="o">,</span> <span class="n">outliers</span><span class="o">)</span> <span class="k">=</span> <span class="n">cid</span><span class="o">.</span><span class="n">splitFilter</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">dist</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">dist</span> <span class="o">&lt;=</span> <span class="mi">5</span> <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Generate a histogram of features used in the RF model</span>
</span><span class='line'><span class="k">val</span> <span class="n">featureHist</span> <span class="k">=</span> <span class="n">rfcModel</span><span class="o">.</span><span class="n">randomForestModel</span><span class="o">.</span><span class="n">histFeatures</span><span class="o">(</span><span class="n">ext</span><span class="o">.</span><span class="n">names</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The final code fragment simply formats clusters and outliers into a tabular form, as displayed in the <a href="#results">next section</a> of this post.  Note that there is neither Spark nor silex code here; standard Scala methods are sufficient to post-process the clustering data:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Format clusters for display</span>
</span><span class='line'><span class="k">val</span> <span class="n">clusterStr</span> <span class="k">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">j</span><span class="o">,</span> <span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">j</span><span class="o">,</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">))</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">groupByKey</span>
</span><span class='line'>  <span class="o">.</span><span class="n">collect</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">j</span><span class="o">,</span> <span class="n">nodes</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="n">nodes</span><span class="o">.</span><span class="n">toSeq</span><span class="o">.</span><span class="n">sorted</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="s">&quot;$d  $n&quot;</span> <span class="o">}.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n\n&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Format outliers for display</span>
</span><span class='line'><span class="k">val</span> <span class="n">outlierStr</span> <span class="k">=</span> <span class="n">outliers</span><span class="o">.</span><span class="n">collect</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">d</span><span class="o">,</span><span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">toVector</span><span class="o">.</span><span class="n">sorted</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="s">&quot;$d  $n&quot;</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><a name="results"></a></p>

<h5>Package Configuration Clustering Results</h5>

<p>The result of running the code in the <a href="#code">previous section</a> is seven clusters of machines.  In the following files, the first column represents distance from the cluster center, and the second is the actual machine&#8217;s node name.  A cluster distance of 0.0 indicates that the machine was indistinguishable from cluster center, as far as the Random Forest model was concerned.   The larger the distance, the more different from the cluster&#8217;s center a machine was, in terms of its installed RPM packages.</p>

<p>Was the clustering meaningful?  Examining the first two clusters below is promising; the machine names in these clusters are clearly similar, likely configured for some common task by the IT department.  The first cluster of machines appears to be web servers and corresponding backend services.  It would be unsurprising to find their RPM configurations were similar.</p>

<p>The second cluster is a series of executor machines of varying sizes, but presumably these would be configured similarly to one another.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_1"></script>




<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_2"></script>


<p>The second pair of clusters (3 &amp; 4) are small.  All of their names are similar (and furthermore, similar to some machines in other clusters), and so an IT administrator might wonder why they ended up in oddball small clusters.  Perhaps they have some spurious, non-standard packages installed that ought to be cleaned up.  Identifying these kinds of structure in a clustering is one common clustering application.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_3"></script>




<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_4"></script>


<p>Cluster 5 is a series of bugzilla web servers and corresponding back-end bugzilla data base services.  Although they were clustered together, we see that the web servers have a larger distance from the center, indicating a somewhat different configuration.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_5"></script>


<p>Cluster 6 represents a group of performance-related machines.  Not all of these machines occupy the same distance, even though most of their names are similar.  These are also the same series of machines as in clusters 3 &amp; 4.  Does this indicate spurious package installations, or some other legitimate configuration difference?  A question for the IT department&#8230;</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_6"></script>


<p>Cluster 7 is by far the largest.  It is primarily a combination of OpenStack machines and yet more perf machines.   This clustering was relatively stable &#8211; it appeared across multiple independent clustering runs.  Because of its stability I would suggest to an IT administrator that the performance and OpenStack machines are sharing some configuration similarities, and the performance machines in other clusters suggest that there might be yet more configuration anomalies.  Perhaps these were OpenStack nodes that were re-purposed as performance machines?  Yet another question for IT&#8230;</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_7"></script>


<p><a name="outliers"></a></p>

<h5>Outliers</h5>

<p>This last grouping represents machines which were &#8220;far&#8221; from any of the previous cluster centers.  They may be interpreted as &#8220;outliers&#8221; - machines that don&#8217;t fit any model category.  Of these the node <code>frodo</code> is clearly somebody&#8217;s personal machine, likely with a customized or idiosyncratic package configuration.  Unsurprising that it is farthest of all machines from any cluster, with distance 9.0.   The <code>jenkins</code> machine is also somewhat unique among the nodes, and so perhaps not surprising that its registers as anomalous.  The remaining machines match node series from other clusters.   Their large distance is another indication of spurious configurations for IT to examine.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=outliers"></script>


<p>I will conclude with another useful feature of Random Forest Models, which is that you can interrogate them for information such as which features were used most frequently.  Here is a histogram of model features (in this case, installed packages) that were used most frequently in the clustering model.  This particular histogram i sinteresting, as no feature was used more than twice.  The remaining features were all used exactly once.  This is a bit unusual for a Random Forest model.  Frequently some features are used commonly, with a longer tail.  This histogram is rather &#8220;flat,&#8221; which may be a consequence of there being many more features (over 4000 installed packages) than there are data elements (135 machines).  This makes the problem somewhat under-determined.  To its credit, the model still achieves a meaningful clustering.</p>

<p>Lastly I&#8217;ll note that full histogram length was 186; in other words, of the nearly 4400 installed packages, the Random Forest model used only 186 of them &#8211; a tiny fraction!  A nice illustration of Random Forest Clustering performing in the face of <a href="#payoff">high dimensionality</a>!</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=histogram"></script>


<p><head><style type="text/css">
.gist {max-height:500px; overflow:auto}
</style></head></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing Simplex Vertex Locations From Pairwise Object Distances]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/03/26/computing-simplex-vertex-locations-from-pairwise-vertex-distances/"/>
    <updated>2016-03-26T16:22:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/03/26/computing-simplex-vertex-locations-from-pairwise-vertex-distances</id>
    <content type="html"><![CDATA[<p>Suppose I have a collection of (N) objects, and distances d(j,k) between each pair of objects (j) and (k); that is, my objects are members of a <a href="https://en.wikipedia.org/wiki/Metric_space">metric space</a>.  I have no knowledge about my objects, beyond these pair-wise distances.  These objects could be construed as vertices in an (N-1) dimensional <a href="https://en.wikipedia.org/wiki/Simplex">simplex</a>.  However, since I have no spatial information about my objects, I first need a way to assign spatial locations to each object, in vector space R<sup>(N-1),</sup> with only my object distances to work with.</p>

<p>In this post I will derive an algorithm for assigning vertex locations in R<sup>(N-1)</sup> for each of N objects, using only pairwise object distances.</p>

<p>I will assume that N >= 2, since at least two object are required to define a pairwise distance.  The case N=2 is easy, as I can assign vertex 1 to the origin, and vertex 2 to the point d(1,2), to form a 1-simplex (i.e. a line segment) whose single edge is just the distance between the two objects.  I will also assume that my N objects are distinct; that is, each pair has a non-zero distance.</p>

<p>Next consider an arbitrary N, and suppose I have already added vertices 1 through k.  The next vertex (k+1) must obey the pairwise distance relations, as follows:</p>

<p><img src="http://mathurl.com/jm56vxq.png" alt="figure 1" /></p>

<p>Adding the new vertex (k+1) involves adding another dimension (k) to the simplex.  I define this new kth coordinate x(k) to be zero for the existing k vertices, as annotated above; only the new vertex (k+1) will have a non-zero kth coordinate.  Expanding the quadratic terms on the left yields the following form:</p>

<p><img src="http://mathurl.com/jtm7dpq.png" alt="figure 2" /></p>

<p>The squared terms for the coordinates of the new vertex (k+1) are inconvenient, however I can get rid of them by subtracting pairs of equations above.  For example, if I subtract equation 1 from the remaining k-1 equations (2 through k), these squared terms disappear, leaving me with the following system of k-1 equations, which we can see is linear in the 1st k-1 coordinates of the new vertex.  Therefore, I know I&#8217;ll be able to solve for those coordinates.  I can solve for the remaining kth coordinate by plugging it into the first distance equation:</p>

<p><img src="http://mathurl.com/haovm32.png" alt="figure 3" /></p>

<p>To clarify matters, the equations above can be re-written as the following matrix equation, solveable by any linear systems library:</p>

<p><img src="http://mathurl.com/h6qdtms.png" alt="figure 4" /></p>

<p>This gives me a recusion relation for adding a new vertex (k+1), given that I have already added the first k vertices.  The basis case of adding the first two vertices was already described above.  And so I can iteratively add all my vertices one at a time by applying the recursion relation.</p>

<p>As a corollary, assume that I have constructed a simplex having k vertices, as shown above, and I would like to assign a spatial location to a new object, (y), given its k distances to each vertex.  The corresponding distance relations are given by:</p>

<p><img src="http://mathurl.com/zdw9uv8.png" alt="figure 5" /></p>

<p>I can apply a derivation very similar to the one above, to obtain the following linear equation for the (k-1) coordinates of (y):</p>

<p><img src="http://mathurl.com/zvr5jre.png" alt="figure 6" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Efficient Multiplexing for Spark RDDs]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/02/08/efficient-multiplexing-for-spark-rdds/"/>
    <updated>2016-02-08T10:09:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/02/08/efficient-multiplexing-for-spark-rdds</id>
    <content type="html"><![CDATA[<p>In this post I&#8217;m going to propose a new abstract operation on <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark RDDs</a> &#8211; <strong>multiplexing</strong> &#8211; that makes some categories of operations on RDDs both easier to program and in many cases much faster.</p>

<p>My main working example will be the operation of splitting a collection of data elements into N randomly-selected subsamples.  This operation is quite common in machine learning, for the purpose of dividing data into a <a href="https://en.wikipedia.org/wiki/Test_set">training and testing set</a>, or the related task of <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics">creating folds for cross-validation</a>).</p>

<p>Consider the current standard RDD method for accomplishing this task, <code>randomSplit()</code>.  This method takes a collection of N weights, and returns N output RDDs, each of which contains a randomly-sampled subset of the input, proportional to the corresponding weight.  The <code>randomSplit()</code> method generates the jth output by running a random number generator (RNG) for each input data element and accepting all elements which are in the corresponding jth (normalized) weight range.  As a diagram, the process looks like this at each RDD partition:</p>

<p><img src="http://erikerlandson.github.com/assets/images/mux/randomsplit.png" title="Figure 1" alt="Figure 1" /></p>

<p>The observation I want to draw attention to is that to produce the N output RDDs, it has to run a random sampling over every element in the input <em>for each output</em>.  So if you are splitting into 10 outputs (e.g. for a 10-fold cross-validation), you are re-sampling your input 10 times, the only difference being that each output is created using a different acceptance range for the RNG output.</p>

<p>To see what this looks like in code, consider a simplified version of random splitting that just takes an integer <code>n</code> and always produces (n) equally-weighted outputs:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">splitSample</span><span class="o">[</span><span class="kt">T</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">T</span><span class="o">],</span> <span class="n">n</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">seed</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">42</span><span class="o">)</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">T</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="nc">Vector</span><span class="o">.</span><span class="n">tabulate</span><span class="o">(</span><span class="n">n</span><span class="o">)</span> <span class="o">{</span> <span class="n">j</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span> <span class="o">{</span> <span class="n">data</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span><span class="o">.</span><span class="n">setSeed</span><span class="o">(</span><span class="n">seed</span><span class="o">)</span>
</span><span class='line'>      <span class="n">data</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="n">unused</span> <span class="k">=&gt;</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="n">n</span><span class="o">)</span> <span class="o">==</span> <span class="n">j</span> <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>(Note that for this method to operate correctly, the RNG seed must be set to the same value each time, or the data will not be correctly partitioned)</p>

<p>While this approach to random splitting works fine, resampling the same data N times is somewhat wasteful.  However, it is possible to re-organize the computation so that the input data is sampled only once.  The idea is to run the RNG once per data element, and save the element into a randomly-chosen collection.  To make this work in the RDD compute model, all N output collections reside in a single row of an <em>intermediate</em> RDD &#8211; a &#8220;manifold&#8221; RDD.  Each output RDD then takes its data from the corresponding collection in the manifold RDD, as in this diagram:</p>

<p><img src="http://erikerlandson.github.com/assets/images/mux/multiplex.png" alt="Figure 2" /></p>

<p>If you abstract the diagram above into a generalized operation, you end up with methods that might like the following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">muxPartitions</span><span class="o">[</span><span class="kt">U</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">n</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">f</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="nc">Seq</span><span class="o">[</span><span class="kt">U</span><span class="o">],</span>
</span><span class='line'>  <span class="n">persist</span><span class="k">:</span> <span class="kt">StorageLevel</span><span class="o">)</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">U</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux</span> <span class="k">=</span> <span class="n">self</span><span class="o">.</span><span class="n">mapPartitionsWithIndex</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">f</span><span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}.</span><span class="n">persist</span><span class="o">(</span><span class="n">persist</span><span class="o">)</span>
</span><span class='line'>  <span class="nc">Vector</span><span class="o">.</span><span class="n">tabulate</span><span class="o">(</span><span class="n">n</span><span class="o">)</span> <span class="o">{</span> <span class="n">j</span> <span class="k">=&gt;</span> <span class="n">mux</span><span class="o">.</span><span class="n">mapPartitions</span> <span class="o">{</span> <span class="n">itr</span> <span class="k">=&gt;</span> <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="n">next</span><span class="o">()(</span><span class="n">j</span><span class="o">))</span> <span class="o">}</span> <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="n">flatMuxPartitions</span><span class="o">[</span><span class="kt">U</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">n</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">f</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="nc">Seq</span><span class="o">[</span><span class="kt">TraversableOnce</span><span class="o">[</span><span class="kt">U</span><span class="o">]],</span>
</span><span class='line'>  <span class="n">persist</span><span class="k">:</span> <span class="kt">StorageLevel</span><span class="o">)</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">U</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux</span> <span class="k">=</span> <span class="n">self</span><span class="o">.</span><span class="n">mapPartitionsWithIndex</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">f</span><span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}.</span><span class="n">persist</span><span class="o">(</span><span class="n">persist</span><span class="o">)</span>
</span><span class='line'>  <span class="nc">Vector</span><span class="o">.</span><span class="n">tabulate</span><span class="o">(</span><span class="n">n</span><span class="o">)</span> <span class="o">{</span> <span class="n">j</span> <span class="k">=&gt;</span> <span class="n">mux</span><span class="o">.</span><span class="n">mapPartitions</span> <span class="o">{</span> <span class="n">itr</span> <span class="k">=&gt;</span> <span class="n">itr</span><span class="o">.</span><span class="n">next</span><span class="o">()(</span><span class="n">j</span><span class="o">).</span><span class="n">toIterator</span> <span class="o">}</span> <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here, the operation of sampling is generalized to any user-supplied function that maps RDD partition data into a sequence of objects that are computed in a single pass, and then multiplexed to the final user-visible outputs.  Note that these functions take a <code>StorageLevel</code> argument that can be used to control the caching level of the internal &#8220;manifold&#8221; RDD.  This typically defaults to <code>MEMORY_ONLY</code>, so that the computation can be saved and re-used for efficiency.</p>

<p>An efficient split-sampling method based on multiplexing, as described above, might be written using <code>flatMuxPartitions</code> as follows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">splitSampleMux</span><span class="o">[</span><span class="kt">T</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">T</span><span class="o">],</span> <span class="n">n</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
</span><span class='line'>  <span class="n">persist</span><span class="k">:</span> <span class="kt">StorageLevel</span> <span class="o">=</span> <span class="nc">MEMORY_ONLY</span><span class="o">,</span>
</span><span class='line'>  <span class="n">seed</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">42</span><span class="o">)</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">T</span><span class="o">]]</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">rdd</span><span class="o">.</span><span class="n">flatMuxPartitions</span><span class="o">(</span><span class="n">n</span><span class="o">,</span> <span class="o">(</span><span class="n">id</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span><span class="o">.</span><span class="n">setSeed</span><span class="o">(</span><span class="n">id</span><span class="o">.</span><span class="n">toLong</span> <span class="o">*</span> <span class="n">seed</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">samples</span> <span class="k">=</span> <span class="nc">Vector</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">n</span><span class="o">)</span> <span class="o">{</span> <span class="n">scala</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">mutable</span><span class="o">.</span><span class="nc">ArrayBuffer</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="o">}</span>
</span><span class='line'>    <span class="n">data</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">e</span> <span class="k">=&gt;</span> <span class="n">samples</span><span class="o">(</span><span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="n">n</span><span class="o">))</span> <span class="o">+=</span> <span class="n">e</span> <span class="o">}</span>
</span><span class='line'>    <span class="n">samples</span>
</span><span class='line'>  <span class="o">},</span> <span class="n">persist</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>To test whether multiplexed RDDs actually improve compute efficiency, I collected run-time data at various split values of <code>n</code> (from 1 to 10), for both the non-multiplexing logic (equivalent to the standard <code>randomSplit</code>) and the multiplexed version:</p>

<p><img src="http://erikerlandson.github.com/assets/images/mux/benchmark.png" title="Figure 3" alt="Figure 3" /></p>

<p>As the timing data above show, the computation required to run a non-multiplexed version grows linearly with <code>n</code>, just as predicted.  The multiplexed version, by computing the (n) outputs in a single pass, takes a nearly constant amount of time regardless of how many samples the input is split into.</p>

<p>There are other potential applications for multiplexed RDDs.  Consider the following tuple-based versions of multiplexing:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">muxPartitions</span><span class="o">[</span><span class="kt">U1</span> <span class="kt">:ClassTag</span>, <span class="kt">U2</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">f</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">U1</span><span class="o">,</span> <span class="n">U2</span><span class="o">),</span>
</span><span class='line'>  <span class="n">persist</span><span class="k">:</span> <span class="kt">StorageLevel</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">U1</span><span class="o">],</span> <span class="nc">RDD</span><span class="o">[</span><span class="kt">U2</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux</span> <span class="k">=</span> <span class="n">self</span><span class="o">.</span><span class="n">mapPartitionsWithIndex</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">f</span><span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}.</span><span class="n">persist</span><span class="o">(</span><span class="n">persist</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux1</span> <span class="k">=</span> <span class="n">mux</span><span class="o">.</span><span class="n">mapPartitions</span><span class="o">(</span><span class="n">itr</span> <span class="k">=&gt;</span> <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux2</span> <span class="k">=</span> <span class="n">mux</span><span class="o">.</span><span class="n">mapPartitions</span><span class="o">(</span><span class="n">itr</span> <span class="k">=&gt;</span> <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">_2</span><span class="o">))</span>
</span><span class='line'>  <span class="o">(</span><span class="n">mux1</span><span class="o">,</span> <span class="n">mux2</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="n">flatMuxPartitions</span><span class="o">[</span><span class="kt">U1</span> <span class="kt">:ClassTag</span>, <span class="kt">U2</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">f</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="nc">TraversableOnce</span><span class="o">[</span><span class="kt">U1</span><span class="o">],</span> <span class="nc">TraversableOnce</span><span class="o">[</span><span class="kt">U2</span><span class="o">]),</span>
</span><span class='line'>  <span class="n">persist</span><span class="k">:</span> <span class="kt">StorageLevel</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">U1</span><span class="o">],</span> <span class="nc">RDD</span><span class="o">[</span><span class="kt">U2</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux</span> <span class="k">=</span> <span class="n">self</span><span class="o">.</span><span class="n">mapPartitionsWithIndex</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="nc">Iterator</span><span class="o">.</span><span class="n">single</span><span class="o">(</span><span class="n">f</span><span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">itr</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}.</span><span class="n">persist</span><span class="o">(</span><span class="n">persist</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux1</span> <span class="k">=</span> <span class="n">mux</span><span class="o">.</span><span class="n">mapPartitions</span><span class="o">(</span><span class="n">itr</span> <span class="k">=&gt;</span> <span class="n">itr</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">_1</span><span class="o">.</span><span class="n">toIterator</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mux2</span> <span class="k">=</span> <span class="n">mux</span><span class="o">.</span><span class="n">mapPartitions</span><span class="o">(</span><span class="n">itr</span> <span class="k">=&gt;</span> <span class="n">itr</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">toIterator</span><span class="o">)</span>
</span><span class='line'>  <span class="o">(</span><span class="n">mux1</span><span class="o">,</span> <span class="n">mux2</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Suppose you wanted to run an input-validation filter on some data, sending the data that pass validation into one RDD, and data that failed into a second RDD, paired with information about the error that occurred.  Data validation is a potentially expensive operation.  With multiplexing, you can easily write the filter to operate in a single efficient pass to obtain both the valid stream and the stream of error-data:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">validate</span><span class="o">[</span><span class="kt">T</span> <span class="kt">:ClassTag</span><span class="o">](</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">T</span><span class="o">],</span> <span class="n">validator</span><span class="k">:</span> <span class="kt">T</span> <span class="o">=&gt;</span> <span class="nc">Boolean</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">rdd</span><span class="o">.</span><span class="n">flatMuxPartitions</span><span class="o">((</span><span class="n">id</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">valid</span> <span class="k">=</span> <span class="n">scala</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">mutable</span><span class="o">.</span><span class="nc">ArrayBuffer</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">bad</span> <span class="k">=</span> <span class="n">scala</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">mutable</span><span class="o">.</span><span class="nc">ArrayBuffer</span><span class="o">.</span><span class="n">empty</span><span class="o">[(</span><span class="kt">T</span>, <span class="kt">Exception</span><span class="o">)]</span>
</span><span class='line'>    <span class="n">data</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">e</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(!</span><span class="n">validator</span><span class="o">(</span><span class="n">e</span><span class="o">))</span> <span class="k">throw</span> <span class="k">new</span> <span class="nc">Exception</span><span class="o">(</span><span class="s">&quot;returned false&quot;</span><span class="o">)</span>
</span><span class='line'>        <span class="n">valid</span> <span class="o">+=</span> <span class="n">e</span>
</span><span class='line'>      <span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">case</span> <span class="n">err</span><span class="k">:</span> <span class="kt">Exception</span> <span class="o">=&gt;</span> <span class="n">bad</span> <span class="o">+=</span> <span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">err</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="o">(</span><span class="n">valid</span><span class="o">,</span> <span class="n">bad</span><span class="o">)</span>
</span><span class='line'>  <span class="o">})</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>RDD multiplexing is currently a <a href="https://github.com/willb/silex/pull/50">PR against the silex project</a>.  The code I used to run the timing experiments above is <a href="https://github.com/erikerlandson/silex/blob/blog/muxrdd/src/main/scala/com/redhat/et/silex/sample/split.scala#L90">saved for posterity here</a>.</p>

<p>Happy multiplexing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The 'prepare' operation considered harmful in Algebird aggregation]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/"/>
    <updated>2015-11-24T16:32:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird</id>
    <content type="html"><![CDATA[<p>I want to make an argument that the Algebird <a href="http://twitter.github.io/algebird/#com.twitter.algebird.Aggregator">Aggregator</a> design, in particular its use of the <code>prepare</code> operation in a map-reduce context, has substantial inefficiencies, compared to an equivalent formulation that is more directly suited to taking advantage of Scala&#8217;s <a href="http://www.scala-lang.org/api/current/index.html#scala.collection.Seq">aggregate method on collections</a> method.</p>

<p>Consider the definition of aggregation in the Aggregator class:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">inputs</span><span class="k">:</span> <span class="kt">TraversableOnce</span><span class="o">[</span><span class="kt">A</span><span class="o">])</span><span class="k">:</span> <span class="kt">C</span> <span class="o">=</span> <span class="n">present</span><span class="o">(</span><span class="n">reduce</span><span class="o">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">prepare</span><span class="o">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can see that it is a standard map/reduce operation, where <code>reduce</code> is defined as a monoidal (or semigroup &#8211; more on this later) operation. Under the hood, it boils down to an invocation of Scala&#8217;s <code>reduceLeft</code> method.  The key thing to notice is that the role of <code>prepare</code> is to map a collection of data elements into the required monoids, which are then aggregated using that monoid&#8217;s <code>plus</code> operation.  In other words, <code>prepare</code> converts data elements into &#8220;singleton&#8221; monoids each representing a data element.</p>

<p>Now, if the monoid in question is simple, say some numeric type, this conversion is free, or nearly so.  For example, the conversion of an integer into the &#8220;integer monoid&#8221; is a no-op.  However, there are other kinds of &#8220;non-trivial&#8221; monoids, for which the conversion of a data element into its corresponding monoid may be costly.  In this post, I will be using the monoid defined by Scala Set[Int], where the monoid <code>plus</code> operation is set union, and of course the <code>zero</code> element is the empty set.</p>

<p>Consider the process of defining an Algebird aggregator for the task of generating the set of unique elements in a data set.  The corresponding <code>prepare</code> operation is: <code>prepare(e: Int) = Set(e)</code>.  A monoid trait that encodes this idea might look like the following.  (the code I used in this post can be found <a href="https://gist.github.com/erikerlandson/d96dc553bc51e0eb5e4b">here</a>)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// an algebird-like monoid with the &#39;prepare&#39; operation</span>
</span><span class='line'><span class="k">trait</span> <span class="nc">PreparedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">zero</span><span class="k">:</span> <span class="kt">M</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">plus</span><span class="o">(</span><span class="n">m1</span><span class="k">:</span> <span class="kt">M</span><span class="o">,</span> <span class="n">m2</span><span class="k">:</span> <span class="kt">M</span><span class="o">)</span><span class="k">:</span> <span class="kt">M</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">prepare</span><span class="o">(</span><span class="n">e</span><span class="k">:</span> <span class="kt">E</span><span class="o">)</span><span class="k">:</span> <span class="kt">M</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// a PreparedMonoid for a set of integers.  monoid operator is set union.</span>
</span><span class='line'><span class="k">object</span> <span class="nc">intSetPrepared</span> <span class="k">extends</span> <span class="nc">PreparedMonoid</span><span class="o">[</span><span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">zero</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">plus</span><span class="o">(</span><span class="n">m1</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">],</span> <span class="n">m2</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span> <span class="k">=</span> <span class="n">m1</span> <span class="o">++</span> <span class="n">m2</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">prepare</span><span class="o">(</span><span class="n">e</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">(</span><span class="n">e</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">implicit</span> <span class="k">class</span> <span class="nc">SeqWithMapReduce</span><span class="o">[</span><span class="kt">E</span><span class="o">](</span><span class="n">seq</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">E</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// algebird map/reduce Aggregator model</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">mrPrepared</span><span class="o">[</span><span class="kt">M</span><span class="o">](</span><span class="n">mon</span><span class="k">:</span> <span class="kt">PreparedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">])</span><span class="k">:</span> <span class="kt">M</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">seq</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">mon</span><span class="o">.</span><span class="n">prepare</span><span class="o">).</span><span class="n">reduceLeft</span><span class="o">(</span><span class="n">mon</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>If we unpack the above code, as applied to <code>intSetPrepared</code>, we are instantiating a new Set object, containing a single value, for every single input data element.</p>

<p>But there is a potentially better model of aggregation, exemplified by the Scala <code>aggregate</code> method.  This method does not use a <code>prepare</code> operation.  It uses a zero value and a monoidal operator, which the Scala docs refer to as <code>combop</code>, but it also uses an &#8220;update&#8221; operation, that defines how to update the monoid object, directly, with a single element, referred to as <code>seqop</code> in Scala&#8217;s documentation.  This idea can also be encoded as a flavor of monoid, enhanced with an <code>update</code> method:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// an algebird-like monoid with &#39;update&#39; operation</span>
</span><span class='line'><span class="k">trait</span> <span class="nc">UpdatedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">zero</span><span class="k">:</span> <span class="kt">M</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">plus</span><span class="o">(</span><span class="n">m1</span><span class="k">:</span> <span class="kt">M</span><span class="o">,</span> <span class="n">m2</span><span class="k">:</span> <span class="kt">M</span><span class="o">)</span><span class="k">:</span> <span class="kt">M</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">update</span><span class="o">(</span><span class="n">m</span><span class="k">:</span> <span class="kt">M</span><span class="o">,</span> <span class="n">e</span><span class="k">:</span> <span class="kt">E</span><span class="o">)</span><span class="k">:</span> <span class="kt">M</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// an equivalent UpdatedMonoid for a set of integers</span>
</span><span class='line'><span class="k">object</span> <span class="nc">intSetUpdated</span> <span class="k">extends</span> <span class="nc">UpdatedMonoid</span><span class="o">[</span><span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">zero</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">plus</span><span class="o">(</span><span class="n">m1</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">],</span> <span class="n">m2</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span> <span class="k">=</span> <span class="n">m1</span> <span class="o">++</span> <span class="n">m2</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">update</span><span class="o">(</span><span class="n">m</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">],</span> <span class="n">e</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="n">m</span> <span class="o">+</span> <span class="n">e</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">implicit</span> <span class="k">class</span> <span class="nc">SeqWithMapReduceUpdated</span><span class="o">[</span><span class="kt">E</span><span class="o">](</span><span class="n">seq</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">E</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// map/reduce logic, taking advantage of scala &#39;aggregate&#39;</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">mrUpdatedAggregate</span><span class="o">[</span><span class="kt">M</span><span class="o">](</span><span class="n">mon</span><span class="k">:</span> <span class="kt">UpdatedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">])</span><span class="k">:</span> <span class="kt">M</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">seq</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="n">mon</span><span class="o">.</span><span class="n">zero</span><span class="o">)(</span><span class="n">mon</span><span class="o">.</span><span class="n">update</span><span class="o">,</span> <span class="n">mon</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This arrangement promises more efficiency when aggregating w.r.t. nontrivial monoids, by avoiding the construction of &#8220;singleton&#8221; monoids for each data element.  The following demo confirms that for the Set-based monoid, it is over 10 times faster:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">:</span><span class="kt">load</span> <span class="kt">/home/eje/scala/prepare.scala</span>
</span><span class='line'><span class="nc">Loading</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">eje</span><span class="o">/</span><span class="n">scala</span><span class="o">/</span><span class="n">prepare</span><span class="o">.</span><span class="n">scala</span><span class="o">...</span>
</span><span class='line'><span class="n">defined</span> <span class="n">module</span> <span class="n">prepare</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">import</span> <span class="nn">prepare._</span>
</span><span class='line'><span class="k">import</span> <span class="nn">prepare._</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">Vector</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="mi">1000000</span><span class="o">)</span> <span class="o">{</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="n">data</span><span class="k">:</span> <span class="kt">scala.collection.immutable.Vector</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Vector</span><span class="o">(</span><span class="mi">7</span><span class="o">,</span> <span class="mi">9</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">7</span><span class="o">,...</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Verify that output is the same for both implementations:</span>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">mrPrepared</span><span class="o">(</span><span class="n">intSetPrepared</span><span class="o">)</span>
</span><span class='line'><span class="n">res0</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">9</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">7</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">8</span><span class="o">,</span> <span class="mi">4</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// results are the same</span>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">mrUpdatedAggregate</span><span class="o">(</span><span class="n">intSetUpdated</span><span class="o">)</span>
</span><span class='line'><span class="n">res1</span><span class="k">:</span> <span class="kt">Set</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">9</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">7</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">8</span><span class="o">,</span> <span class="mi">4</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Compare timings of prepare-based versus update-based aggregation</span>
</span><span class='line'><span class="c1">// (benchmark values are returned in seconds)</span>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">benchmark</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span> <span class="o">{</span> <span class="n">data</span><span class="o">.</span><span class="n">mrPrepared</span><span class="o">(</span><span class="n">intSetPrepared</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="n">res2</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">0.2957673056</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// update-based aggregation is 10 times faster</span>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">benchmark</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span> <span class="o">{</span> <span class="n">data</span><span class="o">.</span><span class="n">mrUpdatedAggregate</span><span class="o">(</span><span class="n">intSetUpdated</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="n">res3</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">0.027041249300000004</span>
</span></code></pre></td></tr></table></div></figure>


<p>It is also possible to apply Scala&#8217;s <code>aggregate</code> to a monoid enhanced with <code>prepare</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">implicit</span> <span class="k">class</span> <span class="nc">SeqWithMapReducePrepared</span><span class="o">[</span><span class="kt">E</span><span class="o">](</span><span class="n">seq</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">E</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// using &#39;aggregate&#39; with prepared op</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">mrPreparedAggregate</span><span class="o">[</span><span class="kt">M</span><span class="o">](</span><span class="n">mon</span><span class="k">:</span> <span class="kt">PreparedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">])</span><span class="k">:</span> <span class="kt">M</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">seq</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="n">mon</span><span class="o">.</span><span class="n">zero</span><span class="o">)((</span><span class="n">m</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">mon</span><span class="o">.</span><span class="n">plus</span><span class="o">(</span><span class="n">m</span><span class="o">,</span> <span class="n">mon</span><span class="o">.</span><span class="n">prepare</span><span class="o">(</span><span class="n">e</span><span class="o">)),</span> <span class="n">mon</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Although this turns out to be measurably faster than the literal map-reduce implementation, it is still not nearly as fast as the variation using <code>update</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">benchmark</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span> <span class="o">{</span> <span class="n">data</span><span class="o">.</span><span class="n">mrPreparedAggregate</span><span class="o">(</span><span class="n">intSetPrepared</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="n">res2</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">0.1754636707</span>
</span></code></pre></td></tr></table></div></figure>


<p>Readers familiar with Algebird may be wondering about my use of monoids above, when the <code>Aggregator</code> interface is actually based on semigroups.  This is important, since building on Scala&#8217;s <code>aggregate</code> function requires a zero element that semigroups do not have.  Although I believe it might be worth considering changing <code>Aggregator</code> to use monoids, another sensible option is to change the internal logic for the subclass <code>AggregatorMonoid</code>, which does require a monoid, or possibly just define a new <code>AggregatorMonoidUpdated</code> subclass.</p>

<p>A final note on compatability: note that any monoid enhanced with <code>prepare</code> can be converted into an equivalent monoid enhanced with <code>update</code>, as demonstrated by this factory function:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">UpdatedMonoid</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// create an UpdatedMonoid from a PreparedMonoid</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">apply</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">](</span><span class="n">mon</span><span class="k">:</span> <span class="kt">PreparedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">])</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">UpdatedMonoid</span><span class="o">[</span><span class="kt">M</span>, <span class="kt">E</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">zero</span> <span class="k">=</span> <span class="n">mon</span><span class="o">.</span><span class="n">zero</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">plus</span><span class="o">(</span><span class="n">m1</span><span class="k">:</span> <span class="kt">M</span><span class="o">,</span> <span class="n">m2</span><span class="k">:</span> <span class="kt">M</span><span class="o">)</span> <span class="k">=</span> <span class="n">mon</span><span class="o">.</span><span class="n">plus</span><span class="o">(</span><span class="n">m1</span><span class="o">,</span> <span class="n">m2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">update</span><span class="o">(</span><span class="n">m</span><span class="k">:</span> <span class="kt">M</span><span class="o">,</span> <span class="n">e</span><span class="k">:</span> <span class="kt">E</span><span class="o">)</span> <span class="k">=</span> <span class="n">mon</span><span class="o">.</span><span class="n">plus</span><span class="o">(</span><span class="n">m</span><span class="o">,</span> <span class="n">mon</span><span class="o">.</span><span class="n">prepare</span><span class="o">(</span><span class="n">e</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Very Fast Reservoir Sampling]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling/"/>
    <updated>2015-11-20T11:27:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling</id>
    <content type="html"><![CDATA[<p>In this post I will demonstrate how to do reservoir sampling orders of magnitude faster than the traditional &#8220;naive&#8221; reservoir sampling algorithm, using a fast high-fidelity approximation to the reservoir sampling-gap distribution.</p>

<blockquote><p>The code I used to collect the data for this post can be viewed <a href="https://github.com/erikerlandson/silex/blob/blog/reservoir/src/main/scala/com/redhat/et/silex/sample/reservoir/reservoir.scala">here</a>.  I generated the plots using the <a href="https://github.com/quantifind/wisp">quantifind WISP</a> project.</p>

<p>Update (April 4, 2016): my colleague <a href="http://rnowling.github.io/">RJ Nowling</a> ran across a <a href="http://www.ittc.ku.edu/~jsv/Papers/Vit87.RandomSampling.pdf">paper by J.S. Vitter</a> that shows Vitter developed the trick of accelerating sampling with a sampling-gap distribution in 1987 &#8211; I re-invented Vitter&#8217;s wheel 30 years after the fact!  I&#8217;m surprised it never caught on, as it is not much harder to implement than the naive version.</p></blockquote>

<p>In a <a href="http://erikerlandson.github.io/blog/2014/09/11/faster-random-samples-with-gap-sampling/">previous post</a>, I showed that random Bernoulli and Poisson sampling could be made much faster by modeling the <em>sampling gap distribution</em> for the corresponding sampling distributions.  More recently, I also began exploring whether <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">reservoir sampling</a> might also be optimized using the gap sampling technique, by deriving the <a href="http://erikerlandson.github.io/blog/2015/08/17/the-reservoir-sampling-gap-distribution/">reservoir sampling gap distribution</a>.  For a sampling reservoir of size (R), starting at data element (j), the probability distribution of the sampling gap is:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure6.png" title="Figure 1" alt="Figure 1" /></p>

<p>Modeling a sampling gap distribution is a powerful tool for optimizing a sampling algorithm, but it presupposes that you can actually draw values from that distribution substantially faster than just applying a random process to drawing each data element.  I was unable to come up with a &#8220;direct&#8221; algorithm for drawing samples from P(k) above (I suspect none exists), however I also know the CDF F(k), so it <em>is</em> possible to apply <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inversion sampling</a>, which runs in logarithmic time w.r.t the desired accuracy.  Although its logarithmic cost effectively guarantees that it will be a net efficiency win for sufficiently large (j), it still involves a substantial number of computations to yield its samples, and it seems unlikely to be competitive with straight &#8220;naive&#8221; reservoir sampling over many real-world data sizes, where (j) may never grow very large.</p>

<p>Well, if exact computations are too expensive, we can always look for a fast approximation.  Consider the original &#8220;first principles&#8221; formula for the sampling gap P(k):</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/figure2.png" title="Figure 2" alt="Figure 2" /></p>

<p>As the figure above alludes to, if (j) is relatively large compared to (k), then values (j+1),(j+2)&#8230;(j+k) are all going to be effectively &#8220;close&#8221; to (j), and so we can replace them all with (j) as an approximation.  Note that the resulting approximation is just the PMF of the <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a>, with probability of success p=(R/j), and we already saw how to efficiently draw values from a geometric distribution from our experience with Bernoulli sampling.</p>

<p>Do we have any reason to hope that this approximation will be useful?  For reasons that are similar to those for Bernoulli gap sampling, it will only be efficient to employ gap sampling when the probability (R/j) becomes small enough.  From our experiences with Bernoulli sampling that is <em>at least</em> j>=2R.  So, we have some assurance that (j) itself will be never be <em>very</em> small.  What about (k)?  Note that a geometric distribution &#8220;favors&#8221; smaller values of (k) &#8211; that is, small values of (k) have the highest probabilities.  In fact, the smaller that (j) is, the larger the probability (R/j) is, and so the more likely that (k) values that are small relative to (j) will be the frequent ones.  It is also promising that the true distribution for P(k) <em>also</em> favors smaller values of (k) (in fact it favors them even a bit more strongly than the approximation).</p>

<p>Although it is encouraging, it is also clear that my argument above is limited to heuristic hand-waving.  What does this approximation really <em>look</em> like, compared to the true distribution?  Fortunately, it is easy to plot both distributions numerically, since we now know the formulas for both:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/CDFs_R=10.png" title="Figure 3" alt="Figure 3" /></p>

<p>The plot above shows that, in fact, the geometric approximation is a <em>surprisingly good</em> approximation to the true distribution!  Furthermore, the approximation remains good as both (j) and (k) grow larger.</p>

<p>Our numeric eye-balling looks quite promising.  Is there an effective way to <em>measure</em> how good this approximation is?  One useful measure is the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov D statistic</a>, which is just the maximum absolute error between two cumulative distributions.  Here is a plot of the D statistic for reservoir size R=10, as (j) varies across several magnitudes:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/R=10.png" title="Figure 4" alt="Figure 4" /></p>

<p>This plot is also good news: we can see that deviation, as measured by D, remains bounded at a small value (less than 0.0262).  As this is for the specific value R=10, we also want to know how things change as reservoir size changes:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/R=all.png" title="Figure 5" alt="Figure 5" /></p>

<p>The news is still good!  As reservoir size grows, the approximation only gets better: the D values get smaller as R increases, and remain asymptotically bounded as (j) increases.</p>

<p>Now we have some numeric assurance that the geometric approximation is a good one, and stays good as reservoir size grows and sampling runs get longer.  However, we should also verify that an actual implementation of the approximation works as expected.</p>

<p>Here is pseudocode for an implementation of reservoir sampling using the fast geometric approximation:</p>

<pre><code>// data is array to sample from
// R is the reservoir size
function reservoirFast(data: Array, R: Int) {
  n = data.length
  // Initialize reservoir with first R elements of data:
  res = data[0 until R]
  // Until this threshold, use traditional sampling.  This value may
  // depend on performance characteristics of random number generation and/or
  // numeric libraries:
  t = 4 * R
  j = 1 + R
  while (j &lt; n  &amp;&amp;  j &lt;= t) {
    k = randomInt(j) // random integer &gt;= 0 and &lt; j
    if (k &lt; R) res[k] = data[j]
    j = j + 1
  }
  // Once gaps become significant, it pays to do gap sampling
  while (j &lt; n) {
    // draw gap size (g) from geometric distribution with probability p = R/j
    p = R / j
    u = randomFloat() // random float &gt; 0 and &lt;= 1
    g = floor(log(u) / log(1-p))
    j = j + g
    if (j &lt; n) {
      k = randomInt(R)
      res[k] = data[j]
    }
    j = j + 1
  }
  // return the reservoir
  return res
}
</code></pre>

<p>Following is a plot that shows two-sample D statistics, comparing the distribution in sample gaps between runs of the exact &#8220;naive&#8221; reservoir sampling with the fast geometric approximation:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/D_naive_vs_fast.png" title="Figure 6" alt="Figure 6" /></p>

<p>As expected, the measured difference in sampling characteristics between naive and fast approximation are small, confirming the numeric predictions.</p>

<p>Since the point of this exercise was to achieve faster random sampling, it remains to measure what kind of speed improvements the fast approximation provides.  As a point of reference, here is a plot of run times for reservoir sampling over 10<sup>8</sup> integers:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/naive_sample_time_vs_R.png" title="Figure 7" alt="Figure 7" /></p>

<p>As expected, sample time remains constant at around 1.5 seconds, regardless of reservoir size, since the naive algorithm always samples from its RNG per each sample.</p>

<p>Compare this to the corresponding plot for the fast geometric approximation:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/gap_sample_times_vs_R.png" title="Figure 8" alt="Figure 8" /></p>

<p>Firstly, we see that the sampling times are <em>much faster</em>, as originally anticipated in my <a href="http://erikerlandson.github.io/blog/2015/08/17/the-reservoir-sampling-gap-distribution/">previous post</a> &#8211; in the neighborhood of 3 orders of magnitude faster.  Secondly, we see that the sampling times do increase as a linear function of reservoir size.  Based on our experience with Bernoulli gap sampling, this is expected; the sampling probabilities are given by (R/j), and therefore the amount of sampling is proportional to R.</p>

<p>Another property anticipated in my previous post was that the efficiency of gap sampling should continue to increase as the amount of data sampled grows; the sampling probability being (R/j), the probability of sampling decreases as j gets larger, and so the corresponding gap sizes grow.  The following plot verifies this property, holding reservoir size R constant, and increasing the data size:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir2/gap_sampling_efficiency.png" title="Figure 9" alt="Figure 9" /></p>

<p>The sampling time (per million elements) decreases as the sample size grows, as predicted by the formula.</p>

<p>In conclusion, I have demonstrated that a geometric distribution can be used as a high quality approximation to the true sampling gap distribution for reservoir sampling, which allows reservoir sampling to be performed much faster than the naive algorithm while still retaining sampling quality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Library of Binary Tree Algorithms as Mixable Scala Traits]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/"/>
    <updated>2015-09-26T12:43:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits</id>
    <content type="html"><![CDATA[<p>In this post I am going to describe some work I&#8217;ve done recently on a system of Scala traits that support tree-based collection algorithms prefix-sum, nearest key query and value increment in a mixable format, all backed by Red-Black balanced tree logic, which is also a fully inheritable trait.</p>

<blockquote><p>(update) Since I wrote this post, the code has evolved into a <a href="https://github.com/isarn/isarn-collections">library on the isarn project</a>. The original source files, containing the exact code fragments discussed in the remainder of this post, are preserved for posterity <a href="https://github.com/erikerlandson/silex/tree/blog/rbtraits/src/main/scala/com/redhat/et/silex/maps">here</a>.</p></blockquote>

<p>This post eventually became a bit more sprawling and &#8220;tl/dr&#8221; than I was expecting, so by way of apology, here is a table of contents with links:</p>

<ol>
<li><a href="#motivation">Motivating Use Case</a></li>
<li><a href="#overview">Library Overview</a></li>
<li><a href="#redblack">A Red-Black Tree Base Class</a></li>
<li><a href="#nodemap">Node Inheritance Example: NodeMap[K,V]</a></li>
<li><a href="#orderedmaplike">Collection Trait Example: OrderedMapLike[K,V,IN,M]</a></li>
<li><a href="#orderedmap">Collection Example: OrderedMap[K,V]</a></li>
<li><a href="#mixing">Finale: Trait Mixing</a></li>
</ol>


<p><a name="motivation"></a></p>

<h5>A Motivating Use Case</h5>

<p>The skeptical programmer may be wondering what the point of Yet Another Map Collection really is, much less an entire class hierarchy.  The use case that inspired this work was <a href="https://github.com/twitter/algebird/pull/495">my project</a> of implementing the <a href="https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf">t-digest algorithm</a>.  Discussion of t-digest is beyond the scope of this post, but suffice it to say that constructing a t-digest requires the maintenance of a collection of &#8220;cluster&#8221; objects, that needs to satisfy the following several properties:</p>

<ol>
<li>an entry contains one <strong>or more</strong> cluster objects at a given numeric location</li>
<li>entries are maintained in a numeric key order</li>
<li>entries will be frequently inserted and deleted, in arbitrary order</li>
<li>given a numeric key value, be able to find the entry nearest to that value</li>
<li>given a key, compute a <a href="https://en.wikipedia.org/wiki/Prefix_sum">prefix-sum</a> for that value</li>
<li>all of the above should be bounded by logarithmic time complexity</li>
</ol>


<p>Propreties 2,3 and 6 are commonly satisfied by a map structure backed by some variety of balanced tree representation, of which the best-known is the <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">Red-Black tree</a>.</p>

<p>Properties 1, 4 and 5 are more interesting.  Property 1 &#8211; representing a collection of multiple objects at each entry &#8211; can be accomplished in a generalizable way by noting that a collection is representable as a monoid, and so supporting values that can be incremented with respect to a <a href="http://twitter.github.io/algebird/index.html#com.twitter.algebird.Monoid">user-supplied monoid relation</a> can satisfy property-1, but also can support many other kinds of update, including but not limited to classical numeric incrementing operations.</p>

<p>Properties 4 and 5 &#8211; nearest-entry queries and prefix-sum queries &#8211; are also both supportable in logarithmic time using a tree data structure, provided that tree is balanced.  Again, the details of the algorithms are out of the current scope, however they are not extremely complex, and their implementations are available in the code.</p>

<p>A reader with their software engineering hat on will notice that these properties are <em>orthogonal</em>.  A programmer might be interested in a data structure supporting any one of them, or in some mixed combination.   This kind of situation fairly shouts &#8220;Scala traits&#8221; (or, alternatively, interfaces in Java, etc).  With that idea in mind, I designed a system of Scala collection traits that support all of the above properties, in a pure trait form that is fully &#8220;mixable&#8221; by the programmer, so that one can use exactly the properties needed, but not pay for anything else.</p>

<p><a name="overview"></a></p>

<h5>Library Overview</h5>

<p>The library consists broadly of 3 kinds of traits:</p>

<ul>
<li>tree node traits &#8211; implement core tree support for some functionality</li>
<li>collection traits &#8211; provide additional collection API methods the user</li>
<li>collections &#8211; instantiate a usable incarnation of a collection</li>
</ul>


<p>For the programmer who wishes to either create a trait mixture, or add new mixable traits, the collections also function as reference implementations.</p>

<p>The three tables that follow summarize the currently available traits of each kind listed above.  They are (at the time of this posting) all under the package namespace <code>com.redhat.et.silex.maps</code>:</p>

<p><head><style>
table, th, td {
border: 1px solid black;
border-collapse: collapse;
}
th, td {
padding: 10px;
}
th {
text-align: center;
}
</style></head></p>

<table>
<caption>Tree Node Traits</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>Node[K]</td> <td>redblack.tree</td><td>Fundamental Red-Black tree functionality</td></tr>
<tr><td>NodeMap[K,V]</td><td>ordered.tree</td><td>Support a mapping from keys to values</td></tr>
<tr><td>NodeNear[K]</td><td>nearest.tree</td><td>Nearest-entry query (key-only)</td></tr>
<tr><td>NodeNearMap[K,V]</td><td>nearest.tree</td><td>Nearest-entry query for key/value maps</td></tr>
<tr><td>NodeInc[K,V]</td><td>increment.tree</td><td>Increment values w.r.t. a monoid</td></tr>
<tr><td>NodePS[K,V,P]</td><td>prefixsum.tree</td><td>Prefix sum queries by key (w.r.t. a monoid)</td></tr>
</table>




<br>


<table>
<caption>Collection Traits</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>OrderedSetLike[K,IN,M]</td><td>ordered</td><td>ordered set of keys</td></tr>
<tr><td>OrderedMapLike[K,V,IN,M]</td><td>ordered</td><td>ordered key/value map</td></tr>
<tr><td>NearestSetLike[K,IN,M]</td><td>nearest</td><td>nearest entry query on keys</td></tr>
<tr><td>NearestMapLike[K,V,IN,M]</td><td>nearest</td><td>nearest entry query on key/value map</td></tr>
<tr><td>IncrementMapLike[K,V,IN,M]</td><td>increment</td><td>increment values w.r.t a monoid</td></tr>
<tr><td>PrefixSumMapLike[K,V,P,IN,M]</td><td>prefixsum</td><td>prefix sum queries w.r.t. a monoid</td></tr>
</table>




<br>


<table>
<caption>Concrete Collections</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>OrderedSet[K]</td><td>ordered</td><td>ordered set</td></tr>
<tr><td>OrderedMap[K,V]</td><td>ordered</td><td>ordered key/value map</td></tr>
<tr><td>NearestSet[K]</td><td>nearest</td><td>ordered set with nearest-entry query</td></tr>
<tr><td>NearestMap[K,V]</td><td>nearest</td><td>ordred map with nearest-entry query</td></tr>
<tr><td>IncrementMap[K,V]</td><td>increment</td><td>ordered map with value increment w.r.t. a monoid</td></tr>
<tr><td>PrefixSumMap[K,V,P]</td><td>prefixsum</td><td>ordered map with prefix sum query w.r.t. a monoid</td></tr>
</table>




<br>


<p>The following diagram summarizes the organization and inheritance relationships of the classes.</p>

<p><img src="http://erikerlandson.github.com/assets/images/rbtraits/rbtraits.png" alt="diagram" /></p>

<p><a name="redblack"></a></p>

<h5>A Red/Black Tree Base Class</h5>

<p>The most fundamental trait in this hierarchy is the trait that embodies Red-Black balancing; a &#8220;red-black-ness&#8221; trait, as it were.  This trait supplies the axiomatic tree operations of insertion, deletion and key lookup, where the Red-Black balancing operations are encapsulated for insertion (due to <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=44273">Chris Okasaki</a>) and deletion (due to <a href="http://www.cs.kent.ac.uk/people/staff/smk/redblack/rb.html">Stefan Kahrs</a>)  Note that Red-Black trees do not assume a separate value, as in a map, but require only keys (thus implementing an ordered set over the key type):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">tree</span> <span class="o">{</span>
</span><span class='line'>  <span class="cm">/** The color (red or black) of a node in a Red/Black tree */</span>
</span><span class='line'>  <span class="k">sealed</span> <span class="k">trait</span> <span class="nc">Color</span>
</span><span class='line'>  <span class="k">case</span> <span class="k">object</span> <span class="nc">R</span> <span class="k">extends</span> <span class="nc">Color</span>
</span><span class='line'>  <span class="k">case</span> <span class="k">object</span> <span class="nc">B</span> <span class="k">extends</span> <span class="nc">Color</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Defines the data payload of a tree node */</span>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">Data</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="cm">/** The axiomatic unit of data for R/B trees is a key */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">key</span><span class="k">:</span> <span class="kt">K</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Base class of a Red/Black tree node</span>
</span><span class='line'><span class="cm">    * @tparam K The key type</span>
</span><span class='line'><span class="cm">    */</span>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/** The ordering that is applied to key values */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">keyOrdering</span><span class="k">:</span> <span class="kt">Ordering</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/** Instantiate an internal node. */</span>
</span><span class='line'>    <span class="k">protected</span> <span class="k">def</span> <span class="n">iNode</span><span class="o">(</span><span class="n">color</span><span class="k">:</span> <span class="kt">Color</span><span class="o">,</span> <span class="n">d</span><span class="k">:</span> <span class="kt">Data</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span> <span class="n">lsub</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span> <span class="n">rsub</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">])</span><span class="k">:</span> <span class="kt">INode</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// ... declarations for insertion, deletion and key lookup ...</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// ... red-black balancing rules ...</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>   <span class="cm">/** Represents a leaf node in the Red Black tree system */</span>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">LNode</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// ... basis case insertion, deletion, lookup ...</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Represents an internal node (Red or Black) in the Red Black tree system */</span>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">INode</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="cm">/** The Red/Black color of this node */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">color</span><span class="k">:</span> <span class="kt">Color</span>
</span><span class='line'>    <span class="cm">/** Including, but not limited to, the key */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Data</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>    <span class="cm">/** The left sub-tree */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">lsub</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>    <span class="cm">/** The right sub-tree */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">rsub</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// ... implementations for insertion, deletion, lookup ...</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>I will assume most readers are familiar with basic binary tree operations, and the Red-Black rules are described elsewhere (I adapted them from the Scala red-black implementation).  For the purposes of this discussion, the most interesting feature is that this is a <em>pure Scala trait</em>.  All <code>val</code> declarations are abstract.  This trait, by itself, cannot function without a subclass to eventually perform dependency injection.   However, this abstraction allows the trait to be inherited freely &#8211; any programmer can inherit from this trait and get a basic Red-Black balanced tree for (nearly) free, as long as a few basic principles are adhered to for proper dependency injection.</p>

<p>Another detail to call out is the abstraction of the usual <code>key</code> with a <code>Data</code> element.  This element represents any node payload that is moved around as a unit during tree structure manipulations, such as balancing pivots.  In the case of a map-like subclass, <code>Data</code> is extended to include a <code>value</code> field as well as a <code>key</code> field.</p>

<p>The other noteworthy detail is the abstract definition <code>def iNode(color: Color, d: Data[K], lsub: Node[K], rsub: Node[K]): INode[K]</code> - this is the function called to create any new tree node.  In fact, this function, when eventually instantiated, is what performs dependency injection of other tree node fields.</p>

<p><a name="nodemap"></a></p>

<h5>Node Inheritance Example: NodeMap[K,V]</h5>

<p>A relatively simple example of node inheritance is hopefully instructive.  Here is the definition for tree nodes supporting a key/value map:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">tree</span> <span class="o">{</span>
</span><span class='line'>  <span class="cm">/** Trees that back a map-like object have a value as well as a key */</span>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">DataMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Data</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">value</span><span class="k">:</span> <span class="kt">V</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Base class of ordered K/V tree node</span>
</span><span class='line'><span class="cm">    * @tparam K The key type</span>
</span><span class='line'><span class="cm">    * @tparam V The value type</span>
</span><span class='line'><span class="cm">    */</span>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">NodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">LNodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">NodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">LNode</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">INodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">NodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">INode</span><span class="o">[</span><span class="kt">K</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that in this case very little is added to the red/black functionality already provided by <code>Node[K]</code>.  A <code>DataMap[K,V]</code> trait is defined to add a <code>value</code> field in addition to the <code>key</code>, and the internal node <code>INodeMap[K,V]</code> refines the type of its <code>data</code> field to be <code>DataMap[K,V]</code>.  The semantics is little more than &#8220;tree nodes now carry a value in addition to a key.&#8221;</p>

<p>A tree node trait inherits from its own parent class <em>and</em> the corresponding traits for any mixed-in functionality.  So for example <code>INodeMap[K,V]</code> inherits from <code>NodeMap[K,V]</code> but also <code>INode[K]</code>.</p>

<p><a name="orderedmaplike"></a></p>

<h5>Collection Trait Example: OrderedMapLike[K,V,IN,M]</h5>

<p>Continuing with the ordered map example, here is the definition of the collection trait for an ordered map:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">OrderedMapLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">IN</span> <span class="k">&lt;:</span> <span class="kt">INodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span>, <span class="kt">M</span> <span class="k">&lt;:</span> <span class="kt">OrderedMapLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">IN</span>, <span class="kt">M</span><span class="o">]]</span>
</span><span class='line'>    <span class="nc">extends</span> <span class="nc">NodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">OrderedLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">IN</span>, <span class="kt">M</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Obtain a new map with a (key, val) pair inserted */</span>
</span><span class='line'>  <span class="k">def</span> <span class="o">+(</span><span class="n">kv</span><span class="k">:</span> <span class="o">(</span><span class="kt">K</span><span class="o">,</span> <span class="kt">V</span><span class="o">))</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="n">insert</span><span class="o">(</span>
</span><span class='line'>    <span class="k">new</span> <span class="nc">DataMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">key</span> <span class="k">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">_1</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">value</span> <span class="k">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">_2</span>
</span><span class='line'>    <span class="o">}).</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">M</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Get the value stored at a key, or None if key is not present */</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">get</span><span class="o">(</span><span class="n">k</span><span class="k">:</span> <span class="kt">K</span><span class="o">)</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="n">getNode</span><span class="o">(</span><span class="n">k</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">value</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Iterator over (key,val) pairs, in key order */</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">iterator</span> <span class="k">=</span> <span class="n">nodesIterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">n</span> <span class="k">=&gt;</span> <span class="o">((</span><span class="n">n</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">key</span><span class="o">,</span> <span class="n">n</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">value</span><span class="o">)))</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Container of values, in key order */</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">values</span> <span class="k">=</span> <span class="n">valuesIterator</span><span class="o">.</span><span class="n">toIterable</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/** Iterator over values, in key order */</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">valuesIterator</span> <span class="k">=</span> <span class="n">nodesIterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">value</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can see that this trait supplies collection API methods that a Scala programmer will recognize as being standard for any map-like collection.  Note that this trait also inherits other standard methods from <code>OrderedLike[K,IN,M]</code> (common to both sets and maps) and <em>also</em> inherits from <code>NodeMap[K,V]</code>: In other words, a collection is effectively yet another kind of tree node, with additional collection API methods mixed in.   Note also the use of &#8220;self types&#8221; (the type parameter <code>M</code>), which allows the collection to return objects of its own kind.  This is crucial for allowing operations like data insertion to return an object that also supports node insertion, and to maintain consistency of type across operations.  The collection type is properly &#8220;closed&#8221; with respect to its own operations.</p>

<p><a name="orderedmap"></a></p>

<h5>Collection Example: OrderedMap[K,V]</h5>

<p>To conclude the ordered map example, consider the task of defining a concrete instantiation of an ordered map:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">sealed</span> <span class="k">trait</span> <span class="nc">OrderedMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">OrderedMapLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">INodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span>, <span class="kt">OrderedMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">toString</span> <span class="k">=</span>
</span><span class='line'>    <span class="s">&quot;OrderedMap(&quot;</span> <span class="o">+</span>
</span><span class='line'>      <span class="n">nodesIterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">n</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="s">&quot;${n.data.key} -&gt; ${n.data.value}&quot;</span><span class="o">).</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;, &quot;</span><span class="o">)</span> <span class="o">+</span>
</span><span class='line'>    <span class="s">&quot;)&quot;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can see that (aside from a convenience override of <code>toString</code>) the trait <code>OrderedMap[K,V]</code> is nothing more than a vehicle for instantiating a particular concrete <code>OrderedMapLike[K,V,IN,M]</code> subtype, with particular concrete types for internal node (<code>INodeMap[K,V]</code>) and its own self-type.</p>

<p>Things become a little more interesting inside the companion object <code>OrderedMap</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">OrderedMap</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">key</span><span class="o">[</span><span class="kt">K</span><span class="o">](</span><span class="k">implicit</span> <span class="n">ord</span><span class="k">:</span> <span class="kt">Ordering</span><span class="o">[</span><span class="kt">K</span><span class="o">])</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">AnyRef</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">value</span><span class="o">[</span><span class="kt">V</span><span class="o">]</span><span class="k">:</span> <span class="kt">OrderedMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">=</span>
</span><span class='line'>      <span class="k">new</span> <span class="nc">InjectMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="n">ord</span><span class="o">)</span> <span class="k">with</span> <span class="nc">LNodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">OrderedMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that the object returned by the factory method is upcast to <code>OrderedMap[K,V]</code>, but in fact has the more complicated type: <code>InjectMap[K,V] with LNodeMap[K,V] with OrderedMap[K,V]</code>.  There are a couple things going on here.  The trait <code>LNodeMap[K,V]</code> ensures that the new object is in particular a leaf node, which embodies a new empty tree in the Red-Black tree system.</p>

<p>The type <code>InjectMap[K,V]</code> has an even more interesting purpose.  Here is its definition:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">InjectMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="k">val</span> <span class="n">keyOrdering</span><span class="k">:</span> <span class="kt">Ordering</span><span class="o">[</span><span class="kt">K</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">iNode</span><span class="o">(</span><span class="n">clr</span><span class="k">:</span> <span class="kt">Color</span><span class="o">,</span> <span class="n">dat</span><span class="k">:</span> <span class="kt">Data</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span> <span class="n">ls</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span> <span class="n">rs</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">])</span> <span class="k">=</span>
</span><span class='line'>    <span class="k">new</span> <span class="nc">InjectMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="n">keyOrdering</span><span class="o">)</span> <span class="k">with</span> <span class="nc">INodeMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">OrderedMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>      <span class="c1">// INode</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">color</span> <span class="k">=</span> <span class="n">clr</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">lsub</span> <span class="k">=</span> <span class="n">ls</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">rsub</span> <span class="k">=</span> <span class="n">rs</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">DataMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Firstly, note that it is a bona fide <em>class</em>, as opposed to a trait.  This class is where, finally, all things abstract are made real &#8211; &#8220;dependency injection&#8221; in the parlance of Scala idioms.  You can see that it defines the implementation of abstract method <code>iNode</code>, and that it does this by returning yet <em>another</em> <code>InjectMap[K,V]</code> object, mixed with both <code>INodeMap[K,V]</code> and <code>OrderedMap[K,V]</code>, thus maintaining closure with respect to all three slices of functionality: dependency injection, the proper type of internal node, and map collection methods.</p>

<p>The various abstract <code>val</code> fields <code>color</code>, <code>data</code>, <code>lsub</code> and <code>rsub</code> are all given concrete values inside of <code>iNode</code>.  Here is where the value of concrete &#8220;reference&#8221; implementations manifests.  Any fields in the relevant internal-node type must be instantiated here, and the logic of instantiation cannot be inherited while still preserving the ability to mix abstract traits.  Therefore, any programmer wishing to create a new concrete sub-class must replicate the logic for instantiating all inherited in an internal node.</p>

<p>Another example makes the implications more clear.  Here is the definition of injection for a <a href="https://github.com/erikerlandson/silex/blob/blog/rbtraits/src/test/scala/com/redhat/et/silex/maps/mixed.scala">collection that mixes in all three traits</a> for incrementable values, nearest-key queries, and prefix-sum queries:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">class</span> <span class="nc">Inject</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">](</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">keyOrdering</span><span class="k">:</span> <span class="kt">Numeric</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">valueMonoid</span><span class="k">:</span> <span class="kt">Monoid</span><span class="o">[</span><span class="kt">V</span><span class="o">],</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">prefixMonoid</span><span class="k">:</span> <span class="kt">IncrementingMonoid</span><span class="o">[</span><span class="kt">P</span>, <span class="kt">V</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">def</span> <span class="n">iNode</span><span class="o">(</span><span class="n">clr</span><span class="k">:</span> <span class="kt">Color</span><span class="o">,</span> <span class="n">dat</span><span class="k">:</span> <span class="kt">Data</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span> <span class="n">ls</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">],</span> <span class="n">rs</span><span class="k">:</span> <span class="kt">Node</span><span class="o">[</span><span class="kt">K</span><span class="o">])</span> <span class="k">=</span>
</span><span class='line'>      <span class="k">new</span> <span class="nc">Inject</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">](</span><span class="n">keyOrdering</span><span class="o">,</span> <span class="n">valueMonoid</span><span class="o">,</span> <span class="n">prefixMonoid</span><span class="o">)</span>
</span><span class='line'>          <span class="k">with</span> <span class="nc">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="k">with</span> <span class="nc">TDigestMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// INode[K]</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">color</span> <span class="k">=</span> <span class="n">clr</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">lsub</span> <span class="k">=</span> <span class="n">ls</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]]</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">rsub</span> <span class="k">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]]</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">DataMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]</span>
</span><span class='line'>        <span class="c1">// INodePS[K, V, P]</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">prefix</span> <span class="k">=</span> <span class="n">prefixMonoid</span><span class="o">.</span><span class="n">inc</span><span class="o">(</span><span class="n">prefixMonoid</span><span class="o">.</span><span class="n">plus</span><span class="o">(</span><span class="n">lsub</span><span class="o">.</span><span class="n">pfs</span><span class="o">,</span> <span class="n">rsub</span><span class="o">.</span><span class="n">pfs</span><span class="o">),</span> <span class="n">data</span><span class="o">.</span><span class="n">value</span><span class="o">)</span>
</span><span class='line'>        <span class="c1">// INodeNear[K, V]</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">kmin</span> <span class="k">=</span> <span class="n">lsub</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>          <span class="k">case</span> <span class="n">n</span><span class="k">:</span> <span class="kt">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="k">=&gt;</span> <span class="n">n</span><span class="o">.</span><span class="n">kmin</span>
</span><span class='line'>          <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">key</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">kmax</span> <span class="k">=</span> <span class="n">rsub</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>          <span class="k">case</span> <span class="n">n</span><span class="k">:</span> <span class="kt">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="k">=&gt;</span> <span class="n">n</span><span class="o">.</span><span class="n">kmax</span>
</span><span class='line'>          <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">key</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here you can see that all logic for both &#8220;basic&#8221; internal nodes and also for maintaining prefix sums, and key min/max information for nearest-entry queries, must be supplied.  If there is a singularity in this design here is where it is.  The saving grace is that it is localized into a single well defined place, and any logic can be transcribed from a proper reference implementation of whatever traits are being mixed.</p>

<p><a name="mixing"></a></p>

<h5>Finale: Trait Mixing</h5>

<p>I will conclude by showing the code for mixing tree node traits and collection traits, which is elegant.  Here are type definitions for tree nodes and collection traits that inherit from incrementable values, nearest-key queries, and prefix-sum queries, and there is almost no code except the proper inheritances:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">tree</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">com.redhat.et.silex.maps.increment.tree._</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">com.redhat.et.silex.maps.prefixsum.tree._</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">com.redhat.et.silex.maps.nearest.tree._</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">NodePS</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="k">with</span> <span class="nc">NodeInc</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">NodeNearMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">LNodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>
</span><span class='line'>      <span class="k">with</span> <span class="nc">LNodePS</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="k">with</span> <span class="nc">LNodeInc</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">LNodeNearMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">trait</span> <span class="nc">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>
</span><span class='line'>      <span class="k">with</span> <span class="nc">INodePS</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span> <span class="k">with</span> <span class="nc">INodeInc</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">with</span> <span class="nc">INodeNearMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">lsub</span><span class="k">:</span> <span class="kt">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">rsub</span><span class="k">:</span> <span class="kt">NodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// ...</span>
</span><span class='line'>
</span><span class='line'><span class="k">sealed</span> <span class="k">trait</span> <span class="nc">TDigestMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>
</span><span class='line'>  <span class="nc">extends</span> <span class="nc">IncrementMapLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>, <span class="kt">TDigestMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]]</span>
</span><span class='line'>  <span class="k">with</span> <span class="nc">PrefixSumMapLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span>, <span class="kt">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>, <span class="kt">TDigestMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]]</span>
</span><span class='line'>  <span class="k">with</span> <span class="nc">NearestMapLike</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">INodeTD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]</span>, <span class="kt">TDigestMap</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span>, <span class="kt">P</span><span class="o">]]</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">toString</span> <span class="k">=</span> <span class="c1">// ...</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lightweight Non-Negative Numerics for Better Scala Type Signatures]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures/"/>
    <updated>2015-08-18T17:42:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures</id>
    <content type="html"><![CDATA[<p>In this post I want to discuss several advantages of defining lightweight non-negative numeric types in Scala, whose primary benefit is that they allow improved type signatures for Scala functions and methods.  I&#8217;ll first describe the simple class definition, and then demonstrate how it can be used in function signatures and the benefits of doing so.</p>

<p>If the following ideas interest you at all, I highly recommend looking at the <a href="https://github.com/fthomas/refined">&#8216;refined&#8217; project</a> authored by <a href="http://timepit.eu/~frank/">Frank S. Thomas</a>, which generalizes on the ideas below and supports additional static checking functionalities via macros.</p>

<h5>A Non-Negative Integer Type</h5>

<p>As a working example, I&#8217;ll discuss a non-negative integer type <code>NonNegInt</code>.  My proposed definition is sufficiently lightweight to view as a single code block:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">nonneg</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">scala.language.implicitConversions</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">class</span> <span class="nc">NonNegInt</span> <span class="k">private</span> <span class="o">(</span><span class="k">val</span> <span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">AnyVal</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">object</span> <span class="nc">NonNegInt</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">v</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">require</span><span class="o">(</span><span class="n">v</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="o">,</span> <span class="s">&quot;NonNegInt forbids negative integer values&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="k">new</span> <span class="nc">NonNegInt</span><span class="o">(</span><span class="n">v</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">implicit</span> <span class="k">def</span> <span class="n">toNonNegInt</span><span class="o">(</span><span class="n">v</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="nc">NonNegInt</span><span class="o">(</span><span class="n">v</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">implicit</span> <span class="k">def</span> <span class="n">toInt</span><span class="o">(</span><span class="n">nn</span><span class="k">:</span> <span class="kt">NonNegInt</span><span class="o">)</span> <span class="k">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">value</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The notable properties and features of <code>NonNegInt</code> are:</p>

<ul>
<li><code>NonNegInt</code> is a value class around an <code>Int</code>, and so invokes no actual object construction or allocation</li>
<li>Its constructor is private, and so is safe from directly constructing around a negative integer</li>
<li>It supplies factory method <code>NonNegInt(v)</code> to construct a non negative integer value</li>
<li>It supplies implicit conversion from <code>Int</code> values to <code>NonNegInt</code></li>
<li>Both factory method and implicit conversion check for negative values.  There is no way to construct a <code>NonNegInt</code> that contains a negative integer value.</li>
<li>It also supplies implicit conversion from <code>NonNegInt</code> back to <code>Int</code>.  Moving back and forth between <code>Int</code> and <code>NonNegInt</code> is effectively transparent.</li>
</ul>


<p>The above properties work to make <code>NonNegInt</code> very lightweight with respect to size and runtime properties, and semantically safe in the sense that it is impossible to construct one with a negative value inside it.</p>

<h5>Application of <code>NonNegInt</code></h5>

<p>I primarily envision <code>NonNegInt</code> as an easy and informative way to declare function parameters that are only well defined for non-negative values, without the need to write any explicit checking code, and yet allowing the programmer to call the function with normal <code>Int</code> values, due to the implicit conversions:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">example</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">nonneg._</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">element</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">seq</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">],</span> <span class="n">j</span><span class="k">:</span> <span class="kt">NonNegInt</span><span class="o">)</span> <span class="k">=</span> <span class="n">seq</span><span class="o">(</span><span class="n">j</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// call element function with a regular Int index</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">e</span> <span class="k">=</span> <span class="n">element</span><span class="o">(</span><span class="nc">Vector</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">),</span> <span class="mi">1</span><span class="o">)</span> <span class="c1">// e is set to 2</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This short example demonstrates some appealing properties of <code>NonNegInt</code>.  Firstly, the constraint that index <code>j &gt;= 0</code> is enforced via the type definition, and so the programmer does not have to write the usual <code>require(j &gt;= 0, ...)</code> check (or worry about forgetting it).  Secondly, the implicit conversion from <code>Int</code> to <code>NonNegInt</code> means the programmer can just provide a regular integer value for parameter <code>j</code>, instead of having to explicitly say <code>NonNegInt(1)</code>.  Third, the implicit conversion from <code>NonNegInt</code> to <code>Int</code> means that <code>j</code> can easily be used anywhere a regular <code>Int</code> is used.  Last, and very definitely not least, the fact that function <code>element</code> requires a non-negative integer is obvious <strong>right in the function signature</strong>.  There is no need for a programmer to guess whether <code>j</code> can be negative, and no need for the author of <code>element</code> to document that <code>j</code> cannot be negative.  Its type makes that completely clear.</p>

<h5>Conclusions</h5>

<p>In this post I&#8217;ve laid out some advantages of defining lightweight non-negative numeric types, in particular using <code>NonNegInt</code> as a working example.  Clearly, if you want to apply this idea, you&#8217;d want to also define <code>NonNegLong</code>, <code>NonNegDouble</code>, <code>NonNegFloat</code> and for that matter <code>PosInt</code>, <code>PosLong</code>, etc.  Happy computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Reservoir Sampling Gap Distribution]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/08/17/the-reservoir-sampling-gap-distribution/"/>
    <updated>2015-08-17T07:35:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/08/17/the-reservoir-sampling-gap-distribution</id>
    <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2014/09/11/faster-random-samples-with-gap-sampling/">previous post</a>, I showed that random Bernoulli and Poisson sampling could be made much faster by modeling the <em>sampling gap distribution</em> - that is, directly drawing random samples from the distribution of how many elements would be skipped over between actual samples taken.</p>

<p>Another popular sampling algorithm is <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">Reservoir Sampling</a>.  Its sampling logic is a bit more complicated than Bernoulli or Poisson sampling, in the sense that the probability of sampling any given (jth) element <em>changes</em>. For a sampling reservoir of size R, and all j>R, the probability of choosing element (j) is R/j.  You can see that the potential payoff for gap-sampling is big, particularly as data size becomes large; as (j) approaches infinity, the probability R/j goes to zero, and the corresponding gaps between samples grow without bound.</p>

<p>Modeling a sampling gap distribution is a powerful tool for optimizing a sampling algorithm, but it requires that (1) you actually <em>know</em> the sampling distribution, and (2) that you can effectively draw values from that distribution faster than just applying a random process to drawing each data element.</p>

<p>With that goal in mind, I derived the probability mass function (pmf) and cumulative distribution function (cdf) for the sampling gap distribution of reservoir sampling.  In this post I will show the derivations.</p>

<h3>The Sampling Gap Distribution</h3>

<p>In the interest of making it easy to get at the actual answers, here are the pmf and cdf for the Reservoir Sampling Gap Distribution.  For a sampling reservoir of size (R), starting at data element (j), the probability distribution of the sampling gap is:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure6.png" title="Figure 6" alt="Figure 6" /></p>

<h3>Conventions</h3>

<p>In the derivations that follow, I will keep to some conventions:</p>

<ul>
<li>R = the sampling reservoir size.  R > 0.</li>
<li>j = the index of a data element being considered for sampling.  j > R.</li>
<li>k = the size of a gap between samples.  k >= 0.</li>
</ul>


<p>P(k) is the probability that the gap between one sample and the next is of size k.  The support for P(k) is over all k>=0.  I will generally assume that j>R, as the first R samples are always loaded into the reservoir and the actual random sampling logic starts at j=R+1.  The constraint j>R will also be relevant to many binomial coefficient expressions, where it ensures the coefficient is well defined.</p>

<h3>Deriving the Probability Mass Function, P(k)</h3>

<p>Suppose we just chose (randomly) to sample data element (j-1).  Now we are interested in the probability distribution of the next sampling gap.  That is, the probability P(k) that we will <em>not</em> sample the next (k) elements {j,j+1,&#8230;j+k-1}, and sample element (j+k):</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure1.png" title="Figure 1" alt="Figure 1" /></p>

<p>By arranging the product terms in descending order as above, you can see that they can be written as factorial quotients:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure2.png" title="Figure 2" alt="Figure 2" /></p>

<p>Now we apply <a href="#LemmaA">Lemma A</a>.  The 2nd case (a&lt;=b) of the Lemma applies, since (j-1-R)&lt;=j, so we have:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure3.png" title="Figure 3" alt="Figure 3" /></p>

<p>And so we have now derived a compact, closed-form expression for P(k).</p>

<h3>Deriving the Cumulative Distribution Function, F(k)</h3>

<p>Now that we have a derivation for the pmf P(k), we can tackle a derivation for the cdf.  First I will make note of this <a href="https://en.wikipedia.org/wiki/Binomial_coefficient#Series_involving_binomial_coefficients">useful identity</a> that I scraped off of Wikipedia (I substituted (x) => (a) and (k) => (b)):</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/identity1.png" title="identity 1" alt="identity 1" /></p>

<p>The cumulative distribution function for the sampling gap, F(k), is of course just the sum over P(t), for (t) from 0 up to (k):</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure4.png" title="Figure 4" alt="Figure 4" /></p>

<p>This is a closed-form solution, but we can apply a bit more simplification:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure5.png" title="Figure 5" alt="Figure 5" /></p>

<h3>Conclusions</h3>

<p>We have derived closed-form expressions for the pmf and cdf of the Reservoir Sampling gap distribution:</p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/figure6.png" title="Figure 6" alt="Figure 6" /></p>

<p>In order to apply these results to a practical gap-sampling implementation of Reservoir Sampling, we would next need a way to efficiently sample from P(k), to obtain gap sizes to skip over.  How to accomplish this is an open question, but knowing a formula for P(k) and F(k) is a start.</p>

<h3>Acknowledgements</h3>

<p>Many thanks to <a href="http://rnowling.github.io/">RJ Nowling</a> and <a href="http://chapeau.freevariable.com/">Will Benton</a> for proof reading and moral support!  Any remaining errors are my own fault.</p>

<p><a name="LemmaA"></a></p>

<h3>Lemma A, And Its Proof</h3>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/lemmaA.png" title="Lemma A" alt="Lemma A" /></p>

<p><img src="http://erikerlandson.github.com/assets/images/reservoir1/lemmaAproof.png" title="Lemma A Proof" alt="Lemma A Proof" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generalizing Kendall's Tau]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau/"/>
    <updated>2015-08-14T14:35:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau</id>
    <content type="html"><![CDATA[<p>Recently I have been applying <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient">Kendall&#8217;s Tau</a> as an evaluation metric to assess how well a regression model ranks input samples, with respect to a known correct ranking.</p>

<p>The process of implementing the Kendall&#8217;s Tau statistic, with my software engineer&#8217;s hat on, caused me to reflect a bit on how it could be generalized beyond the traditional application of ranking numeric pairs.  In this post I&#8217;ll discuss the generalization of Kendall&#8217;s Tau to non-numeric data, and also generalizing from totally ordered data to partial orderings.</p>

<h5>A Review of Kendall&#8217;s Tau</h5>

<p>I&#8217;ll start with a brief review of Kendall&#8217;s Tau.  For more depth, a good place to start is the Wikipedia article at the link above.</p>

<p>Consider a sequence of (n) observations where each observation is a pair (x,y), where we wish to measure how well a ranking by x-values agrees with a ranking by the y-values.  Informally, Kendall&#8217;s Tau (aka the Kendall Rank Correlation Coefficient) is the difference between number of observation-pairs (pairs of pairs, if you will) whose ordering <em>agrees</em> (&#8220;concordant&#8221; pairs) and the number of such pairs whose ordering <em>disagrees</em> (&#8220;discordant&#8221; pairs).  This difference is divided by the total number of observation pairs.</p>

<p>The commonly-used formulation of Kendall&#8217;s Tau is the &#8220;Tau-B&#8221; statistic, which accounts for observed pairs having tied values in either x or y as being neither concordant nor discordant:</p>

<h6>Figure 1: Kendall&#8217;s Tau-B</h6>

<p><img src="http://erikerlandson.github.com/assets/images/kendalls_tau/figure_1.png" title="Kendall's Tau" alt="Kendall's Tau" /></p>

<p>The formulation above has quadratic complexity, with respect to data size (n).  It is possible to rearrange this computation in a way that can be computed in (n)log(n) time[1]:</p>

<h6>Figure 2: An (n)log(n) formulation of Kendall&#8217;s Tau-B</h6>

<p><img src="http://erikerlandson.github.com/assets/images/kendalls_tau/figure_2.png" title="Kendall's Tau" alt="Kendall's Tau" /></p>

<p>The details of performing this computation can be found at [1] or on the <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Algorithms">Wikipedia entry</a>.  For my purposes, I&#8217;ll note that it requires two (n)log(n) sorts of the data, which becomes relevant below.</p>

<h5>Generalizing to Non-Numeric Values</h5>

<p>Generalizing Kendall&#8217;s Tau to non-numeric values is mostly just making the observation that the definition of &#8220;concordant&#8221; and &#8220;discordant&#8221; pairs is purely based on comparing x-values and y-values (and, in the (n)log(n) formulation, performing sorts on the data).  From the software engineer&#8217;s perspective this means that the computations are well defined on any data type with an ordering relation, which includes numeric types but also chars, strings, sequences of any element supporting an ordering, etc.  Significantly, most programming languages support the concept of defining ordering relations on arbitrary data types, which means that <em><em>Kendall&#8217;s Tau can, in principle, be computed on literally any kind of data structure</em></em>, provided you supply it with a well defined ordering.  Furthermore, an examination of the algorithms shows that values of x and y need not even be of the same type, nor do they require the same ordering.</p>

<h5>Generalizing to Partial Orderings</h5>

<p>When I brought this observation up, my colleague <a href="http://chapeau.freevariable.com/">Will Benton</a> asked the very interesting question of whether it&#8217;s also possible to compute Kendall&#8217;s Tau on objects that have only a <em>partial ordering</em>.  It turns out that you <em><em>can</em></em> define Kendall&#8217;s Tau on partially ordered data, by defining the case of two non-comparable x-values, or y-values, as another kind of tie.</p>

<p>The big caveat with this definition is that the (n)log(n) optimization does not apply.  Firstly, the optimized algorithm relies heavily on (n)log(n) sorting, and there is no unique full sorting of elements that are only partially ordered.  Secondly, the formula&#8217;s definition of the quantities n1, n2 and n3 is founded on the assumption that element equality is transitive; this is why you can count a number of tied values, t, and use t(t-1)/2 as the corresponding number of tied pairs.  But in a partial ordering, this assumption is violated. Consider the case where (a) &lt; (b), but (a) is non-comparable to (c) and (b) is also non-comparable to (c).  By our definition, (a) is tied with (c), and (c) is tied with (b), but transitivity is violated, as (a) &lt; (b).</p>

<p>So how <em>can</em> we compute Tau in this case?  Consider (n1) and (n2), in Figure-1.  These values represent the number of pairs that were tied wrt (x) and (y), respectively.  We can&#8217;t use the shortcut formulas for (n1) and (n2), but we can count them directly, pair by pair, simply by conducting the traditional quadratic iteration over pairs, and incrementing (n1) whenever two x-values are noncomparable, and incrementing (n2) whenever two y-values are non-comparable, just as we increment (nc) and (nd) to count concordant and discordant pairs.  With this modification, we can apply the formula in Figure-1 as-is.</p>

<h5>Conclusions</h5>

<p>I made these observations without any particular application in mind. However, my instincts as a software engineer tell me that making generalizations in this way often paves the way for new ideas, once the generalized concept is made available.  With luck, it will inspire either me or somebody else to apply Kendall&#8217;s Tau in interesting new ways.</p>

<h5>References</h5>

<p>[1] Knight, W. (1966). &#8220;A Computer Method for Calculating Kendall&#8217;s Tau with Ungrouped Data&#8221;. Journal of the American Statistical Association 61 (314): 436–439. doi:10.2307/2282833. JSTOR 2282833.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Parallel K-Medoids Using Scala ParSeq]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq/"/>
    <updated>2015-05-06T16:33:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq</id>
    <content type="html"><![CDATA[<p>Scala supplies a <a href="http://docs.scala-lang.org/overviews/parallel-collections/overview.html">parallel collections library</a> that was designed to make it easy for a programmer to add parallel computing over the elements in a collection.  In this post, I will describe a case study of applying Scala&#8217;s parallel collections to cleanly implement multithreading support for training a K-Medoids clustering model.</p>

<h3>Motivation</h3>

<p><a href="http://en.wikipedia.org/wiki/K-medoids">K-Medoids clustering</a> is a relative of K-Means clustering that does not require an algebra over input data elements.  That is, K-Medoids requires only a distance metric defined on elements in the data space, and can cluster objects which do not have a well-defined concept of addition or division that is necessary for computing the <a href="http://en.wikipedia.org/wiki/Centroid">centroids</a> required by K-Means.  For example, K-Medoids can cluster character strings, which have a notion of <a href="http://en.wikipedia.org/wiki/Edit_distance">distance</a>, but no notion of summation that could be used to compute a geometric centroid.</p>

<p>This additional generality comes at a cost.  The medoid of a collection of elements is the member of the collection that minimizes some function F of the distances from that element to all the other elements in the collection.  For example, F might be the sum of distances from one element to all the elements, or perhaps the maximum distance, etc.  <strong>It is not hard to see that the cost of computing a medoid of (n) elements is quadratic in (n)</strong>: Evaluating F is linear in (n) and F in turn must be evaluated with respect to each element.  Furthermore, unlike centroid-based computations used in K-Means, computing a medoid does not naturally lend itself to common scale-out computing formalisms such as Spark RDDs, due to the full-cross-product nature of the computation.</p>

<p>With this in mind, a more traditional multithreading approach is a good candidate to achieve some practical parallelism on modern multi-core hardware.  I&#8217;ll demonstrate that this is easy to implement in Scala with parallel sequences.</p>

<h3>Non-Parallel Code</h3>

<p>Consider a baseline non-parallel implementation of K-Medoids, as in the following example skeleton code.  (A working version of this code, under review at the time of this post, can be <a href="https://github.com/erikerlandson/silex/blob/parseq_blog/src/main/scala/com/redhat/et/silex/cluster/KMedoids.scala">viewed here</a>)</p>

<figure class='code'><figcaption><span>A skeleton K-Medoids implementation </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">KMedoids</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">k</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">metric</span><span class="k">:</span> <span class="o">(</span><span class="kt">T</span><span class="o">,</span> <span class="kt">T</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nc">Double</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// Train a K-Medoids cluster on some input data</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">train</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">current</span> <span class="k">=</span> <span class="c1">// randomly select k data elements as initial cluster</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">var</span> <span class="n">model_converged</span> <span class="k">=</span> <span class="kc">false</span>
</span><span class='line'>    <span class="k">while</span> <span class="o">(!</span><span class="n">model_converged</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="c1">// assign each element to its closest medoid</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">clusters</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">medoidIdx</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">current</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>      <span class="c1">// recompute the medoid from the latest cluster elements</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">benchmark</span><span class="o">(</span><span class="s">&quot;medoids&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">clusters</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">medoid</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>      <span class="n">model_converged</span> <span class="k">=</span> <span class="c1">// test for model convergence</span>
</span><span class='line'>
</span><span class='line'>      <span class="n">current</span> <span class="k">=</span> <span class="n">next</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// Return the medoid of some collection of elements</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoid</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">benchmark</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;medoid: n= ${data.length}&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">data</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="n">medoidCost</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">data</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// The sum of an element&#39;s distance to all the elements in its cluster</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoidCost</span><span class="o">(</span><span class="n">e</span><span class="k">:</span> <span class="kt">T</span><span class="o">,</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">metric</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="k">_</span><span class="o">)).</span><span class="n">sum</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// Index of the closest medoid to an element</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoidIdx</span><span class="o">(</span><span class="n">e</span><span class="k">:</span> <span class="kt">T</span><span class="o">,</span> <span class="n">mv</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="n">mv</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">metric</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="k">_</span><span class="o">)).</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">min</span><span class="o">.</span><span class="n">_2</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// Output a benchmark timing of some expression</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">benchmark</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">label</span><span class="k">:</span> <span class="kt">String</span><span class="o">)(</span><span class="n">blk</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="n">T</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">t0</span> <span class="k">=</span> <span class="nc">System</span><span class="o">.</span><span class="n">nanoTime</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">t</span> <span class="k">=</span> <span class="n">blk</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">sec</span> <span class="k">=</span> <span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="n">nanoTime</span> <span class="o">-</span> <span class="n">t0</span><span class="o">)</span> <span class="o">/</span> <span class="mi">1</span><span class="n">e9</span>
</span><span class='line'>    <span class="n">println</span><span class="o">(</span><span class="n">f</span><span class="s">&quot;Run time for $label = $sec%.1f&quot;</span><span class="o">);</span> <span class="nc">System</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">flush</span>
</span><span class='line'>    <span class="n">t</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>If we run the code above (de-skeletonized), then we might see something like this output from our benchmarking, where I clustered a dataset of 40,000 randomly-generated (x,y,z) points by Gaussian sampling around 5 chosen centers.  (This data is numeric, but I provide only a distance metric on the points.  K-Medoids has no knowledge of the data except that it can run the given metric function on it):</p>

<figure class='code'><figcaption><span>One iteration of a clustering run (k = 5) </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Run time for medoid: n= 8299 = 7.7
</span><span class='line'>Run time for medoid: n= 3428 = 1.2
</span><span class='line'>Run time for medoid: n= 12581 = 17.0
</span><span class='line'>Run time for medoid: n= 5731 = 3.3
</span><span class='line'>Run time for medoid: n= 9961 = 10.2
</span><span class='line'>Run time for medoids = 39.8</span></code></pre></td></tr></table></div></figure>


<p>Observe that cluster sizes are generally not the same, and we can see the time per cluster varying quadratically with respect to cluster size.</p>

<h3>A First Take On Parallel K-Medoids</h3>

<p>Studying our non-parallel code above, we can see that the computation of each new medoid is independent, which makes it a likely place to inject some parallelism. A Scala sequence can be transformed into a corresponding parallel sequence using the <code>par</code> method, and so parallelizing our code is literally this simple:</p>

<figure class='code'><figcaption><span>Parallelizing a collection with .par </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>      <span class="c1">// recompute the medoid from the latest cluster elements</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">benchmark</span><span class="o">(</span><span class="s">&quot;medoids&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">clusters</span><span class="o">.</span><span class="n">par</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">medoid</span><span class="o">).</span><span class="n">seq</span>
</span><span class='line'>      <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>In this block, I also apply <code>.seq</code> at the end, which is not always necessary but can avoid type mismatches between <code>Seq[T]</code> and <code>ParSeq[T]</code> under some circumstances.</p>

<p>In my case I also wish to exercise some control over the threading used by the parallelism, and so I explicitly assign a <code>ForkJoinPool</code> thread pool to the sequence:</p>

<figure class='code'><figcaption><span>Set the threading used by a Scala ParSeq </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>      <span class="c1">// establish a thread pool for use by K-Medoids</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">threadPool</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ForkJoinPool</span><span class="o">(</span><span class="n">numThreads</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>      <span class="c1">// ...</span>
</span><span class='line'>
</span><span class='line'>      <span class="c1">// recompute the medoid from the latest cluster elements</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">benchmark</span><span class="o">(</span><span class="s">&quot;medoids&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">pseq</span> <span class="k">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">par</span>
</span><span class='line'>        <span class="n">pseq</span><span class="o">.</span><span class="n">tasksupport</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ForkJoinTaskSupport</span><span class="o">(</span><span class="n">threadPool</span><span class="o">)</span>
</span><span class='line'>        <span class="n">pseq</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">medoid</span><span class="o">).</span><span class="n">seq</span>
</span><span class='line'>      <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Minor grievance: it would be nice if Scala supported some &#8216;in-line&#8217; methods, like <code>seq.par(n)...</code> and <code>seq.par(threadPool)...</code>, instead of requiring the programmer to break the flow of the code to invoke <code>tasksupport =</code>, which returns <code>Unit</code>.</p>

<p>Now that we&#8217;ve parallelized our K-Medoids training, we should see how well it responds to additional threads.  I ran the above parallelized version using <code>{1, 2, 4, 8, 16, 32}</code> threads, on a machine with 40 cores, so that my benchmarking would not be impacted by attempting to run more threads than there are cores to support them.  I also ran two versions of test data.  The first I generated with clusters of equal size (5 clusters of ~8000 elements), and the second with one cluster being twice as large (1 cluster of ~13300 and 4 clusters of ~6700).  Following is a plot of throughput (iterations / second) versus threads:</p>

<p><img class="left" src="http://erikerlandson.github.com/assets/images/parseq/by_cluster_1.png" title="Throughput As A Function Of Threads" ></p>

<p>In the best of all possible worlds, our throughput would increase linearly with the number of threads; double the threads, double our iterations per second.  Instead, our throughput starts to increase nicely as we add threads, but hits a hard ceiling at 8 threads.  It is not hard to see why: our parallelism is limited by the number of elements in our collection of clusters.  In our case that is k = 5, and so we reach our ceiling at 8 threads, the first thread number >= 5.  Furthermore, we see that when the size of clusters is unequal, the throughput suffers even more.  The time required to complete the clustering is dominated by the most expensive element.  In our case, the cluster that is twice the size of other clusters:</p>

<figure class='code'><figcaption><span>Run time is dominated by largest cluster </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Run time for medoid: n= 6695 = 5.1
</span><span class='line'>Run time for medoid: n= 6686 = 5.2
</span><span class='line'>Run time for medoid: n= 6776 = 5.3
</span><span class='line'>Run time for medoid: n= 6682 = 5.4
</span><span class='line'>Run time for medoid: n= 13161 = 19.9
</span><span class='line'>Run time for medoids = 19.9</span></code></pre></td></tr></table></div></figure>


<h3>Take 2: Improving The Use Of Threads</h3>

<p>Fortunately it is not hard to improve on this situation.  If parallelizing by cluster is too coarse, we can try pushing our parallelism down one level of granularity.  In our case, that means parallelizing the outer loop of our medoid function, and it is just as easy as before:</p>

<figure class='code'><figcaption><span>Parallelize the outer loop of medoid computation </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="c1">// Return the medoid of some collection of elements</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoid</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">benchmark</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;medoid: n= ${data.length}&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">pseq</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">par</span>
</span><span class='line'>      <span class="n">pseq</span><span class="o">.</span><span class="n">tasksupport</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ForkJoinTaskSupport</span><span class="o">(</span><span class="n">threadPool</span><span class="o">)</span>
</span><span class='line'>      <span class="n">pseq</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="n">medoidCost</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">data</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that I retained the previous parallelism at the cluster level, otherwise the algorithm would execute parallel medoids, but one cluster at a time.  Also observe that we are applying the same thread pool we supplied to the ParSeq at the cluster level.  Scala&#8217;s parallel logic can utilize the same thread pool at multiple granularities without blocking.  This makes it very clean to control the total number of threads used by some computation, by simply re-using the same threadpool across all points of parallelism.</p>

<p>Now, when we re-run our experiment, we see that our throughput continues to increase as we add threads.  The following plot illustrates the throughput increasing in comparison to the previous ceiling, and also that throughput is less sensitive to the cluster size, as threads can be allocated flexibly across clusters as they are available:</p>

<p><img class="left" src="http://erikerlandson.github.com/assets/images/parseq/all_1.png" title="Thread utilization improves at finer granularity" ></p>

<p>I hope this short case study has demonstrated how easy it is to add multithreading to computations with Scala parallel sequences, and some considerations for making the best use of available threads.  Happy Parallel Programming!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hygienic Closures for Scala Function Serialization]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization/"/>
    <updated>2015-03-31T06:06:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization</id>
    <content type="html"><![CDATA[<p>In most use cases of Scala closures, what you see is what you get, but there are exceptions where looks can be deceiving and this can have a big impact on closure serialization.  Closure serialization is of more than academic interest.  Tools like Apache Spark cannot operate without serializing functions over the network.  In this post I&#8217;ll describe some scenarios where closures include more than what is evident in the code, and then a technique for preventing unwanted inclusions.</p>

<p>To establish a bit of context, consider this simple example that obtains a function and serializes it to disk, and which <em>does</em> behave as expected:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  object foo {
    val v = 42
    // The returned function includes 'v' in its closure
    def f() = (x: Int) =&gt; v * x
  }

  // The function 'f' will serialize as expected
  val f = foo.f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>When this app is compiled and run, it will serialize <code>f</code> to &#8220;/tmp/demo.f1&#8221;, which of course includes the value of <code>v</code> as part of the closure for <code>f</code>.</p>

<pre><code>$ scalac -d /tmp closures.scala
$ scala -cp /tmp Demo
$ ls /tmp/demo*
/tmp/demo.f
</code></pre>

<p>Now, imagine you wanted to make a straightforward change, where <code>object foo</code> becomes <code>class foo</code>:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  // foo is a class instead of an object
  class foo() {
    val v = 42
    // The returned function includes 'v' in its closure, but also a secret surprise
    def f() = (x: Int) =&gt; v * x
  }

  // This will throw an exception!
  val f = new foo().f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>It would be reasonable to expect that this minor variation behaves exactly as the previous one, but instead it throws an exception!</p>

<pre><code>$ scalac -d /tmp closures.scala
$ scala -cp /tmp Demo
java.io.NotSerializableException: Demo$foo
</code></pre>

<p>If we look at the exception message, we see that it&#8217;s complaining about not knowing how to serialize objects of class <code>foo</code>.  But we weren&#8217;t including any values of <code>foo</code> in the closure for <code>f</code>, only a particular member &#8216;v&#8217;!  What gives?  Scala is not very helpful with diagnosing this problem, but when a class member value shows up in a closure that is defined <em>inside</em> the class body, the <em>entire instance</em>, including any and all other member values, is included in the closure.  Presumably this is because a class may have any number of instances, and the compiler is including the entire instance in the closure to properly resolve the correct member value.</p>

<p>One straightforward way to fix this is to simply make class <code>foo</code> serializable:</p>

<pre><code>class foo() extends Serializable {
  // ...
}
</code></pre>

<p>If you make this change to the above code, the example with <code>class foo</code> now works correctly, but it is working by serializing the entire <code>foo</code> instance, not just the value of <code>v</code>.</p>

<p>In many cases, this is not a problem and will work fine.  Serializing a few additional members may be inexpensive.  In other cases, however, it can be an impractical or impossible option.  For example, <code>foo</code> might include other very large members, which will be expensive or outright impossible to serialize:</p>

<pre><code>class foo() extends Serializable {
  val v = 42    // easy to serialize
  val w = 4.5   // easy to serialize
  val data = (1 to 1000000000).toList  // serialization landmine hiding in your closure

  // The returned function includes all of 'foo' instance in its closure
  def f() = (x: Int) =&gt; v * x
}
</code></pre>

<p>A variation on the above problem is class members that are small or moderate in size, but serialized many times.  In this case, the serialization cost can become intractable via repetition of unwanted inclusions.</p>

<p>Another potential problem is class members that are not serializable, and perhaps not under your control:</p>

<pre><code>class foo() extends Serializable {
  import some.class.NotSerializable

  val v = 42                      // easy to serialize
  val x = new NotSerializable     // I'll hide in your closure and fail to serialize

  // The returned function includes all of 'foo' instance in its closure
  def f() = (x: Int) =&gt; v * x
}
</code></pre>

<p>There is a relatively painless way to decouple values from their parent instance, so that only desired values are included in a closure.  Passing desired values as parameters to a shim function whose job is to assemble the closure will prevent the parent instance from being pulled into the closure.  In the following example, a shim function named <code>closureFunction</code> is defined for this purpose:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  // apply a generator to create a function with safe decoupled closures
  def closureFunction[E,D,R](enclosed: E)(gen: E =&gt; (D =&gt; R)) = gen(enclosed)

  class NotSerializable {}

  class foo() {
    val v1 = 42
    val v2 = 73
    val n = new NotSerializable

    // use shim function to enclose *only* the values of 'v1' and 'v2'
    def f() = closureFunction((v1, v2)) { enclosed =&gt;
      val (v1, v2) = enclosed
      (x: Int) =&gt; (v1 + v2) * x   // Desired function, with 'v1' and 'v2' enclosed
    }
  }

  // This will work!
  val f = new foo().f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>Being aware of the scenarios where parent instances are pulled into closures, and how to keep your closures clean, can save some frustration and wasted time.  Happy programming!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monadic 'break' and 'continue' for Scala Sequence Comprehensions]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions/"/>
    <updated>2015-01-24T11:54:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions</id>
    <content type="html"><![CDATA[<p>Author&#8217;s note: I&#8217;ve since received some excellent feedback from the Scala community, which I included in some <a href="#notes">end notes</a>.</p>

<p>Author&#8217;s note the 2nd: I later realized I could apply an implicit conversion and mediator class to preserve the traditional ordering: the code has been updated with that approach.</p>

<p>Author&#8217;s note the 3rd: This concept has been submitted to the Scala project as JIRA <a href="https://issues.scala-lang.org/browse/SI-9120">SI-9120</a> (PR <a href="https://github.com/scala/scala/pull/4275">#4275</a>)</p>

<p>Scala <a href="http://docs.scala-lang.org/tutorials/tour/sequence-comprehensions.html">sequence comprehensions</a> are an excellent functional programming idiom for looping in Scala.  However, sequence comprehensions encompass much more than just looping &#8211; they represent a powerful syntax for manipulating <em>all</em> monadic structures<a href="#ref1">[1]</a>.</p>

<p>The <code>break</code> and <code>continue</code> looping constructs are a popular framework for cleanly representing multiple loop halting and continuation conditions at differing stages in the execution flow.  Although there is no native support for <code>break</code> or <code>continue</code> in Scala control constructs, it is possible to implement them in a clean and idiomatic way for sequence comprehensions.</p>

<p>In this post I will describe a lightweight and easy-to-use implementation of <code>break</code> and <code>continue</code> for use in Scala sequence comprehensions (aka <code>for</code> statements).  The entire implementation is as follows:</p>

<pre><code>object BreakableGenerators {
  import scala.language.implicitConversions

  type Generator[+A] = Iterator[A]
  type BreakableGenerator[+A] = BreakableIterator[A]

  // Generates a new breakable generator from any traversable object.
  def breakable[A](t1: TraversableOnce[A]): Generator[BreakableGenerator[A]] =
    List(new BreakableIterator(t1.toIterator)).iterator

  // Mediates boolean expression with 'break' and 'continue' invocations
  case class BreakableGuardCondition(cond: Boolean) {
    // Break the looping over one or more breakable generators, if 'cond' 
    // evaluates to true.
    def break(b: BreakableGenerator[_], bRest: BreakableGenerator[_]*): Boolean = {
      if (cond) {
        b.break
        for (x &lt;- bRest) { x.break }
      }
      !cond
    }

    // Continue to next iteration of enclosing generator if 'cond' 
    // evaluates to true.
    def continue: Boolean = !cond
  }

  // implicit conversion of boolean values to breakable guard condition mediary
  implicit def toBreakableGuardCondition(cond: Boolean) =
    BreakableGuardCondition(cond)

  // An iterator that can be halted via its 'break' method.  Not invoked directly
  class BreakableIterator[+A](itr: Iterator[A]) extends Iterator[A] {
    private var broken = false
    private[BreakableGenerators] def break { broken = true }

    def hasNext = !broken &amp;&amp; itr.hasNext
    def next = itr.next
  }
}
</code></pre>

<p>The approach is based on a simple subclass of <code>Iterator</code> &#8211; <code>BreakableIterator</code> &#8211; that can be halted by &#8216;breaking&#8217; it.  The function <code>breakable(&lt;traversable-object&gt;)</code> returns an Iterator over a single <code>BreakableIterator</code> object.  Iterators are monad-like structures in that they implement <code>map</code> and <code>flatMap</code>, and so its output can be used with <code>&lt;-</code> at the start of a <code>for</code> construct in the usual way.  Note that this means the result of the <code>for</code> statement will also be an Iterator.</p>

<p>Whenever the boolean expression for an <code>if</code> guard is followed by either <code>break</code> or <code>continue</code>, it is implicitly converted to a &#8220;breakable guard condition&#8221; that supports those methods.  The function <code>break</code> accepts one or more instances of <code>BreakableIterator</code>.  If it evaluates to <code>true</code>, the loops embodied by the given iterators are immediately halted via the associated <code>if</code> guard, and the iterators are halted via their <code>break</code> method.  The <code>continue</code> function is mostly syntactic sugar for a standard <code>if</code> guard, simply with the condition inverted.</p>

<p>Here is a simple example of <code>break</code> and <code>continue</code> in use:</p>

<pre><code>object Main {
  import BreakableGenerators._

  def main(args: Array[String]) {

    val r = for (
      // generate a breakable sequence from some sequential input
      loop &lt;- breakable(1 to 1000);
      // iterate over the breakable sequence
      j &lt;- loop;
      // print out at each iteration
      _ = { println(s"iteration j= $j") };
      // continue to next iteration when 'j' is even
      if { j % 2 == 0 } continue;
      // break out of the loop when 'j' exceeds 5
      if { j &gt; 5 } break(loop)
    ) yield {
      j
    }
    println(s"result= ${r.toList}")
  }
}
</code></pre>

<p>We can see from the resulting output that <code>break</code> and <code>continue</code> function in the usual way.  The <code>continue</code> clause ignores all subsequent code when <code>j</code> is even.  The <code>break</code> clause halts the loop when it sees its first value > 5, which is 7.  Only odd values &lt;= 5 are output from the <code>yield</code> statement:</p>

<pre><code>$ scalac -d /home/eje/class monadic_break.scala
$ scala -classpath /home/eje/class Main
iteration j= 1
iteration j= 2
iteration j= 3
iteration j= 4
iteration j= 5
iteration j= 6
iteration j= 7
result= List(1, 3, 5)
</code></pre>

<p>Breakable iterators can be nested in the way one would expect.  The following example shows an inner breakable loop nested inside an outer one:</p>

<pre><code>object Main {
  import BreakableGenerators._

  def main(args: Array[String]) {
    val r = for (
      outer &lt;- breakable(1 to 7);
      j &lt;- outer;
      _ = { println(s"outer  j= $j") };
      if { j % 2 == 0 } continue;
      inner &lt;- breakable(List("a", "b", "c", "d", "e"));
      k &lt;- inner;
      _ = { println(s"    inner  j= $j  k= $k") };
      if { k == "d" } break(inner);
      if { j == 5  &amp;&amp;  k == "c" } break(inner, outer)
    ) yield {
      (j, k)
    }
    println(s"result= ${r.toList}")
  }
}
</code></pre>

<p>The output demonstrates that the inner loop breaks whenever <code>k=="d"</code>, and so <code>"e"</code> is never present in the <code>yield</code> result.  When <code>j==5</code> and <code>k=="c"</code>, both the inner and outer loops are broken, and so we see that there is no <code>(5,"c")</code> pair in the result, nor does the outer loop ever iterate over 6 or 7:</p>

<pre><code>$ scalac -d /home/eje/class monadic_break.scala
$ scala -classpath /home/eje/class Main
outer  j= 1
    inner  j= 1  k= a
    inner  j= 1  k= b
    inner  j= 1  k= c
    inner  j= 1  k= d
outer  j= 2
outer  j= 3
    inner  j= 3  k= a
    inner  j= 3  k= b
    inner  j= 3  k= c
    inner  j= 3  k= d
outer  j= 4
outer  j= 5
    inner  j= 5  k= a
    inner  j= 5  k= b
    inner  j= 5  k= c
result= List((1,a), (1,b), (1,c), (3,a), (3,b), (3,c), (5,a), (5,b))
</code></pre>

<p>Using <code>break</code> and <code>continue</code> with <code>BreakableIterator</code> for sequence comprehensions is that easy.  Enjoy!</p>

<p><a name="notesname" id="notes"></a></p>

<h5>Notes</h5>

<p>The helpful community on freenode #scala made some excellent observations:</p>

<p>1: Iterators in Scala are not strictly monadic &#8211; it would be more accurate to say they&#8217;re &#8220;things with a flatMap and map method, also they can use filter or withFilter sometimes.&#8221;  However, I personally still prefer to think of them as &#8220;monadic in spirit if not law.&#8221;</p>

<p>2: The <code>break</code> function, as described in this post, is not truly functional in the sense of referential transparency, as the invocation <code>if break(loop) { condition }</code> involves a side-effect on the variable <code>loop</code>.  I would say that it does maintain &#8220;scoped functionality.&#8221;  That is, the break in non-referential transparency is scoped by the variables in question.  The <code>for</code> statement containing them is referentially transparent with respect to its inputs (provided no other code is breaking referential transparency, of course).</p>

<h5>References</h5>

<p><a name="ref1name" id="ref1">[1] </a><em><a href="http://www.manning.com/bjarnason/">Functional Programming in Scala</a></em>, Paul Chiusano and Runar Bjarnason, (section 6.6)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
    <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
    <updated>2014-09-11T07:57:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
    <content type="html"><![CDATA[<blockquote><p>Update (April 4, 2016): my colleague <a href="http://rnowling.github.io/">RJ Nowling</a> ran across a <a href="http://www.ittc.ku.edu/~jsv/Papers/Vit87.RandomSampling.pdf">paper by J.S. Vitter</a> that shows Vitter developed the trick of accelerating sampling with a sampling-gap distribution in 1987 &#8211; I re-invented Vitter&#8217;s wheel 30 years after the fact!  I&#8217;m surprised it never caught on, as it is not much harder to implement than the naive version.</p></blockquote>

<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode:</p>

<pre><code>sample(data: array, p: real) {
    n = length(data)
    m = floor(p * n)
    for j = 0 to m-1 {
        k = random(j, n-1)
        swap(data[j], data[k])
    }
    emit the first m elements of 'data' to output
}
</code></pre>

<p>We can see that this sampling algorithm is indeed O(np).  However, it makes some nontrivial assumptions about its input data:</p>

<ul>
<li>It is random access</li>
<li>It is writable</li>
<li>Its size is known</li>
<li>It can be destructively modified</li>
</ul>


<p>These assumptions can be violated in several ways.  The input data might not support random access, for example it might be a list, or stream, or an iterator over the same.  We might not know its size a priori.  It might be read-only.  It might be up-cast to some superclass where knowledge about these assumed properties is no longer available.</p>

<p>In cases such as this, there is another common sampling algorithm:</p>

<pre><code>sample(data: sequence, p: real) {
    while not end(data) {
        v = next(data)
        if random(0.0, 1.0) &lt; p then emit v to output
    }
}
</code></pre>

<p>The above algorithm enjoys all the advantage in flexibility.  It requires only linear access, does not require writable input, and makes no assumptions about input size.  However it comes at a price: this algorithm is no longer O(np), it is O(n).  Each element must be traversed directly, and worse yet the random number generagor (RNG) must be invoked on each element.  O(n) invocation of the RNG is a substantial cost &#8211; random number generation is typically very expensive compared to the cost of iterating to the next element in a sequence.</p>

<p>But&#8230; does linear sampling truly require us to invoke our RNG on every element?   Consider the pattern of data access, divorced from code.   It looks like a sequence of choices: for each element we either (skip) or (sample):</p>

<pre><code>(skip) (skip) (sample) (skip) (sample) (skip) (sample) (sample) (skip) (skip) (sample) ...
</code></pre>

<p>The number of consecutive (skip) events between each (sample) &#8211; the <em>sampling gap</em> &#8211; can itself be modeled as a random variable.  Each (skip)/(sample) choice is an independent Bernoulli trial, where the probability of (skip) is (1-p).   The PMF of the sampling gap for gap of {0, 1, 2, &#8230;} is therefore a geometric distribution: P(k) = p(1-p)<sup>k</sup></p>

<p>This suggests an alternative algorithm for sampling, where we only need to randomly choose sample gaps instead of randomly choosing whether we sample each individual element:</p>

<pre><code>// choose a random sampling gap 'k' from P(k) = p(1-p)^k
// caution: this explodes for p = 0 or p = 1
random_gap(p: real) {
    u = max(random(0.0, 1.0), epsilon)
    return floor(log(u) / log(1-p))
}

sample(data: sequence, p: real) {
    advance(data, random_gap(p))
    while not end(data) {
        emit next(data) to output
        advance(data, random_gap(p))
    }
}
</code></pre>

<p>The above algorithm calls the RNG only once per actual collected sample, and so the cost of RNG calls is O(np).  Note that the algorithm is still O(n), but the cost of the RNG tends to dominate the cost of sequence traversal, and so the resulting efficiency improvement is substantial.  I measured the following performance improvements with gap sampling, compared to traditional linear sequence sampling, on a <a href="https://gist.github.com/erikerlandson/05db1f15c8d623448ff6">Scala prototype testing rig</a>:</p>

<p><head><style>
table, th, td {
border: 1px solid black;
border-collapse: collapse;
}
th, td {
padding: 10px;
}
th {
text-align: center;
}
</style></head></p>

<table>
<tr> <th>Type</th> <th>p</th> <th>linear</th> <th>gap</th> </tr>
<tr> <td>Array</td> <td>0.001</td> <td>2833</td> <td>29</td> </tr>
<tr> <td>Array</td> <td>0.01</td> <td>2825</td> <td>76</td> </tr>
<tr> <td>Array</td> <td>0.1</td> <td>2985</td> <td>787</td> </tr>
<tr> <td>Array</td> <td>0.5</td> <td>3526</td> <td>3478</td> </tr>
<tr> <td>Array</td> <td>0.9</td> <td>3023</td> <td>6081</td> </tr>
<tr> <td>List</td> <td>0.001</td> <td>2213</td> <td>230</td> </tr>
<tr> <td>List</td> <td>0.01</td> <td>2220</td> <td>265</td> </tr>
<tr> <td>List</td> <td>0.1</td> <td>2337</td> <td>796</td> </tr>
<tr> <td>List</td> <td>0.5</td> <td>2794</td> <td>3151</td> </tr>
<tr> <td>List</td> <td>0.9</td> <td>2513</td> <td>4849</td> </tr>
</table>




<br>


<p>In the results above, we see that the gap sampling times are essentially linear in (p), as expected.  In the case of the linear-access List type, there is a higher baseline time (230 vs 29) due to the constant cost of actual data traversal.  Efficiency improvements are substantial at small sampling probabilities.</p>

<p>We can also see that the cost of gap sampling begins to meet and then exceed the cost of traditinal linear sampling, in the vicinnity (p) = 0.5.  This is due to the fact that the gap sampling logic is about twice the cost (in my test environment) of simply calling the RNG once.  For example, the gap sampling invokes a call to the numeric logarithm code that isn&#8217;t required in traditional sampling.  And so at (p) = 0.5 the time spent doing the gap sampling approximates the time spent invoking the RNG once per sample, and at higher values of (p) the cost is greater.</p>

<p>This suggests that one should in fact fall back to traditional linear sampling when the sampling probability (p) >= some threshold.  That threshold appears to be about 0.5 or 0.6 in my testing rig, but is likely to depend on underlying numeric libraries, the particular RNG being used, etc, and so I would expect it to benefit from customized tuning on a per-environment basis.  With this in mind, a sample algorithm as deployed would look like this:</p>

<pre><code>// threshold is a tuning parameter
threshold = 0.5

sample(data: sequence, p: real) {
    if (p &lt; threshold) {
        gap_sample(data, p)
    } else {
        traditional_linear_sample(data, p)
    }
}
</code></pre>

<p>The gap-sampling algorithm described above is for sampling <em>without</em> replacement.   However, the same approach can be modified to generate sampling <em>with</em> replacement.</p>

<p>When sampling with replacement, it is useful to consider the <em>replication factor</em> of each element (where a replication factor of zero means the element wasn&#8217;t sampled).  Pretend for the moment that the actual data size (n) is known.  The sample size (m) = (n)(p).  The probability that each element gets sampled, per trial, is 1/n, with (m) independent trials, and so the replication factor (r) for each element obeys a binomial distribution: Binomial(m, 1/n).  If we substitute (n)(p) for (m), we have Binomial(np, 1/n).  As the (n) grows, the Binomial is <a href="http://en.wikipedia.org/wiki/Binomial_distribution#Poisson_approximation">well approximated by a Poisson distribution</a> Poisson(L), where (L) = (np)(1/n) = (p).  And so for our purposes we may sample from Poisson(p), where P(r) = (p<sup>r</sup> / r!)e<sup>(-p),</sup> for our sampling replication factors.  Note that we have now discarded any dependence on sample size (n), as we desire.</p>

<p>In our gap-sampling context, the sampling gaps are now elements whose replication factor is zero, which occurs with probability P(0) = e<sup>(-p).</sup>  And so our sampling gaps are now drawn from geometric distribution P(k) = (1-q)(q)<sup>k,</sup> where q = e<sup>(-p).</sup>   When we <em>do</em> sample an element, its replication factor is drawn from Poisson(p), however <em>conditioned such that the value is >= 1.</em>  It is straightforward to adapt a <a href="http://en.wikipedia.org/wiki/Poisson_distribution#Generating_Poisson-distributed_random_variables">standard Poisson generator</a>, as shown below.</p>

<p>Given the above, gap sampling with replacement in pseudocode looks like:</p>

<pre><code>// sample 'k' from Poisson(p), conditioned to k &gt;= 1
poisson_ge1(p: real) {
    q = e^(-p)
    // simulate a poisson trial such that k &gt;= 1
    t = q + (1-q)*random(0.0, 1.0)
    k = 1

    // continue standard poisson generation trials
    t = t * random(0.0, 1.0)
    while (t &gt; q) {
        k = k + 1
        t = t * random(0.0, 1.0)
    }
    return k
}

// choose a random sampling gap 'k' from P(k) = p(1-p)^k
// caution: this explodes for p = 0 or p = 1
random_gap(p: real) {
    u = max(random(0.0, 1.0), epsilon)
    return floor(log(u) / -p)
}

sample(data: sequence, p: real) {
    advance(data, random_gap(p))
    while not end(data) {
        rf = poisson_ge1(p)
        v = next(data)
        emit (rf) copies of (v) to output
        advance(data, random_gap(p))
    }
}
</code></pre>

<p>The efficiency improvements I have measured for gap sampling with replacement are shown here:</p>

<table>
<tr> <th>Type</th> <th>p</th> <th>linear</th> <th>gap</th> </tr>
<tr> <td>Array</td> <td>0.001</td> <td>2604</td> <td>45</td> </tr>
<tr> <td>Array</td> <td>0.01</td> <td>3442</td> <td>117</td> </tr>
<tr> <td>Array</td> <td>0.1</td> <td>3653</td> <td>1044</td> </tr>
<tr> <td>Array</td> <td>0.5</td> <td>5643</td> <td>5073</td> </tr>
<tr> <td>Array</td> <td>0.9</td> <td>7668</td> <td>8388</td> </tr>
<tr> <td>List</td> <td>0.001</td> <td>2431</td> <td>233</td> </tr>
<tr> <td>List</td> <td>0.01</td> <td>2450</td> <td>299</td> </tr>
<tr> <td>List</td> <td>0.1</td> <td>2984</td> <td>1330</td> </tr>
<tr> <td>List</td> <td>0.5</td> <td>5331</td> <td>4752</td> </tr>
<tr> <td>List</td> <td>0.9</td> <td>6744</td> <td>7811</td> </tr>
</table>




<br>


<p>As with the results for sampling without replacement, we see that gap sampling cost is linear with (p), which yields large cost savings at small sampling, but begins to exceed traditional linear sampling at higher sampling probabilities.</p>
]]></content>
  </entry>
  
</feed>
