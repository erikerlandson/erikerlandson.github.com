
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Measuring Decision Tree Split Quality with Test Statistic P-Values - tool monkey</title>
  <meta name="author" content="Erik Erlandson">

  
  <meta name="description" content="When training a decision tree learning model (or an ensemble of such models) it is often nice to have a policy for deciding when a tree node can no &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="tool monkey" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

  <!-- enables inclusion of MathJax LaTeX: http://greglus.com/blog/2011/11/29/integrate-MathJax-LaTeX-and-MathML-Markup-in-Octopress/ -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">tool monkey</a></h1>
  
    <h2>adventures of an unfrozen caveman programmer</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:erikerlandson.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Measuring Decision Tree Split Quality with Test Statistic P-Values</h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-26T14:39:00-07:00" pubdate data-updated="true">May 26<span>th</span>, 2016</time>
        
        
         | <a href="#feedback">Feedback</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>When training a <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a> learning model (or an <a href="https://en.wikipedia.org/wiki/Random_forest">ensemble</a> of such models) it is often nice to have a policy for deciding when a tree node can no longer be usefully split.  There are a variety possibilities.  For example, halting when node population size becomes smaller than some threshold is a simple and effective policy.  Another approach is to halt when some measure of node purity fails to increase by some minimum threshold.  <strong>The underlying concept is to have some measure of split <em>quality</em>, and to halt when no candidate split has sufficient quality.</strong></p>

<p>In this post I am going to discuss some advantages to one of my favorite approaches to measuring split quality, which is to use a <a href="https://en.wikipedia.org/wiki/Statistical_significance">test statistic significance</a> &#8211; aka &#8220;p-value&#8221; &#8211; of the null hypothesis that the left and right sub-populations are the same after the split.  The idea is that if a split is of good quality, then it ought to have caused the sub-populations to the left and right of the split to be <em>meaningfully different</em>.  That is to say: the null hypothesis (that they are the same) should be <em>rejected</em> with high confidence, i.e. a small p-value.  What constitutes &#8220;small&#8221; is always context dependent, but popular p-values from applied statistics are 0.05, 0.01, 0.005, etc.</p>

<blockquote><p>update &#8211; there is now an Apache Spark <a href="https://issues.apache.org/jira/browse/SPARK-15699">JIRA</a> and a <a href="https://github.com/apache/spark/pull/13440">pull request</a> for this feature</p></blockquote>

<p>The remainder of this post is organized in the following sections:</p>

<p><a href="#consistency">Consistency</a> <br>
<a href="#awareness">Awareness of Sample Sizes</a> <br>
<a href="#results">Training Results</a> <br>
<a href="#conclusion">Conclusion</a> <br></p>

<p><a name="consistency"></a></p>

<h5>Consistency</h5>

<p>Test statistic p-values have some appealing properties as a split quality measure.  The test statistic methodology has the advantage of working essentially the same way regardless of the particular test being used.  We begin with two sample populations; in our case, these are the left and right sub-populations created by a candidate split.  We want to assess whether these two populations have the same distribution (the null hypothesis) or different distributions.  We measure some test statistic &#8216;S&#8217; (<a href="https://en.wikipedia.org/wiki/Student's_t-test">Student&#8217;s t</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a>, etc).  We then compute the probability that |S| >= the value we actually measured.  This probability is commonly referred to as the p-value.  The smaller the p-value, the less likely it is that our two populations are the same.  <strong>In our case, we can interpret this as: a smaller p-value indicates a better quality split.</strong></p>

<p>This consistent methodology has a couple advantages contributing to user experience (UX).  If all measures of split quality work in the same way, then there is a lower cognitive load to move between measures once the user understands the common pattern of use.  A second advantage is better &#8220;unit analysis.&#8221;  Since all such quality measures take the form of p-values, there is no risk of a chosen quality measure getting mis-aligned with a corresponding quality threshold.  They are all probabilities, on the interval [0,1], and &#8220;smaller threshold&#8221; always means &#8220;higher threshold of split quality.&#8221;   By way of comparison, if an application is measuring <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">entropy</a> and then switches to using <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini impurity</a>, these measures are in differing units and care has to be taken that the correct quality threshold is used in each case or the model training policy will be broken.  Switching between differing statistical tests does not come with the same risk.  <strong>A p-value quality threshold will have the same semantic regardless of which statistical test is being applied:</strong> probability that left and right sub-populations are the same, given the particular statistic being measured.</p>

<p><a name="awareness"></a></p>

<h5>Awareness of Sample Size</h5>

<p>Test statistics have another appealing property: many are &#8220;aware&#8221; of sample size in a way that captures the idea that the smaller the sample size, the larger the difference between populations should be to conclude a given significance.  For one example, consider <a href="https://en.wikipedia.org/wiki/Welch's_t-test#Statistical_test">Welch&#8217;s t-test</a>, the two-sample variation of the t distribution that applies well to comparing left and right sub populations of candidate decision tree splits:</p>

<p><img src="/assets/images/pval_halting/figure_1.png" alt="Figure 1" /></p>

<p>Visualizing the effects of sample sizes n1 and n2 on these equations directly is a bit tricky, but assuming equal sample sizes and variances allows the equations to be simplified quite a bit, so that we can observe the effect of sample size:</p>

<p><img src="/assets/images/pval_halting/figure_2.png" alt="Figure 2" /></p>

<p>These simplified equations show clearly that (all else remaining equal) as sample size grows smaller, the measured t-statistic correspondingly grows smaller (proportional to sqrt(n)), and furthermore the corresponding variance of the t distribution to be applied grows larger.  For any given shift in left and right sub-populations, each of these trends yields a larger (i.e. weaker) p-value.   This behavior is desirable for a split quality metric.  <strong>The less data there is at a given candidate split, the less confidence there <em>should</em> be in split quality.</strong>  Put another way: we would like to require a larger difference before a split is measured as being good quality when we have less data to work with, and that is exactly the behavior the t-test provides us.</p>

<p><a name="results"></a></p>

<h5>Training Results</h5>

<p>These propreties are pleasing, but it remains to show that test statistics can actually improve decision tree training in practice.  In the following sections I will compare the effects of training with test statstics with other split quality policies based on entropy and gini index.</p>

<p>To conduct these experiments, I modified a <a href="https://github.com/erikerlandson/spark/blob/pval_halting/mllib/src/main/scala/org/apache/spark/mllib/tree/impurity/ChiSquared.scala">local copy</a> of Apache Spark with the <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a> test statistic for comparing categorical distributions.  The demo script, which I ran in <code>spark-shell</code>, can be viewed <a href="https://github.com/erikerlandson/spark/blob/pval_halting/pval_demo">here</a>.</p>

<p>I generated an example data set that represents a two-class learning problem, where labels may be 0 or 1.  Each sample has 10 clean binary features, such that if the bit is 1, the probability of the label is 90% 1 and 10% 0.  There are 5 noise features, also binary, which are completely random.   There are 50 samples of each clean feature being on, for a total of 500 samples.   There are also 500 samples where all clean features are 0 and the corresponding labels are 90% 0 and 10% 1.  The total number of samples in the data set is 1000.  The shape of the data is illustrated by the following table:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>truth |     features 0 - 9 (one on at a time)     |   random noise
</span><span class='line'>------+-------------------------------------------+--------------
</span><span class='line'>90% 1 | 1   0   0   0   0   0   0   0   0   0   0 | 1   0   0   1   0
</span><span class='line'>90% 1 |  ... 50 samples with feature 0 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 | 0   1   0   0   0   0   0   0   0   0   0 | 0   1   1   0   0
</span><span class='line'>90% 1 |  ... 50 samples with feature 1 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 2 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 3 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 4 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 5 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 6 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 7 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 8 'on' ...   |   ... noise ...
</span><span class='line'>90% 1 |  ... 50 samples with feature 9 'on' ...   |   ... noise ...
</span><span class='line'>90% 0 | 0   0   0   0   0   0   0   0   0   0   0 | 1   1   0   0   1
</span><span class='line'>90% 0 |  ... 500 samples with all 'off  ...       |   ... noise ...</span></code></pre></td></tr></table></div></figure>


<p>For the first run I use my customized chi-squared statistic as the split quality measure.  I used a p-value threshold of 0.01 &#8211; that is, I would like my chi-squared test to conclude that the probability of left and right split populations are the same is &lt;= 0.01, or that split will not be used.  Note, this means I can expect that around 1% of the time, it will conclude a split was good, when it was just luck.  This is a reasonable false-positive rate; random forests are by nature robust to noise, including noise in their own split decisions:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; :load pval_demo
</span><span class='line'>Loading pval_demo...
</span><span class='line'>defined module demo
</span><span class='line'>
</span><span class='line'>scala&gt; val rf = demo.train("chisquared", 0.01, noise = 0.1)
</span><span class='line'>  pval= 1.578e-09
</span><span class='line'>gain= 20.2669
</span><span class='line'>  pval= 1.578e-09
</span><span class='line'>gain= 20.2669
</span><span class='line'>  pval= 1.578e-09
</span><span class='line'>gain= 20.2669
</span><span class='line'>  pval= 9.140e-09
</span><span class='line'>gain= 18.5106
</span><span class='line'>
</span><span class='line'>... more tree p-value demo output ...
</span><span class='line'>
</span><span class='line'>  pval= 0.7429
</span><span class='line'>gain= 0.2971
</span><span class='line'>  pval= 0.9287
</span><span class='line'>gain= 0.0740
</span><span class='line'>  pval= 0.2699
</span><span class='line'>gain= 1.3096
</span><span class='line'>rf: org.apache.spark.mllib.tree.model.RandomForestModel = 
</span><span class='line'>TreeEnsembleModel classifier with 1 trees
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>scala&gt; println(rf.trees(0).toDebugString)
</span><span class='line'>DecisionTreeModel classifier of depth 10 with 21 nodes
</span><span class='line'>  If (feature 5 in {1.0})
</span><span class='line'>   Predict: 1.0
</span><span class='line'>  Else (feature 5 not in {1.0})
</span><span class='line'>   If (feature 6 in {1.0})
</span><span class='line'>    Predict: 1.0
</span><span class='line'>   Else (feature 6 not in {1.0})
</span><span class='line'>    If (feature 0 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 0 not in {1.0})
</span><span class='line'>     If (feature 1 in {1.0})
</span><span class='line'>      Predict: 1.0
</span><span class='line'>     Else (feature 1 not in {1.0})
</span><span class='line'>      If (feature 2 in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 2 not in {1.0})
</span><span class='line'>       If (feature 8 in {1.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>       Else (feature 8 not in {1.0})
</span><span class='line'>        If (feature 3 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 3 not in {1.0})
</span><span class='line'>         If (feature 4 in {1.0})
</span><span class='line'>          Predict: 1.0
</span><span class='line'>         Else (feature 4 not in {1.0})
</span><span class='line'>          If (feature 7 in {1.0})
</span><span class='line'>           Predict: 1.0
</span><span class='line'>          Else (feature 7 not in {1.0})
</span><span class='line'>           If (feature 9 in {1.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>           Else (feature 9 not in {1.0})
</span><span class='line'>            Predict: 0.0
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p>The first thing to observe is that <strong>the resulting decision tree used exactly the 10 clean features 0 through 9, and none of the five noise features.</strong>   The tree splits off each of the clean features to obtain an optimally accurate leaf-node (one with 90% 1s and 10% 0s).  A second observation is that the p-values shown in the demo output are extremely small (i.e. strong) values &#8211; around 1e-9 (one part in a billion) &#8211; for good-quality splits.  We can also see &#8220;weak&#8221; p-values with magnitudes such as 0.7, 0.2, etc.  These are poor quality splits on the noise features that it rejects and does not use in the tree, exactly as we hope to see.</p>

<p>Next, I will show a similar run with the standard available &#8220;entropy&#8221; quality measure, and a minimum gain threshold of 0.035, which is a value I had to determine by trial and error, as what kind of entropy gains one can expect to see, and where to cut them off, is somewhat unintuitive and likely to be very data dependent.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val rf = demo.train("entropy", 0.035, noise = 0.1)
</span><span class='line'>  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
</span><span class='line'>gain= 0.0310
</span><span class='line'>  impurity parent= 0.9970, left= 0.1414 (  50), right= 0.9998 ( 950) weighted= 0.9569
</span><span class='line'>gain= 0.0402
</span><span class='line'>  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
</span><span class='line'>gain= 0.0310
</span><span class='line'>
</span><span class='line'>... more demo output ...
</span><span class='line'>
</span><span class='line'>rf: org.apache.spark.mllib.tree.model.RandomForestModel = 
</span><span class='line'>TreeEnsembleModel classifier with 1 trees
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>scala&gt; println(rf.trees(0).toDebugString)
</span><span class='line'>DecisionTreeModel classifier of depth 11 with 41 nodes
</span><span class='line'>  If (feature 4 in {1.0})
</span><span class='line'>   If (feature 12 in {1.0})
</span><span class='line'>    If (feature 11 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 11 not in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>   Else (feature 12 not in {1.0})
</span><span class='line'>    Predict: 1.0
</span><span class='line'>  Else (feature 4 not in {1.0})
</span><span class='line'>   If (feature 1 in {1.0})
</span><span class='line'>    If (feature 12 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 12 not in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>   Else (feature 1 not in {1.0})
</span><span class='line'>    If (feature 0 in {1.0})
</span><span class='line'>     If (feature 10 in {0.0})
</span><span class='line'>      If (feature 14 in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 14 not in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>     Else (feature 10 not in {0.0})
</span><span class='line'>      If (feature 14 in {0.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 14 not in {0.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>    Else (feature 0 not in {1.0})
</span><span class='line'>     If (feature 6 in {1.0})
</span><span class='line'>      Predict: 1.0
</span><span class='line'>     Else (feature 6 not in {1.0})
</span><span class='line'>      If (feature 3 in {1.0})
</span><span class='line'>       Predict: 1.0
</span><span class='line'>      Else (feature 3 not in {1.0})
</span><span class='line'>       If (feature 7 in {1.0})
</span><span class='line'>        If (feature 13 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 13 not in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>       Else (feature 7 not in {1.0})
</span><span class='line'>        If (feature 2 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 2 not in {1.0})
</span><span class='line'>         If (feature 8 in {1.0})
</span><span class='line'>          Predict: 1.0
</span><span class='line'>         Else (feature 8 not in {1.0})
</span><span class='line'>          If (feature 9 in {1.0})
</span><span class='line'>           If (feature 11 in {1.0})
</span><span class='line'>            If (feature 13 in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>            Else (feature 13 not in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>           Else (feature 11 not in {1.0})
</span><span class='line'>            If (feature 12 in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>            Else (feature 12 not in {1.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>          Else (feature 9 not in {1.0})
</span><span class='line'>           If (feature 5 in {1.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>           Else (feature 5 not in {1.0})
</span><span class='line'>            Predict: 0.0
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p>The first observation is that <strong>the resulting tree using entropy as a split quality measure is twice the size of the tree trained using the chi-squared policy.</strong>  Worse, it is using the noise features &#8211; its quality measure is yielding many more false positives.  The entropy-based model is less parsimonious and will also have performance problems since the model has included very noisy features.</p>

<p>Lastly, I ran a similar training using the &#8220;gini&#8221; impurity measure, and a 0.015 quality threshold (again, hopefully optimal value that I had to run multiple experiments to identify).  Its quality is better than the entropy-based measure, but this model is still substantially larger than the chi-squared model, and it still uses some noise features:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val rf = demo.train("gini", 0.015, noise = 0.1)
</span><span class='line'>  impurity parent= 0.4999, left= 0.2952 (  50), right= 0.4987 ( 950) weighted= 0.4885
</span><span class='line'>gain= 0.0113
</span><span class='line'>  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
</span><span class='line'>gain= 0.0158
</span><span class='line'>  impurity parent= 0.4999, left= 0.1472 (  50), right= 0.4981 ( 950) weighted= 0.4806
</span><span class='line'>gain= 0.0193
</span><span class='line'>  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
</span><span class='line'>gain= 0.0158
</span><span class='line'>
</span><span class='line'>... more demo output ...
</span><span class='line'>
</span><span class='line'>rf: org.apache.spark.mllib.tree.model.RandomForestModel = 
</span><span class='line'>TreeEnsembleModel classifier with 1 trees
</span><span class='line'>
</span><span class='line'>scala&gt; println(rf.trees(0).toDebugString)
</span><span class='line'>DecisionTreeModel classifier of depth 12 with 31 nodes
</span><span class='line'>  If (feature 6 in {1.0})
</span><span class='line'>   Predict: 1.0
</span><span class='line'>  Else (feature 6 not in {1.0})
</span><span class='line'>   If (feature 3 in {1.0})
</span><span class='line'>    Predict: 1.0
</span><span class='line'>   Else (feature 3 not in {1.0})
</span><span class='line'>    If (feature 1 in {1.0})
</span><span class='line'>     Predict: 1.0
</span><span class='line'>    Else (feature 1 not in {1.0})
</span><span class='line'>     If (feature 8 in {1.0})
</span><span class='line'>      Predict: 1.0
</span><span class='line'>     Else (feature 8 not in {1.0})
</span><span class='line'>      If (feature 2 in {1.0})
</span><span class='line'>       If (feature 14 in {0.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>       Else (feature 14 not in {0.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>      Else (feature 2 not in {1.0})
</span><span class='line'>       If (feature 5 in {1.0})
</span><span class='line'>        Predict: 1.0
</span><span class='line'>       Else (feature 5 not in {1.0})
</span><span class='line'>        If (feature 7 in {1.0})
</span><span class='line'>         Predict: 1.0
</span><span class='line'>        Else (feature 7 not in {1.0})
</span><span class='line'>         If (feature 0 in {1.0})
</span><span class='line'>          If (feature 12 in {1.0})
</span><span class='line'>           If (feature 10 in {0.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>           Else (feature 10 not in {0.0})
</span><span class='line'>            Predict: 1.0
</span><span class='line'>          Else (feature 12 not in {1.0})
</span><span class='line'>           Predict: 1.0
</span><span class='line'>         Else (feature 0 not in {1.0})
</span><span class='line'>          If (feature 9 in {1.0})
</span><span class='line'>           Predict: 1.0
</span><span class='line'>          Else (feature 9 not in {1.0})
</span><span class='line'>           If (feature 4 in {1.0})
</span><span class='line'>            If (feature 10 in {0.0})
</span><span class='line'>             Predict: 1.0
</span><span class='line'>            Else (feature 10 not in {0.0})
</span><span class='line'>             If (feature 14 in {0.0})
</span><span class='line'>              Predict: 1.0
</span><span class='line'>             Else (feature 14 not in {0.0})
</span><span class='line'>              Predict: 1.0
</span><span class='line'>           Else (feature 4 not in {1.0})
</span><span class='line'>            Predict: 0.0
</span><span class='line'>
</span><span class='line'>scala&gt;</span></code></pre></td></tr></table></div></figure>


<p><a name="conclusion"></a></p>

<h5>Conclusion</h5>

<p>In this post I have discussed some advantages of using test statstics and p-values as split quality metrics for decision tree training:</p>

<ul>
<li>Consistency</li>
<li>Awareness of sample size</li>
<li>Higher quality model training</li>
</ul>


<p>I believe they are a useful tool for improved training of decision tree models!  Happy computing!</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Erik Erlandson</span></span>

      








  


<time datetime="2016-05-26T14:39:00-07:00" pubdate data-updated="true">May 26<span>th</span>, 2016</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/apache-spark/'>apache spark</a>, <a class='category' href='/blog/categories/computing/'>computing</a>, <a class='category' href='/blog/categories/decision-tree/'>decision tree</a>, <a class='category' href='/blog/categories/learning-models/'>learning models</a>, <a class='category' href='/blog/categories/math/'>math</a>, <a class='category' href='/blog/categories/random-forests/'>random forests</a>, <a class='category' href='/blog/categories/statistics/'>statistics</a>
  
</span>


      <br>
<a id="feedback"></a>Feedback • 
 
    <script type="text/javascript" src="//platform.twitter.com/widgets.js"></script>
    
  <a href="https://twitter.com/intent/tweet?text=@manyangled%20re:%20http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/">Reply to this post on Twitter</a> • 
 
<a href="mailto:erikerlandson@yahoo.com">Email the author</a>

    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/" data-via="manyangled" data-counturl="http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations/" title="Previous Post: Random Forest Clustering of Machine Package Configurations in Apache Spark">&laquo; Random Forest Clustering of Machine Package Configurations in Apache Spark</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/" title="Next Post: Exploring the Effects of Dimensionality on a PDF of Distances">Exploring the Effects of Dimensionality on a PDF of Distances &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>About The Author</h1>
  <p>Erik is a software engineer at <a href="http://www.redhat.com">Red Hat</a> where he contributes to the <a href="https://radanalytics.io/">radanalytics.io</a> upstream community.
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/06/03/solving-feasible-points-with-smooth-max/">Solving Feasible Points With Smooth-Max</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">Computing Smooth Max and its Gradients Without Over- and Underflow</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">The Gradient and Hessian of the Smooth Max Over Functions</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/08/23/generalizing-the-concept-of-release-versioning/">Rethinking the Concept of Release Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/19/converging-monoid-addition-for-t-digest/">Converging Monoid Addition for T-Digest</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/erikerlandson">@erikerlandson</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'erikerlandson',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("manyangled", 4, true);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/manyangled" class="twitter-follow-button" data-show-count="false">Follow @manyangled</a>
  
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Erik Erlandson -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
