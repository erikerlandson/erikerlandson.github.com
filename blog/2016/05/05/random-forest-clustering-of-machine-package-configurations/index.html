
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Random Forest Clustering of Machine Package Configurations in Apache Spark - tool monkey</title>
  <meta name="author" content="Erik Erlandson">

  
  <meta name="description" content="In this post I am going to describe some results I obtained for clustering machines by which RPM packages that were installed on them. The clustering &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://erikerlandson.github.com/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="tool monkey" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

  <!-- enables inclusion of MathJax LaTeX: http://greglus.com/blog/2011/11/29/integrate-MathJax-LaTeX-and-MathML-Markup-in-Octopress/ -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">tool monkey</a></h1>
  
    <h2>adventures of an unfrozen caveman programmer</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:erikerlandson.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Random Forest Clustering of Machine Package Configurations in Apache Spark</h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-05T15:05:00-07:00" pubdate data-updated="true">May 5<span>th</span>, 2016</time>
        
        
         | <a href="#feedback">Feedback</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>In this post I am going to describe some results I obtained for <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a> machines by which <a href="https://en.wikipedia.org/wiki/RPM_Package_Manager">RPM packages</a> that were installed on them.  The clustering technique I used was <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup">Random Forest Clustering</a>.</p>

<p><a name="data"></a></p>

<h5>The Data</h5>

<p>The data I clustered consisted of 135 machines, each with a list of installed RPM packages.  The number of unique package names among all 135 machines was 4397.  Each machine was assigned a vector of Boolean values: a value of <code>1</code> indicates that the corresponding RPM was installed on that machine.  This means that the clustering data occupied a space of nearly 4400 dimensions.  I discuss the implications of this <a href="#payoff">later in the post</a>, and what it has to do with Random Forest Clustering in particular.</p>

<p>For ease of navigation and digestion, the remainder of this post is organized in sections:</p>

<p><a href="#clustering">Introduction to Random Forest Clustering</a> <br>
&nbsp; &nbsp; &nbsp; &nbsp;  (<a href="#payoff">The Pay-Off</a>) <br>
<a href="#code">Package Configuration Clustering Code</a> <br>
<a href="#results">Clustering Results</a> <br>
&nbsp; &nbsp; &nbsp; &nbsp;  (<a href="#outliers">Outliers</a>) <br></p>

<p><a name="clustering"></a></p>

<h5>Random Forests and Random Forest Clustering</h5>

<p>Full explainations of <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests</a> and <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup">Random Forest Clustering</a> could easily occupy blog posts of their own, but I will attempt to summarize them briefly here.  Random Forest learning models <em>per se</em> are well covered in the machine learning community, and available in most machine learning toolkits.  With that in mind, I will focus on their application to Random Forest Clustering, as it is less commonly used.</p>

<p>A Random Forest is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble learning model</a>, consisting of some number of individual <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a>, each trained on a random subset of the training data, and which choose from a random subset of candidate features when learning each internal decision node.</p>

<p>Random Forest Clustering begins by training a Random Forest to distinguish between the data to be clustered, and a corresponding <em>synthetic</em> data set created by sampling from the <a href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal</a> distributions of each <a href="https://en.wikipedia.org/wiki/Feature_vector">feature</a>.  If the data has well defined clusters in the <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint feature space</a> (a common scenario), then the model can identify these clusters as standing out from the more homogeneous distribution of synthetic data.  A simple example of what this looks like in 2 dimensional data is displayed in Figure 1, where the dark red dots are the data to be clustered, and the lighter pink dots represent synthetic data generated from the marginal distributions:</p>

<p><img src="/assets/images/rfc_machines/demo1_both.png" alt="Figure 1" /></p>

<p>Each interior decision node, in each tree of a Random Forest, typically divides the space of feature vectors in half: the half-space &lt;= some threshold, and the half-space > that threshold.  The result is that the model learned for our data can be visualized as rectilinear regions of space.  In this simple example, these regions can be plotted directly over the data, and show that the Random Forest did indeed learn the location of the data clusters against the background of synthetic data:</p>

<p><img src="/assets/images/rfc_machines/demo1_rules.png" alt="Figure 2" /></p>

<p>Once this model has been trained, the actual data to be clustered are evaluated against this model.  Each data element navigates the interior decision nodes and eventually arrives at a leaf-node of each tree in the Random Forest ensemble, as illustrated in the following schematic:</p>

<p><img src="/assets/images/rfc_machines/eval_leafs.png" alt="Figure 3" /></p>

<p>A key insight of Random Forest Clustering is that if two objects (or, their feature vectors) are similar, then they are likely to arrive at the same leaf nodes more often than not.  As the figure above suggests, it means we can cluster objects by their corresponding vectors of leaf nodes, <em>instead</em> of their raw feature vectors.</p>

<p>If we map the points in our toy example to leaf ids in this way, and then cluster the results, we obtain the following two clusters, which correspond well with the structure of the data:</p>

<p><img src="/assets/images/rfc_machines/demo1_clust.png" alt="Figure 4" /></p>

<p>A note on clustering leaf ids.  A leaf id is just that &#8211; an identifier &#8211; and in that respect a vector of leaf ids has no <em>algebra</em>; it is not meaningful to take an average of such identifiers, any more than it would be meaningful to take the average of people&#8217;s names.  Pragmatically, what this means is that the popular <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering algorithm</a> <em>cannot</em> be applied to this problem.</p>

<p>These vectors do, however, have <em>distance</em>: for any pair of vectors, add 1 for each corresponding pair of leaf ids that differ.  If two data elements arrived at all the same leafs in the Random Forest model, all their leaf ids are the same, and their distance is zero (with respect to the model, they are the same).  Therefore, we <em>can</em> apply <a href="https://en.wikipedia.org/wiki/K-medoids">k-medoids clustering</a>.</p>

<p><a name="payoff"></a></p>

<h5>The Pay-Off</h5>

<p>What does this somewhat indirect method of clustering buy us?  Why <em>not</em> just cluster objects by their raw feature vectors?</p>

<p>The problem is that in many real-world cases (unlike in our toy example above), feature vectors computed for objects have <em>many dimensions</em> &#8211; hundreds, thousands, perhaps millions &#8211; instead of the two dimensions in this example.  Computing distances on such objects, necessary for clustering, is often expensive, and worse yet the quality of these distances is frequently poor due to the fact that most features in large spaces will be poorly correlated with <em>any</em> structure in the data.  This problem is so common, and so important, it has a name: the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a>.</p>

<p>Random Forest Clustering, which clusters on vectors of leaf-node ids from the trees in the model, side-steps the curse of dimensionality because the Random Forest training process, by learning where the data is against the background of the synthetic data, has already identified the features that are useful for identifying the structure of the data!   If any particular feature was poorly correlated with that struture, it has already been ignored by the model.  In other words, a Random Forest Clustering model is implicitly examining <strong> <em>exactly those features that are most useful for clustering</em> </strong>, thus providing a cure for the Curse of Dimensionality.</p>

<p>The <a href="#data">machine package configurations</a> whose clustering I describe for this post are a good example of high dimensional data that is vulnerable to the Curse of Dimensionality.  The dimensionality of the feature space is nearly 4400, making distances between vectors potentially expensive to evaluate.  Any individual feature contributes little to the distance, having to contend with over 4000 other features.  Installed packages are also noisy.  Many packages, such as kernels, are installed everywhere.  Others may be installed but not used, making them potentially irrelevant to grouping machines.  Furthermore, there are only 135 machines, and so there are far more features than data examples, making this an underdetermined data set.</p>

<p>All of these factors make the machine package configuration data a good test of the strenghts of Random Forest Clustering.</p>

<p><a name="code"></a></p>

<h5>Package Configuration Clustering Code</h5>

<p>The implementation of Random Forest Clustering I used for the results in this post is a library available from the <a href="http://silex.freevariable.com/">silex project</a>, a package of analytics libraries and utilities for <a href="http://spark.apache.org/">Apache Spark</a>.</p>

<p>In this section I will describe three code fragments that load the machine configuration data, perform a Random Forest clustering, and format some of the output.  This is the code I ran to obtain the <a href="#results">results</a> described in the final section of this post.</p>

<p>The first fragment of code illustrates the logistics of loading the feature vectors from file <code>train.txt</code> that represent the installed-package configurations for each machine. A corresponding &#8220;parallel&#8221; file <code>nodesclean.txt</code> contains corresponding machine names for each vector.  A third companion file <code>rpms.txt</code> contains names of each installed package.  These are used to instantiate a specialized Scala function (<code>InvertibleIndexFunction</code>) between feature indexes and human-readable feature names (in this case, names of RPM packages).  Finally, another specialized function (<code>Extractor</code>) for instantiating Spark feature vectors is created.</p>

<p>Note: <code>Extractor</code> and <code>InvertibleIndexFunction</code> are also component libraries of <a href="http://silex.freevariable.com/">silex</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Load installed-package feature vectors</span>
</span><span class='line'><span class="k">val</span> <span class="n">fields</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$dataDir/train.txt&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Pair feature vectors with machine names</span>
</span><span class='line'><span class="k">val</span> <span class="n">nodes</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$dataDir/nodesclean.txt&quot;</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)(</span><span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="k">val</span> <span class="n">ids</span> <span class="k">=</span> <span class="n">fields</span><span class="o">.</span><span class="n">paste</span><span class="o">(</span><span class="n">nodes</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Load map from feature indexes to package names</span>
</span><span class='line'><span class="k">val</span> <span class="n">inp</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$dataDir/rpms.txt&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">r</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">toInt</span><span class="o">,</span> <span class="n">r</span><span class="o">(</span><span class="mi">1</span><span class="o">)))</span>
</span><span class='line'>  <span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">toVector</span><span class="o">.</span><span class="n">sorted</span>
</span><span class='line'><span class="k">val</span> <span class="n">nf</span> <span class="k">=</span> <span class="nc">InvertibleIndexFunction</span><span class="o">(</span><span class="n">inp</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// A feature extractor maps features into sequence of doubles</span>
</span><span class='line'><span class="k">val</span> <span class="n">m</span> <span class="k">=</span> <span class="n">fields</span><span class="o">.</span><span class="n">first</span><span class="o">.</span><span class="n">length</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class='line'><span class="k">val</span> <span class="n">ext</span> <span class="k">=</span> <span class="nc">Extractor</span><span class="o">(</span><span class="n">m</span><span class="o">,</span> <span class="o">(</span><span class="n">v</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">=&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">).</span><span class="n">tail</span> <span class="k">:</span><span class="kt">FeatureSeq</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">withNames</span><span class="o">(</span><span class="n">nf</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">withCategoryInfo</span><span class="o">(</span><span class="nc">IndexFunction</span><span class="o">.</span><span class="n">constant</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="n">m</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>The next section of code is where the work of Random Forest Clustering happens.  A <code>RandomForestCluster</code> object is instantiated, and configured.  Here, the configuration is for 7 clusters, 250 synthetic points (about twice as many synthetic points as true data), and a Random Forest of 20 trees.  Training against the input data is a simple call to the <code>run</code> method.</p>

<p>The <code>predictWithDistanceBy</code> method is then applied to the data paired with machine names, to yield tuples of cluster-id, distance to cluster center, and the associated machine name.  These tuples are split by distance into data with a cluster, and data considered to be &#8220;outliers&#8221; (i.e. elements far from any cluster center).  Lastly, the <code>histFeatures</code> method is applied, to examine the Random Forest Model and identify any commonly-used features.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Train a Random Forest Clustering Model</span>
</span><span class='line'><span class="k">val</span> <span class="n">rfcModel</span> <span class="k">=</span> <span class="nc">RandomForestCluster</span><span class="o">(</span><span class="n">ext</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setClusterK</span><span class="o">(</span><span class="mi">7</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setSyntheticSS</span><span class="o">(</span><span class="mi">250</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setRfNumTrees</span><span class="o">(</span><span class="mi">20</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">setSeed</span><span class="o">(</span><span class="mi">37</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">run</span><span class="o">(</span><span class="n">fields</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Evaluate to get tuples: (cluster, distance, machine-name)</span>
</span><span class='line'><span class="k">val</span> <span class="n">cid</span> <span class="k">=</span> <span class="n">ids</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">rfcModel</span><span class="o">.</span><span class="n">predictWithDistanceBy</span><span class="o">(</span><span class="k">_</span><span class="o">)(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Split by closest distances into clusters and outliers  </span>
</span><span class='line'><span class="k">val</span> <span class="o">(</span><span class="n">clusters</span><span class="o">,</span> <span class="n">outliers</span><span class="o">)</span> <span class="k">=</span> <span class="n">cid</span><span class="o">.</span><span class="n">splitFilter</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">dist</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">dist</span> <span class="o">&lt;=</span> <span class="mi">5</span> <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Generate a histogram of features used in the RF model</span>
</span><span class='line'><span class="k">val</span> <span class="n">featureHist</span> <span class="k">=</span> <span class="n">rfcModel</span><span class="o">.</span><span class="n">randomForestModel</span><span class="o">.</span><span class="n">histFeatures</span><span class="o">(</span><span class="n">ext</span><span class="o">.</span><span class="n">names</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The final code fragment simply formats clusters and outliers into a tabular form, as displayed in the <a href="#results">next section</a> of this post.  Note that there is neither Spark nor silex code here; standard Scala methods are sufficient to post-process the clustering data:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Format clusters for display</span>
</span><span class='line'><span class="k">val</span> <span class="n">clusterStr</span> <span class="k">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">j</span><span class="o">,</span> <span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">j</span><span class="o">,</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">))</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">groupByKey</span>
</span><span class='line'>  <span class="o">.</span><span class="n">collect</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">j</span><span class="o">,</span> <span class="n">nodes</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="n">nodes</span><span class="o">.</span><span class="n">toSeq</span><span class="o">.</span><span class="n">sorted</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="s">&quot;$d  $n&quot;</span> <span class="o">}.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n\n&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Format outliers for display</span>
</span><span class='line'><span class="k">val</span> <span class="n">outlierStr</span> <span class="k">=</span> <span class="n">outliers</span><span class="o">.</span><span class="n">collect</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">d</span><span class="o">,</span><span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">toVector</span><span class="o">.</span><span class="n">sorted</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">d</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="s">&quot;$d  $n&quot;</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><a name="results"></a></p>

<h5>Package Configuration Clustering Results</h5>

<p>The result of running the code in the <a href="#code">previous section</a> is seven clusters of machines.  In the following files, the first column represents distance from the cluster center, and the second is the actual machine&#8217;s node name.  A cluster distance of 0.0 indicates that the machine was indistinguishable from cluster center, as far as the Random Forest model was concerned.   The larger the distance, the more different from the cluster&#8217;s center a machine was, in terms of its installed RPM packages.</p>

<p>Was the clustering meaningful?  Examining the first two clusters below is promising; the machine names in these clusters are clearly similar, likely configured for some common task by the IT department.  The first cluster of machines appears to be web servers and corresponding backend services.  It would be unsurprising to find their RPM configurations were similar.</p>

<p>The second cluster is a series of executor machines of varying sizes, but presumably these would be configured similarly to one another.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_1"></script>




<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_2"></script>


<p>The second pair of clusters (3 &amp; 4) are small.  All of their names are similar (and furthermore, similar to some machines in other clusters), and so an IT administrator might wonder why they ended up in oddball small clusters.  Perhaps they have some spurious, non-standard packages installed that ought to be cleaned up.  Identifying these kinds of structure in a clustering is one common clustering application.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_3"></script>




<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_4"></script>


<p>Cluster 5 is a series of bugzilla web servers and corresponding back-end bugzilla data base services.  Although they were clustered together, we see that the web servers have a larger distance from the center, indicating a somewhat different configuration.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_5"></script>


<p>Cluster 6 represents a group of performance-related machines.  Not all of these machines occupy the same distance, even though most of their names are similar.  These are also the same series of machines as in clusters 3 &amp; 4.  Does this indicate spurious package installations, or some other legitimate configuration difference?  A question for the IT department&#8230;</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_6"></script>


<p>Cluster 7 is by far the largest.  It is primarily a combination of OpenStack machines and yet more perf machines.   This clustering was relatively stable &#8211; it appeared across multiple independent clustering runs.  Because of its stability I would suggest to an IT administrator that the performance and OpenStack machines are sharing some configuration similarities, and the performance machines in other clusters suggest that there might be yet more configuration anomalies.  Perhaps these were OpenStack nodes that were re-purposed as performance machines?  Yet another question for IT&#8230;</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=cluster_7"></script>


<p><a name="outliers"></a></p>

<h5>Outliers</h5>

<p>This last grouping represents machines which were &#8220;far&#8221; from any of the previous cluster centers.  They may be interpreted as &#8220;outliers&#8221; - machines that don&#8217;t fit any model category.  Of these the node <code>frodo</code> is clearly somebody&#8217;s personal machine, likely with a customized or idiosyncratic package configuration.  Unsurprising that it is farthest of all machines from any cluster, with distance 9.0.   The <code>jenkins</code> machine is also somewhat unique among the nodes, and so perhaps not surprising that its registers as anomalous.  The remaining machines match node series from other clusters.   Their large distance is another indication of spurious configurations for IT to examine.</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=outliers"></script>


<p>I will conclude with another useful feature of Random Forest Models, which is that you can interrogate them for information such as which features were used most frequently.  Here is a histogram of model features (in this case, installed packages) that were used most frequently in the clustering model.  This particular histogram i sinteresting, as no feature was used more than twice.  The remaining features were all used exactly once.  This is a bit unusual for a Random Forest model.  Frequently some features are used commonly, with a longer tail.  This histogram is rather &#8220;flat,&#8221; which may be a consequence of there being many more features (over 4000 installed packages) than there are data elements (135 machines).  This makes the problem somewhat under-determined.  To its credit, the model still achieves a meaningful clustering.</p>

<p>Lastly I&#8217;ll note that full histogram length was 186; in other words, of the nearly 4400 installed packages, the Random Forest model used only 186 of them &#8211; a tiny fraction!  A nice illustration of Random Forest Clustering performing in the face of <a href="#payoff">high dimensionality</a>!</p>

<script src="https://gist.github.com/erikerlandson/184d202560c628c0383c5050d9f4be24.js?file=histogram"></script>


<p><head><style type="text/css">
.gist {max-height:500px; overflow:auto}
</style></head></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Erik Erlandson</span></span>

      








  


<time datetime="2016-05-05T15:05:00-07:00" pubdate data-updated="true">May 5<span>th</span>, 2016</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/apache-spark/'>apache spark</a>, <a class='category' href='/blog/categories/clustering/'>clustering</a>, <a class='category' href='/blog/categories/computing/'>computing</a>, <a class='category' href='/blog/categories/decision-trees/'>decision trees</a>, <a class='category' href='/blog/categories/learning-models/'>learning models</a>, <a class='category' href='/blog/categories/machine-learning/'>machine learning</a>, <a class='category' href='/blog/categories/random-forests/'>random forests</a>
  
</span>


      <br>
<a id="feedback"></a>Feedback • 
 
    <script type="text/javascript" src="//platform.twitter.com/widgets.js"></script>
    
  <a href="https://twitter.com/intent/tweet?text=@manyangled%20re:%20http://erikerlandson.github.com/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations/">Reply to this post on Twitter</a> • 
 
<a href="mailto:erikerlandson@yahoo.com">Email the author</a>

    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://erikerlandson.github.com/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations/" data-via="manyangled" data-counturl="http://erikerlandson.github.com/blog/2016/05/05/random-forest-clustering-of-machine-package-configurations/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/03/26/computing-simplex-vertex-locations-from-pairwise-vertex-distances/" title="Previous Post: Computing Simplex Vertex Locations From Pairwise Object Distances">&laquo; Computing Simplex Vertex Locations From Pairwise Object Distances</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/" title="Next Post: Measuring Decision Tree Split Quality with Test Statistic P-Values">Measuring Decision Tree Split Quality with Test Statistic P-Values &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>About The Author</h1>
  <p>Erik is a software engineer at <a href="http://www.redhat.com">Red Hat</a> where he contributes to the <a href="https://radanalytics.io/">radanalytics.io</a> upstream community.
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/09/11/the-backtracking-ulp-incident-of-2018/">The Backtracking ULP Incident of 2018</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/08/equality-constraints-for-cubic-b-splines/">Equality Constraints for Cubic B-Splines</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form/">Putting Cubic B-Splines into Standard Polynomial Form</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/06/03/solving-feasible-points-with-smooth-max/">Solving Feasible Points With Smooth-Max</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">Computing Smooth Max and its Gradients Without Over- and Underflow</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/erikerlandson">@erikerlandson</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'erikerlandson',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("manyangled", 4, true);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/manyangled" class="twitter-follow-button" data-show-count="false">Follow @manyangled</a>
  
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Erik Erlandson -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
