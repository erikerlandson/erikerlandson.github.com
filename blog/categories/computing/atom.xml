<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: computing | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/computing/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2018-09-02T18:29:06-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Putting Cubic B-Splines into Standard Polynomial Form]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form/"/>
    <updated>2018-09-02T11:07:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form</id>
    <content type="html"><![CDATA[<p>Lately I have been working on an <a href="https://github.com/erikerlandson/snowball">implementation</a> of monotone smoothing splines, based on <a href="#ref1">[1]</a>. As the title suggests, this technique is based on a univariate cubic <a href="https://en.wikipedia.org/wiki/B-splines">B-spline</a>. The form of the spline function used in the paper is as follows:</p>

<p><img src="/assets/images/bspline/yd2guhxt.png" alt="eq1" /></p>

<p>The You can see that the constant α normalizes knot intervals to 1, and that the four <nobr>N<sub>i</sub>(t)</nobr> are defined in this transformed space of unit-separated knots.</p>

<p>I'm interested in providing an interpolated splines using the Apache Commons Math API, in particular the <a href="https://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/analysis/polynomials/PolynomialSplineFunction.html">PolynomialSplineFunction</a> class. In principle the above is clearly such a polynomial, but there are a few hitches.</p>

<ol>
<li><code>PolynomialSplineFunction</code> wants its knot intervals in closed standard polynomial form <nobr>ax<sup>3</sup> + bx<sup>2</sup> + cx + d</nobr></li>
<li>It wants each such polynomial expressed in transformed space <nobr>(x-K<sub>j</sub>)</nobr>, where K<sub>j</sub> is the greatest knot point that is &lt;= x.</li>
<li>The actual domain of S(x) is <nobr>K<sub>0</sub> ... K<sub>m-1</sub></nobr>. The first 3 "negative" knots are there to make the summation for S(x) cleaner. <code>PolynomialSplineFunction</code> needs its functions to be defined purely on the actual domain.</li>
</ol>


<p>If you study the definition of <nobr>B<sub>3</sub>(t)</nobr> above, you can see that if x lands in the interval <nobr>[K<sub>j</sub>, K<sub>j+1</sub>)</nobr> then it is the four knot points <nobr>K<sub>j-3</sub> ... K<sub>j</sub></nobr> that contribute to its value. This suggests a way to manipulate the equations into a standard form.</p>

<p>For a value x and its appropriate <nobr>K<sub>j</sub></nobr>, S(x) has four non-zero terms:</p>

<p><img src="/assets/images/bspline/y9tpgfqj.png" alt="eq2" /></p>

<p>Consider the first term for (j-3). Recalling that knots are equally spaced by 1/α:</p>

<p><img src="/assets/images/bspline/y79occ29.png" alt="eq3" /></p>

<p>We can apply similar logic for each term to get:</p>

<p><img src="/assets/images/bspline/ya6gsrjy.png" alt="eq4" /></p>

<p>and by plugging in the appropriate <nobr>N<sub>i</sub></nobr> for each term, we arrive at:</p>

<p><img src="/assets/images/bspline/yc6grwxe.png" alt="eq5" /></p>

<p>Now, <code>PolynomialSplineFunction</code> is going to automatically identify the appropriate <nobr>K<sub>j</sub></nobr> and subtract it, and so I can define that transform as <nobr>u = x -  K<sub>j</sub></nobr>, which gives:</p>

<p><img src="/assets/images/bspline/y9p3vgqt.png" alt="eq6" /></p>

<p>I substitute αu into the definitions of the four <nobr>N<sub>i</sub></nobr> to obtain:</p>

<p><img src="/assets/images/bspline/y8apdoqy.png" alt="eq7" /></p>

<p>Lastly, collecting like terms gives me the standard-form coefficients that I need for <code>PolynomialSplineFunction</code>:</p>

<p><img src="/assets/images/bspline/y7eon7kc.png" alt="eq8" /></p>

<p>Now I am equipped to return a <code>PolynomialSplineFunction</code> to my users, which implements the cubic B-spline that I fit to their data. Happy computing!</p>

<h4>References</h4>

<p><a name="anchor1" id="ref1">[1] </a>H. Fujioka and H. Kano: <a href="https://github.com/erikerlandson/snowball/blob/master/monotone-cubic-B-splines-2013.pdf">Monotone smoothing spline curves using normalized uniform cubic B-splines</a>, Trans. Institute of Systems, Control and Information Engineers, Vol. 26, No. 11, pp. 389–397, 2013</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solving Feasible Points With Smooth-Max]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/06/03/solving-feasible-points-with-smooth-max/"/>
    <updated>2018-06-03T14:21:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/06/03/solving-feasible-points-with-smooth-max</id>
    <content type="html"><![CDATA[<h3>Overture</h3>

<p>Lately I have been fooling around with an <a href="https://github.com/erikerlandson/gibbous">implementation</a> of the <a href="#cite1">Barrier Method</a> for convex optimization with constraints.
One of the characteristics of the Barrier Method is that it requires an initial-guess from inside the
<em>feasible region</em>: that is, a point which is known to satisfy all of the inequality constraints provided
by the user.
For some optimization problems, it is straightforward to find such a point by using knowledge about the problem
domain, but in many situations it is not at all obvious how to identify such a point, or even if a
feasible point exists. The feasible region might be empty!</p>

<p>Boyd and Vandenberghe discuss a couple approaches to finding feasible points in §11.4 of <a href="#cite1">[1]</a>.
These methods require you to set up an "augmented" minimization problem:
<img src="/assets/images/feasible/y9czf8u7.png" alt="eq1" /></p>

<p>As you can see from the above, you have to set up an "augmented" space x+s, where (s) represents an additional
dimension, and constraint functions are augmented to f<sub>k</sub>-s</p>

<h3>The Problem</h3>

<p>I experimented a little with these, and while I am confident they work for most problems having multiple
inequality constraints, my unit testing tripped over an ironic deficiency:
when I attempted to solve a feasible point for a single planar constraint, the numerics went a bit haywire.
Specifically, a linear constraint function happens to have a singular Hessian of all zeroes.
The final Hessian, coming out of the log barrier function, could be consumed by SVD to get a search direction
but the resulting gradients behaved poorly.</p>

<p>Part of the problem seems to be that the nature of this augmented minimization problem forces the algorithms
to push (s) ever downward, but letting (s) transitively push the f<sub>k</sub> with the augmented constraint
functions f<sub>k</sub>-s. When only a single linear constraint function is in play, the resulting gradient
caused augmented dimension (s) to converge <em>against</em> the movement of the remaining (unaugmented) sub-space.
The minimization did not converge to a feasible point, even though literally half of the space on one side
of the planar surface is feasible!</p>

<h3>Smooth Max</h3>

<p>Thinking about these issues made me wonder if a more direct approach was possible.
Another way to think about this problem is to minimize the maximum f<sub>k</sub>;
If the maximum f<sub>k</sub> is &lt; 0 at a point x, then x is a feasible point satisfying all f<sub>k</sub>.
If the smallest-possible maximum f<sub>k</sub> is > 0, then we have definitive proof that no
feasible point exists, and our constraints can't be satisfied.</p>

<p>Taking a maximum preserves convexity, which is a good start, but maximum isn't differentiable everywhere.
The boundaries between regions where different functions are the maximum are not smooth, and along
those boundaries there is no gradient, and therefore no Hessian either.</p>

<p>However, there is a variation on this idea, known as smooth-max, defined like so:</p>

<p><img src="/assets/images/feasible/y8cgykuc.png" alt="eq2" /></p>

<p>Smooth-max has a well defined <a href="http://erikerlandson.github.io/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">gradient and Hessian</a>, and furthermore can be computed in a <a href="http://erikerlandson.github.io/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">numerically stable</a> way.
The sum inside the logarithm above is a sum of exponentials of convex functions.
This is good news; exponentials of convex functions are log-convex, and a sum of log-convex functions is also
log-convex.</p>

<p>That means I have the necessary tools to set up the my mini-max problem:
For a given set of convex constraint functions f<sub>k</sub>, I create a functions which is the soft-max of
these, and I minimize it.</p>

<h3>Go Directly to Jail</h3>

<p>I set about implementing my smooth-max idea, and immediately ran into almost the same problem as before.
If I try to solve for a single planar constraint, my Hessian degenerates to all-zeros!
When I unpacked the smoothmax-formula for a single constraint f<sub>k</sub>, it indeed is just f<sub>k</sub>,
zero Hessian and all!</p>

<h3>More is More</h3>

<p>What to do?
Well you know what form of constraint <em>always</em> has a well behaved Hessian? A circle, that's what.
More technically, an n-dimensional ball, or n-ball.
What if I add a new constraint of the form:</p>

<p><img src="/assets/images/feasible/yd8xg64k.png" alt="eq3" /></p>

<p>This constraint equation is quadratic, and its Hessian is I<sub>n</sub>.
If I include this in my set of constraints, my smooth-max Hessian will be non-singular!</p>

<p>Since I do not know a priori where my feasible point might lie, I start with my n-ball centered at
my initial guess, and minimize. The result might look something like this:</p>

<p><img src="/assets/images/feasible/fig1.png" alt="fig1" /></p>

<p>Because the optimization is minimizing the maximum f<sub>k</sub>, the optimal point may not be feasible,
but if not it <em>will</em> end up closer to the feasible region than before.
This suggests an iterative algorithm, where I update the location of the n-ball at each iteration,
until the resulting optimized point lies on the intersection of my original constraints and my
additional n-ball constraint:</p>

<p><img src="/assets/images/feasible/fig2.png" alt="fig2" /></p>

<h3>Caught in the Underflow</h3>

<p>I implemented the iterative algorithm above (you can see what this loop looks like <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L134">here</a>),
and it worked exactly as I hoped...
at least on my initial tests.
However, eventually I started playing with its convergence behavior by moving my constraint region farther
from the initial guess, to see how it would cope.
Suddenly the algorithm began failing again.
When I drilled down on why, I was taken aback to discover that my Hessian matrix was once again showing
up as all zeros!</p>

<p>The reason was interesting.
Recall that I used a <a href="http://erikerlandson.github.io/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">modified formula</a> to stabilize my smooth-max computations.
In particular, the "stabilized" formula for the Hessian looks like this:</p>

<p><img src="/assets/images/smoothmax/eq3b.png" alt="eq4" /></p>

<p>So, what was going on?
As I started moving my feasible region farther away, the corresponding constraint function started to
dominate the exponential terms in the equation above.
In other words, the distance to the feasible region became the (z) in these equations, and
this z value was large enough to drive the terms corresponding to my n-ball constraint to zero!</p>

<p>However, I have a lever to mitigate this problem.
If I make the α parameter <em>small</em> enough, it will compress these exponent ranges and prevent my
n-ball Hessian terms from washing out.
Decreasing α makes smooth-max more rounded-out, and decreases the sharpness of the approximation to the true max,
but minimizing smooth-max still yields the same minimum <em>location</em> as true maximum, and so playing this
trick does not undermine my results.</p>

<p>How small is small enough?
α is essentially a free parameter, but I found that if I set it at each iteration,
such that I make sure that my n-ball Hessian coefficient never drops below 1e-3 (but may be larger),
then my Hessian is always well behaved.
Note that as my iterations grow closer to the true feasible region, I can gradually allow α to
grow larger.
Currently, I don't increase α larger than 1, to avoid creating curvatures too large, but I have not
experimented deeply with what actually happens if it were allowed to grow larger.
You can see what this looks like in my current implementation <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L153">here</a>.</p>

<h3>Convergence</h3>

<p>Tuning the smooth-max α parameter gave me numeric stability, but I noticed that as the feasible region
grew more distant from my initial guess, the algorithm's time to converge grew larger fairly quickly.
When I studied its behavior, I saw that at large distances, the quadratic "cost" of my n-ball constraint
effectively pulled the optimal point fairly close to my n-ball center.
This doesn't prevent the algorithm from finding a solution, but it does prevent it from going long distances
very fast.
To solve this adaptively, I added a scaling factor s to my n-ball constraint function.
The scaled version of the function looks like:</p>

<p><img src="/assets/images/feasible/y9gndl2f.png" alt="eq5" /></p>

<p>In my case, when my distances to a feasible region grow large, I want s to become small, so that it
causes the cost of the n-ball constraint to grow more slowly, and allow the optimization to move
farther, faster.
The following diagram illustrates this intuition:</p>

<p><img src="/assets/images/feasible/fig3.png" alt="fig3" /></p>

<p>In my algorithm, I set s = 1/σ, where σ represents the
"scale" of the current distance to feasible region.
The n-ball function grows as the square of the distance to the ball center; therefore I
set σ=(k)sqrt(s), so that it grows proportionally to the square root of the current largest user constraint
cost.
Here, (k) is a proportionality constant.
It too is a somewhat magic free parameter, but I have found that k=1.5 yields fast convergences and
good results.
One last trick I play is that I prevent σ from becoming less than a minimum value, currently 10.
This ensures that my n-ball constraint never dominates the total constraint sum, even as the
optimization converges close to the feasible region.
I want my "true" user constraints to dominate the behavior near the optimum, since those are the
constraints that matter.
The code is shorter than the explaination: you can see it <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L143">here</a></p>

<h3>Conclusion</h3>

<p>After applying all these intuitions, the resulting algorithm appears to be numerically stable and also
converges pretty quickly even when the initial guess is very far from the true feasible region.
To review, you can look at the main loop of this algorithm starting <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L128">here</a>.</p>

<p>I've learned a lot about convex optimization and feasible point solving from working through practical
problems as I made mistakes and fixed them.
I'm fairly new to the whole arena of convex optimization, and I expect I'll learn a lot more as I go.
Happy Computing!</p>

<h3>References</h3>

<p><a name="cite1"</a>
[1] §11.3 of <em>Convex Optimization</em>, Boyd and Vandenberghe, Cambridge University Press, 2008</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing Smooth Max and its Gradients Without Over- and Underflow]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/"/>
    <updated>2018-05-28T08:13:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow</id>
    <content type="html"><![CDATA[<p>In my <a href="http://erikerlandson.github.io/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">previous post</a> I derived the gradient and Hessian for the smooth max function.
The <a href="https://www.johndcook.com/blog/">Notorious JDC</a> wrote a helpful companion post that describes <a href="https://www.johndcook.com/blog/2010/01/20/how-to-compute-the-soft-maximum/">computational issues</a> of overflow and underflow with smooth max;
values of f<sub>k</sub> don't have to grow very large (or small) before floating point limitations start to force their exponentials to +inf or zero.
In JDC's post he discusses this topic in terms of a two-valued smooth max.
However it isn't hard to generalize the idea to a collection of f<sub>k</sub>.
Start by taking the maximum value over our collection of functions, which I'll define as (z):</p>

<p><img src="/assets/images/smoothmax/eq1b.png" alt="eq1" /></p>

<p>As JDC described in his post, this alternative expression for smooth max (m) is computationally stable.
Individual exponential terms may underflow to zero, but they are the ones which are dominated by the other terms, and so approximating them by zero is numerically accurate.
In the limit where one value dominates all others, it will be exactly the value given by (z).</p>

<p>It turns out that we can play a similar trick with computing the gradient:</p>

<p><img src="/assets/images/smoothmax/eq2b.png" alt="eq2" /></p>

<p>Without showing the derivation, we can apply exactly the same manipulation to the terms of the Hessian:</p>

<p><img src="/assets/images/smoothmax/eq3b.png" alt="eq3" /></p>

<p>And so we now have a computationally stable form of the equations for smooth max, its gradient and its Hessian. Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Gradient and Hessian of the Smooth Max Over Functions]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/"/>
    <updated>2018-05-27T09:36:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions</id>
    <content type="html"><![CDATA[<p>Suppose you have a set of functions over a vector space, and you are interested in taking the smooth-maximum over those functions.
For example, maybe you are doing gradient descent, or convex optimization, etc, and you need a variant on "maximum" that has a defined gradient.
The smooth maximum function has both a defined gradient and Hessian, and in this post I derive them.</p>

<p>I am using the <a href="https://www.johndcook.com/blog/2010/01/13/soft-maximum/">logarithm-based</a> definition of smooth-max, shown here:</p>

<p><img src="/assets/images/smoothmax/eq1.png" alt="eq1" /></p>

<p>I will use the second variation above, ignoring function arguments, with the hope of increasing clarity.
Applying the chain rule gives the ith partial gradient of smooth-max:</p>

<p><img src="/assets/images/smoothmax/eq2.png" alt="eq2" /></p>

<p>Now that we have an ith partial gradient, we can take the jth partial gradient of <em>that</em> to obtain the (i,j)th element of a Hessian:</p>

<p><img src="/assets/images/smoothmax/eq3.png" alt="eq3" /></p>

<p>This last re-grouping of terms allows us to see that we can express the full gradient and Hessian in the following more compact way:</p>

<p><img src="/assets/images/smoothmax/eq4.png" alt="eq4" /></p>

<p>With a gradient and Hessian, we now have the tools we need to use smooth-max in algorithms such as gradient descent and convex optimization. Happy computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Concept of Release Versioning]]></title>
    <link href="http://erikerlandson.github.com/blog/2017/08/23/generalizing-the-concept-of-release-versioning/"/>
    <updated>2017-08-23T17:22:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2017/08/23/generalizing-the-concept-of-release-versioning</id>
    <content type="html"><![CDATA[<p>Recently I've been thinking about a few related problems with our current concepts of software release versioning, release dependencies and release building.
These problems apply to software releases in all languages and build systems that I've experienced,
but in the interest of keeping this post as simple as possible I'm going to limit myself to talking about the Maven ecosystem of release management and build tooling.</p>

<p>Consider how we annotate and refer to release builds for a Scala project:
The <em>version</em> of Scala -- 2.10, 2.11, etc -- that was used to build the project is a <em>qualifier</em> for the release.
For example, if I am building a project using Scala 2.11, and package P is one of my project dependencies, then the maven build tooling (or sbt, etc) looks for a version of P that was <em>also</em> built using Scala 2.11;
the build will fail if no such incarnation of P can be located.
This build constraint propagates recursively throughout the entire dependency tree for a project.</p>

<p>Now consider how we treat the version for the package P dependency itself:
Our build tooling forces us to specify one exact release version x.y.z for P.
This is superficially similar to the constraint for building with Scala 2.11, but <em>unlike</em> the Scala constraint, the knowledge about using P x.y.z is not propagated down the tree.</p>

<p>If the dependency for P appears only once in the depenency tree, everything is fine.
However, as anybody who has ever worked with a large dependency tree for a project knows, package P might very well appear in multiple locations of the dep-tree, as a transitive dependency of different packages.
Worse, these deps may be specified as <em>different versions</em> of P, which may be mutually incompatible.</p>

<p>Transitive dep incompatibilities are a particularly thorny problem to solve, but there are other annoyances related to release versioning.
Often a user would like a "major" package dependency built against a particular version of that dep.
For example, packages that use Apache Spark may need to work with a particular build version of Spark (2.1, 2.2, etc).
If I am the package purveyor, I have no very convenient way to build my package against multiple versions of spark, and then annotate those builds in Maven Central.
At best I can bake the spark version into the name.
But what if I want to specify other package dep verions?
Do I create package names with increasingly-long lists of (package,version) pairs hacked into the name?</p>

<p>Finally, there is simply the annoyance of revving my own package purely for the purpose of building it against the latest versions of my dependencies.
None of my code has changed, but I am cutting a new release just to pick up current dependency releases.
And then hoping that my package users will want those particular releases, and that these won't break <em>their</em> builds with incompatible transitive deps!</p>

<p>I have been toying with a release and build methodology for avoiding these headaches. What follows is full of vigorous hand-waving,
but I believe something like it could be formalized in a useful way.</p>

<p>The key idea is that a release <em>build</em> is defined by a <em>build signature</em> which is the union of all <code>(dep, ver)</code> pairs.
This includes:</p>

<ol>
<li>The actual release version of the package code, e.g. <code>(mypackage, 1.2.3)</code></li>
<li>The <code>(dep, ver)</code> for all dependencies (taken over all transitive deps, recursively)</li>
<li>The <code>(tool, ver)</code> for all impactful build tooling, e.g. <code>(scala, 2.11)</code>, <code>(python, 3.5)</code>, etc</li>
</ol>


<p>For example, if I maintain a package <code>P</code>, whose latest code release is <code>1.2.3</code>,
built with dependencies <code>(A, 0.5)</code>, <code>(B, 2.5.1)</code> and <code>(C, 1.7.8)</code>, and dependency <code>B</code> built against <code>(Q, 6.7)</code> and <code>(R, 3.3)</code>,
and <code>C</code> built against <code>(Q, 6.7)</code>
and all compiled with <code>(scala, 2.11)</code>, then the build signature will be:</p>

<p><code>{ (P, 1.2.3), (A, 0.5), (B, 2.5.1), (C, 1.7.8), (Q, 6.7), (R, 3.3), (scala, 2.11) }</code></p>

<p>Identifying a release build in this way makes several interesting things possible.
First, it can identify a build with a transitive dependency problem.
For example, if <code>C</code> had been built against <code>(Q, 7.0)</code>,
then the resulting build signature would have <em>two</em> pairs for <code>Q</code>; <code>(Q, 6.7)</code> and <code>(Q, 7.0)</code>,
which is an immediate red flag for a potential problem.</p>

<p>More intriguingly, it could provide a foundation for <em>avoiding</em> builds with incompatible dependencies.
Suppose that I redefine my build logic so that I only specify dependency package names, and not specific versions.
Whenever I build a project, the build system automatically searches for the most-recent version of each dependency.
This already addresses some of the release headaches above.
As a project builder, I can get the latest versions of packages when I build.
As a package maintainer, I do not have to rev a release just to update my package deps;
projects using my package will get the latest by default.
Moreover, because the latest package release is always pulled, I never get multiple incompatible dependency releases
in a build.</p>

<p>Suppose that for some reason I <em>need</em> a particular release of some dependency.
From the example above, imagine that I must use <code>(Q, 6.7)</code>.
We can imagine augmenting the build specification to allow overriding the default behavior of pulling the most recent release.
We might either specify a specific version as we do currently, or possibly specify a range of releases, as systems like brew or ruby gemfiles allow.
In the case where some constraint is placed on releases, this constraint would be propagaged down the tree (or possibly up from the leaves),
in essentially the same way that the constraint of scala version is already.
In the event that the total set of constraints over the whole dependency tree is not satisfiable, then the build will fail.</p>

<p>With a build annotation system like the one I just described, one could imagine a new role for registries like Maven Central,
where different builds are automatically cached.
The registry could maybe even automatically run CI testing to identify the most-recent versions of package dependencies that satisfy
any given package build,
or perhaps valid dependency release ranges.</p>

<p>To conclude, I believe that re-thinking how we describe the dependencies used to build and annotate package releases,
by generalizing release version to include the release version of all transitive deps (including build tooling as deps),
may enable more flexible ways to both build software releases and specify them for pulling.</p>

<p>Happy Computing!</p>
]]></content>
  </entry>
  
</feed>
