<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: computing | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/computing/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2018-09-11T08:00:55-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Backtracking ULP Incident of 2018]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/09/11/the-backtracking-ulp-incident-of-2018/"/>
    <updated>2018-09-11T07:01:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/09/11/the-backtracking-ulp-incident-of-2018</id>
    <content type="html"><![CDATA[<p>This week I finally started applying my new <a href="https://github.com/erikerlandson/gibbous/">convex optimization</a> library to solve for interpolating splines with <a href="https://github.com/erikerlandson/snowball">monotonic constraints</a>. Things seemed to be going well. My convex optimization was passing unit tests. My monotone splines were passing their unit tests too. I cut an initial release, and announced it to the world.</p>

<p>Because Murphy rules my world, it was barely an hour later that I was playing around with my new toys in a REPL, and when I tried splining an example data set my library call went into an infinite loop:</p>

<p><code>java
// It looks mostly harmless:
double[] x = { 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0 };
double[] y = { 0.0, 0.15, 0.05, 0.3, 0.5, 0.7, 0.95, 0.98, 1.0 };
MonotonicSplineInterpolator interpolator = new MonotonicSplineInterpolator();
PolynomialSplineFunction s = interpolator.interpolate(x, y);
</code></p>

<p>In addition to being a bit embarrassing, it was also a real head-scratcher. There was nothing odd about the data I had just given it. In fact it was a small variation of a problem it had just solved a few seconds prior.</p>

<p>There was nothing to do but put my code back up on blocks and break out the print statements. I ran my problem data set and watched it spin. Fast forward a half hour or so, and I localized the problem to a bit of code that does the <a href="https://en.wikipedia.org/wiki/Backtracking_line_search">"backtracking" phase</a> of a convex optimization:</p>

<p>```java
for (double t = 1.0 ; t >= epsilon ; t *= beta) {</p>

<pre><code>tx = x.add(xDelta.mapMultiply(t));
tv = convexObjective.value(tx);
if (tv == Double.POSITIVE_INFINITY) continue;
if (tv &lt;= v + t*alpha*gdd) {
    foundStep = true;
    break;
}
</code></pre>

<p>}
```</p>

<p>My infinite loop was happening because my backtracking loop above was "succeeding" -- that is, reporting it had found a forward step -- but not actually moving foward along its vector. And the reason turned out to be that my test <code>tv &lt;= v + t*alpha*gdd</code> was succeding because <code>v + t*alpha*gdd</code> was evaluating to just <code>v</code>, and I effectively had <code>tv == v</code>.</p>

<p>I had been bitten by one of the oldest floating-point fallacies: forgetting that <code>x + y</code> can equal <code>x</code> if <code>y</code> gets smaller than the Unit in the Last Place (ULP) of <code>x</code>.</p>

<p>This was an especially evil bug, as it very frequently <em>doesn't</em> manifest. My unit testing in <em>two libraries</em> failed to trigger it. I have since added the offending data set to my splining unit tests, in case the code ever regresses somehow.</p>

<p>Now that I understood my problem, it turns out that I could use this to my advantage, as an effective test for local convergence. If I can't find a step size that reduces my local objective function by an amount measurable to floating point resolution, then I am as good as converged at this stage of the algorithm. I re-wrote my code to reflect this insight, and added some annotations so I don't forget what I learned:</p>

<p>```java
for (double t = 1.0; t > 0.0; t *= beta) {</p>

<pre><code>tx = x.add(xDelta.mapMultiply(t));
tv = convexObjective.value(tx);
if (Double.isInfinite(tv)) {
    // this is barrier convention for "outside the feasible domain",
    // so try a smaller step
    continue;
}
double vtt = v + (t * alpha * gdd);
if (vtt == v) {
    // (t)(alpha)(gdd) is less than ULP(v)
    // Further tests for improvement are going to fail
    break;
}
if (tv &lt;= vtt) {
    // This step resulted in an improvement, so halt with success
    foundStep = true;
    break;
}
</code></pre>

<p>}
```</p>

<p>I tend to pride myself on being aware that floating point numerics are a leaky abstraction, and the various ways these leaks can show up in computations, but pride goeth before a fall, and after all these years I can still burn myself! It never hurts to be reminded that you can never let your guard down with floating point numbers, and unit testing can never <em>guarantee</em> correctness. That goes double for numeric methods!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Equality Constraints for Cubic B-Splines]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/09/08/equality-constraints-for-cubic-b-splines/"/>
    <updated>2018-09-08T14:32:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/09/08/equality-constraints-for-cubic-b-splines</id>
    <content type="html"><![CDATA[<p>In my <a href="http://erikerlandson.github.io/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form/">previous post</a>
I derived the standard-form polynomial coefficients for cubic B-splines.
As part of the <a href="https://github.com/erikerlandson/snowball">same project</a>,
I also need to add a feature that allows the library user to declare equality constraints of the form <nobr>(x,y)</nobr>,
where <nobr>S(x) = y</nobr>. Under the hood, I am invoking a <a href="https://github.com/erikerlandson/gibbous">convex optimization</a> library, and so I need to convert these
user inputs to a linear equation form that is consumable by the optimizer.</p>

<p>I expected this to be tricky, but it turns out I did most of the work <a href="http://erikerlandson.github.io/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form/">already</a>.
I can take one of my previously-derived expressions for S(x) and put it into a form that gives me coefficients for the four contributing knot points <nobr>K<sub>j-3</sub> ... K<sub>j</sub></nobr>:</p>

<p><img src="/assets/images/bspline/ybblhxfw.png" alt="eq" /></p>

<p>Recall that by the convention from my previous post, <nobr>K<sub>j</sub></nobr> is the largest knot point that is <nobr>&lt;= x</nobr>.</p>

<p>My linear constraint equation is with respect to the vector I am solving for, in particular vector (τ), and so the
equation above yields the following:</p>

<p><img src="/assets/images/bspline/y7jhvnmk.png" alt="eq" /></p>

<p>In this form, it is easy to add into a <a href="https://github.com/erikerlandson/gibbous">convex optimization</a> problem as a linear equality constraint.</p>

<p>Gradient constraints are another common equality constraint in convex optimization, and so I can apply very similar logic to get coefficient values corresponding to the gradient of S:</p>

<p><img src="/assets/images/bspline/yd5fxmwk.png" alt="eq" /></p>

<p>And so my linear equality constraint with respect to (τ) in this case is:</p>

<p><img src="/assets/images/bspline/yalk3puu.png" alt="eq" /></p>

<p>And that gives me the tools I need to let my users supply additional equality constraints as simple <nobr>(x,y)</nobr> pairs, and translate them into a form that can be consumed by convex optimization routines. Happy Computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Putting Cubic B-Splines into Standard Polynomial Form]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form/"/>
    <updated>2018-09-02T11:07:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/09/02/putting-cubic-b-splines-into-standard-polynomial-form</id>
    <content type="html"><![CDATA[<p>Lately I have been working on an <a href="https://github.com/erikerlandson/snowball">implementation</a> of monotone smoothing splines, based on <a href="#ref1">[1]</a>. As the title suggests, this technique is based on a univariate cubic <a href="https://en.wikipedia.org/wiki/B-splines">B-spline</a>. The form of the spline function used in the paper is as follows:</p>

<p><img src="/assets/images/bspline/yd2guhxt.png" alt="eq1" /></p>

<p>The knot points <nobr>K<sub>j</sub></nobr> are all equally spaced by 1/α, and so α normalizes knot intervals to 1. The function <nobr>B<sub>3</sub>(t)</nobr> and the four <nobr>N<sub>i</sub>(t)</nobr> are defined in this transformed space, t, of unit-separated knots.</p>

<p>I'm interested in providing an interpolated splines using the Apache Commons Math API, in particular the <a href="https://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/analysis/polynomials/PolynomialSplineFunction.html">PolynomialSplineFunction</a> class. In principle the above is clearly such a polynomial, but there are a few hitches.</p>

<ol>
<li><code>PolynomialSplineFunction</code> wants its knot intervals in closed standard polynomial form <nobr>ax<sup>3</sup> + bx<sup>2</sup> + cx + d</nobr></li>
<li>It wants each such polynomial expressed in the translated space <nobr>(x-K<sub>j</sub>)</nobr>, where <nobr>K<sub>j</sub></nobr> is the greatest knot point that is &lt;= x.</li>
<li>The actual domain of S(x) is <nobr>K<sub>0</sub> ... K<sub>m-1</sub></nobr>. The first 3 "negative" knots are there to make the summation for S(x) cleaner. <code>PolynomialSplineFunction</code> needs its functions to be defined purely on the actual domain.</li>
</ol>


<p>Consider the arguments to <nobr>B<sub>3</sub></nobr>, for two adjacent knots <nobr>K<sub>j-1</sub></nobr> and <nobr>K<sub>j</sub></nobr>, where <nobr>K<sub>j</sub></nobr> is greatest knot point that is &lt;= x. Recalling that knot points are all equally spaced by 1/α, we have the following relationship in the transformed space t:</p>

<p><img src="/assets/images/bspline/ydcb2ao3.png" alt="eq" /></p>

<p>We can apply this same manipulation to show that the arguments to <nobr>B<sub>3</sub></nobr>, as centered around knot <nobr>K<sub>j</sub></nobr>, are simply <nobr>{... t+2, t+1, t, t-1, t-2 ...}</nobr>.</p>

<p>By the definition of <nobr>B<sub>3</sub></nobr> above, you can see that <nobr>B<sub>3</sub>(t)</nobr> is non-zero only for t in <nobr>[0,4)</nobr>, and so the four corresponding knot points <nobr>K<sub>j-3</sub> ... K<sub>j</sub></nobr> contribute to its value:</p>

<p><img src="/assets/images/bspline/y9tpgfqj.png" alt="eq2" /></p>

<p>This suggests a way to manipulate the equations into a standard form. In the transformed space t, the four nonzero terms are:</p>

<p><img src="/assets/images/bspline/ya6gsrjy.png" alt="eq4" /></p>

<p>and by plugging in the appropriate <nobr>N<sub>i</sub></nobr> for each term, we arrive at:</p>

<p><img src="/assets/images/bspline/yc6grwxe.png" alt="eq5" /></p>

<p>Now, <code>PolynomialSplineFunction</code> is going to automatically identify the appropriate <nobr>K<sub>j</sub></nobr> and subtract it, and so I can define <em>that</em> transform as <nobr>u = x -  K<sub>j</sub></nobr>, which gives:</p>

<p><img src="/assets/images/bspline/y9p3vgqt.png" alt="eq6" /></p>

<p>I substitute the argument (αu) into the definitions of the four <nobr>N<sub>i</sub></nobr> to obtain:</p>

<p><img src="/assets/images/bspline/y8apdoqy.png" alt="eq7" /></p>

<p>Lastly, collecting like terms gives me the standard-form coefficients that I need for <code>PolynomialSplineFunction</code>:</p>

<p><img src="/assets/images/bspline/ya74mlsf.png" alt="eq8" /></p>

<p>Now I am equipped to return a <code>PolynomialSplineFunction</code> to my users, which implements the cubic B-spline that I fit to their data. Happy computing!</p>

<h4>References</h4>

<p><a name="anchor1" id="ref1">[1] </a>H. Fujioka and H. Kano: <a href="https://github.com/erikerlandson/snowball/blob/master/monotone-cubic-B-splines-2013.pdf">Monotone smoothing spline curves using normalized uniform cubic B-splines</a>, Trans. Institute of Systems, Control and Information Engineers, Vol. 26, No. 11, pp. 389–397, 2013</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solving Feasible Points With Smooth-Max]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/06/03/solving-feasible-points-with-smooth-max/"/>
    <updated>2018-06-03T14:21:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/06/03/solving-feasible-points-with-smooth-max</id>
    <content type="html"><![CDATA[<h3>Overture</h3>

<p>Lately I have been fooling around with an <a href="https://github.com/erikerlandson/gibbous">implementation</a> of the <a href="#cite1">Barrier Method</a> for convex optimization with constraints.
One of the characteristics of the Barrier Method is that it requires an initial-guess from inside the
<em>feasible region</em>: that is, a point which is known to satisfy all of the inequality constraints provided
by the user.
For some optimization problems, it is straightforward to find such a point by using knowledge about the problem
domain, but in many situations it is not at all obvious how to identify such a point, or even if a
feasible point exists. The feasible region might be empty!</p>

<p>Boyd and Vandenberghe discuss a couple approaches to finding feasible points in §11.4 of <a href="#cite1">[1]</a>.
These methods require you to set up an "augmented" minimization problem:
<img src="/assets/images/feasible/y9czf8u7.png" alt="eq1" /></p>

<p>As you can see from the above, you have to set up an "augmented" space x+s, where (s) represents an additional
dimension, and constraint functions are augmented to f<sub>k</sub>-s</p>

<h3>The Problem</h3>

<p>I experimented a little with these, and while I am confident they work for most problems having multiple
inequality constraints, my unit testing tripped over an ironic deficiency:
when I attempted to solve a feasible point for a single planar constraint, the numerics went a bit haywire.
Specifically, a linear constraint function happens to have a singular Hessian of all zeroes.
The final Hessian, coming out of the log barrier function, could be consumed by SVD to get a search direction
but the resulting gradients behaved poorly.</p>

<p>Part of the problem seems to be that the nature of this augmented minimization problem forces the algorithms
to push (s) ever downward, but letting (s) transitively push the f<sub>k</sub> with the augmented constraint
functions f<sub>k</sub>-s. When only a single linear constraint function is in play, the resulting gradient
caused augmented dimension (s) to converge <em>against</em> the movement of the remaining (unaugmented) sub-space.
The minimization did not converge to a feasible point, even though literally half of the space on one side
of the planar surface is feasible!</p>

<h3>Smooth Max</h3>

<p>Thinking about these issues made me wonder if a more direct approach was possible.
Another way to think about this problem is to minimize the maximum f<sub>k</sub>;
If the maximum f<sub>k</sub> is &lt; 0 at a point x, then x is a feasible point satisfying all f<sub>k</sub>.
If the smallest-possible maximum f<sub>k</sub> is > 0, then we have definitive proof that no
feasible point exists, and our constraints can't be satisfied.</p>

<p>Taking a maximum preserves convexity, which is a good start, but maximum isn't differentiable everywhere.
The boundaries between regions where different functions are the maximum are not smooth, and along
those boundaries there is no gradient, and therefore no Hessian either.</p>

<p>However, there is a variation on this idea, known as smooth-max, defined like so:</p>

<p><img src="/assets/images/feasible/y8cgykuc.png" alt="eq2" /></p>

<p>Smooth-max has a well defined <a href="http://erikerlandson.github.io/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">gradient and Hessian</a>, and furthermore can be computed in a <a href="http://erikerlandson.github.io/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">numerically stable</a> way.
The sum inside the logarithm above is a sum of exponentials of convex functions.
This is good news; exponentials of convex functions are log-convex, and a sum of log-convex functions is also
log-convex.</p>

<p>That means I have the necessary tools to set up the my mini-max problem:
For a given set of convex constraint functions f<sub>k</sub>, I create a functions which is the soft-max of
these, and I minimize it.</p>

<h3>Go Directly to Jail</h3>

<p>I set about implementing my smooth-max idea, and immediately ran into almost the same problem as before.
If I try to solve for a single planar constraint, my Hessian degenerates to all-zeros!
When I unpacked the smoothmax-formula for a single constraint f<sub>k</sub>, it indeed is just f<sub>k</sub>,
zero Hessian and all!</p>

<h3>More is More</h3>

<p>What to do?
Well you know what form of constraint <em>always</em> has a well behaved Hessian? A circle, that's what.
More technically, an n-dimensional ball, or n-ball.
What if I add a new constraint of the form:</p>

<p><img src="/assets/images/feasible/yd8xg64k.png" alt="eq3" /></p>

<p>This constraint equation is quadratic, and its Hessian is I<sub>n</sub>.
If I include this in my set of constraints, my smooth-max Hessian will be non-singular!</p>

<p>Since I do not know a priori where my feasible point might lie, I start with my n-ball centered at
my initial guess, and minimize. The result might look something like this:</p>

<p><img src="/assets/images/feasible/fig1.png" alt="fig1" /></p>

<p>Because the optimization is minimizing the maximum f<sub>k</sub>, the optimal point may not be feasible,
but if not it <em>will</em> end up closer to the feasible region than before.
This suggests an iterative algorithm, where I update the location of the n-ball at each iteration,
until the resulting optimized point lies on the intersection of my original constraints and my
additional n-ball constraint:</p>

<p><img src="/assets/images/feasible/fig2.png" alt="fig2" /></p>

<h3>Caught in the Underflow</h3>

<p>I implemented the iterative algorithm above (you can see what this loop looks like <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L134">here</a>),
and it worked exactly as I hoped...
at least on my initial tests.
However, eventually I started playing with its convergence behavior by moving my constraint region farther
from the initial guess, to see how it would cope.
Suddenly the algorithm began failing again.
When I drilled down on why, I was taken aback to discover that my Hessian matrix was once again showing
up as all zeros!</p>

<p>The reason was interesting.
Recall that I used a <a href="http://erikerlandson.github.io/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">modified formula</a> to stabilize my smooth-max computations.
In particular, the "stabilized" formula for the Hessian looks like this:</p>

<p><img src="/assets/images/smoothmax/eq3b.png" alt="eq4" /></p>

<p>So, what was going on?
As I started moving my feasible region farther away, the corresponding constraint function started to
dominate the exponential terms in the equation above.
In other words, the distance to the feasible region became the (z) in these equations, and
this z value was large enough to drive the terms corresponding to my n-ball constraint to zero!</p>

<p>However, I have a lever to mitigate this problem.
If I make the α parameter <em>small</em> enough, it will compress these exponent ranges and prevent my
n-ball Hessian terms from washing out.
Decreasing α makes smooth-max more rounded-out, and decreases the sharpness of the approximation to the true max,
but minimizing smooth-max still yields the same minimum <em>location</em> as true maximum, and so playing this
trick does not undermine my results.</p>

<p>How small is small enough?
α is essentially a free parameter, but I found that if I set it at each iteration,
such that I make sure that my n-ball Hessian coefficient never drops below 1e-3 (but may be larger),
then my Hessian is always well behaved.
Note that as my iterations grow closer to the true feasible region, I can gradually allow α to
grow larger.
Currently, I don't increase α larger than 1, to avoid creating curvatures too large, but I have not
experimented deeply with what actually happens if it were allowed to grow larger.
You can see what this looks like in my current implementation <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L153">here</a>.</p>

<h3>Convergence</h3>

<p>Tuning the smooth-max α parameter gave me numeric stability, but I noticed that as the feasible region
grew more distant from my initial guess, the algorithm's time to converge grew larger fairly quickly.
When I studied its behavior, I saw that at large distances, the quadratic "cost" of my n-ball constraint
effectively pulled the optimal point fairly close to my n-ball center.
This doesn't prevent the algorithm from finding a solution, but it does prevent it from going long distances
very fast.
To solve this adaptively, I added a scaling factor s to my n-ball constraint function.
The scaled version of the function looks like:</p>

<p><img src="/assets/images/feasible/y9gndl2f.png" alt="eq5" /></p>

<p>In my case, when my distances to a feasible region grow large, I want s to become small, so that it
causes the cost of the n-ball constraint to grow more slowly, and allow the optimization to move
farther, faster.
The following diagram illustrates this intuition:</p>

<p><img src="/assets/images/feasible/fig3.png" alt="fig3" /></p>

<p>In my algorithm, I set s = 1/σ, where σ represents the
"scale" of the current distance to feasible region.
The n-ball function grows as the square of the distance to the ball center; therefore I
set σ=(k)sqrt(s), so that it grows proportionally to the square root of the current largest user constraint
cost.
Here, (k) is a proportionality constant.
It too is a somewhat magic free parameter, but I have found that k=1.5 yields fast convergences and
good results.
One last trick I play is that I prevent σ from becoming less than a minimum value, currently 10.
This ensures that my n-ball constraint never dominates the total constraint sum, even as the
optimization converges close to the feasible region.
I want my "true" user constraints to dominate the behavior near the optimum, since those are the
constraints that matter.
The code is shorter than the explaination: you can see it <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L143">here</a></p>

<h3>Conclusion</h3>

<p>After applying all these intuitions, the resulting algorithm appears to be numerically stable and also
converges pretty quickly even when the initial guess is very far from the true feasible region.
To review, you can look at the main loop of this algorithm starting <a href="https://github.com/erikerlandson/gibbous/blob/blog/feasible-points/src/main/java/com/manyangled/gibbous/optim/convex/ConvexOptimizer.java#L128">here</a>.</p>

<p>I've learned a lot about convex optimization and feasible point solving from working through practical
problems as I made mistakes and fixed them.
I'm fairly new to the whole arena of convex optimization, and I expect I'll learn a lot more as I go.
Happy Computing!</p>

<h3>References</h3>

<p><a name="cite1"</a>
[1] §11.3 of <em>Convex Optimization</em>, Boyd and Vandenberghe, Cambridge University Press, 2008</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing Smooth Max and its Gradients Without Over- and Underflow]]></title>
    <link href="http://erikerlandson.github.com/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/"/>
    <updated>2018-05-28T08:13:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow</id>
    <content type="html"><![CDATA[<p>In my <a href="http://erikerlandson.github.io/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">previous post</a> I derived the gradient and Hessian for the smooth max function.
The <a href="https://www.johndcook.com/blog/">Notorious JDC</a> wrote a helpful companion post that describes <a href="https://www.johndcook.com/blog/2010/01/20/how-to-compute-the-soft-maximum/">computational issues</a> of overflow and underflow with smooth max;
values of f<sub>k</sub> don't have to grow very large (or small) before floating point limitations start to force their exponentials to +inf or zero.
In JDC's post he discusses this topic in terms of a two-valued smooth max.
However it isn't hard to generalize the idea to a collection of f<sub>k</sub>.
Start by taking the maximum value over our collection of functions, which I'll define as (z):</p>

<p><img src="/assets/images/smoothmax/eq1b.png" alt="eq1" /></p>

<p>As JDC described in his post, this alternative expression for smooth max (m) is computationally stable.
Individual exponential terms may underflow to zero, but they are the ones which are dominated by the other terms, and so approximating them by zero is numerically accurate.
In the limit where one value dominates all others, it will be exactly the value given by (z).</p>

<p>It turns out that we can play a similar trick with computing the gradient:</p>

<p><img src="/assets/images/smoothmax/eq2b.png" alt="eq2" /></p>

<p>Without showing the derivation, we can apply exactly the same manipulation to the terms of the Hessian:</p>

<p><img src="/assets/images/smoothmax/eq3b.png" alt="eq3" /></p>

<p>And so we now have a computationally stable form of the equations for smooth max, its gradient and its Hessian. Enjoy!</p>
]]></content>
  </entry>
  
</feed>
