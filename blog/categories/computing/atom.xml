<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: computing | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/computing/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2012-11-01T08:35:06-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Accounting Groups With Wallaby]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/01/using-accounting-groups-with-wallaby/"/>
    <updated>2012-11-01T07:41:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/01/using-accounting-groups-with-wallaby</id>
    <content type="html"><![CDATA[<p>Here is an introduction to using HTCondor accounting groups with Wallaby.</p>

<p>I will begin by walking through an accounting group configuration on a pool managed by wallaby.  Following, I will demonstrate the configuration in action.</p>

<p>First, it is good practice to snapshot current wallaby configuration for reference:</p>

<pre><code>$ wallaby make-snapshot "pre demo state"
</code></pre>

<p>When demonstrating basic accounting group behavior, it is often cleanest to work with static slots.  Removing any dynamic/partitionable slot configuration is recommended:</p>

<pre><code># e.g. if slot configuration resides on the default group:
$ wallaby remove-features-from-group +++DEFAULT DynamicSlots
$ wallaby add-features-to-group +++DEFAULT ExecuteNode
$ wallaby activate
</code></pre>

<p>We will be constructing a wallaby feature called <code>AcctGroupDemo</code> to hold our accounting group configurations.  This clears out the feature:</p>

<pre><code>$ wallaby remove-feature AcctGroupDemo
$ wallaby add-feature AcctGroupDemo
</code></pre>

<p>Wallaby wants to know about features that are used in configurations, so begin by declaring them to the wallaby store:</p>

<pre><code>$ wallaby add-param GROUP_QUOTA_Demo
$ wallaby add-param GROUP_QUOTA_DYNAMIC_Demo.A
$ wallaby add-param GROUP_QUOTA_DYNAMIC_Demo.B
$ wallaby add-param GROUP_QUOTA_DYNAMIC_Demo.C
$ wallaby add-param GROUP_ACCEPT_SURPLUS_Demo
$ wallaby add-param NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION
</code></pre>

<p>If you alter the configuration parameters, you will want the negotiator to reconfigure itself when you activate.  Here we declare the accounting group features as part of the negotiator subsystem:</p>

<pre><code>$ wallaby add-params-to-subsystem negotiator \
GROUP_QUOTA_Demo \
GROUP_QUOTA_DYNAMIC_Demo.A \
GROUP_QUOTA_DYNAMIC_Demo.B \
GROUP_QUOTA_DYNAMIC_Demo.C \
NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION
</code></pre>

<p>Now we construct the actual configuration as the <code>AcctGroupDemo</code> wallaby feature.  Here we are constructing a group <code>Demo</code> with three subgroups <code>Demo.{A|B|C}</code>.  In a multi-node pool with several cores, it is often easiest to play with group behavior by creating a sub-hierarchy such as this <code>Demo</code> sub-hierarchy, and configuring <code>GROUP_ACCEPT_SURPLUS_Demo=False</code>, so that the sub-hierarchy behaves with a well-defined total slot quota (in this case 15).  The sub-groups A,B and C each take 1/3 of the parent's quota, so in this example each will receive 5 slots.  It is worth noting the use of the 'append' operator <code>&gt;=</code> at the beginning of the value for <code>GROUP_NAMES</code>.  This is good practice for all list-valued config variables, as it allows other features to add onto the list if they are applied:</p>

<pre><code>$ wallaby add-params-to-feature AcctGroupDemo \
NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION=False \
GROUP_NAMES='&gt;= Demo, Demo.A, Demo.B, Demo.C' \
GROUP_ACCEPT_SURPLUS=True \
GROUP_QUOTA_Demo=15 \
GROUP_ACCEPT_SURPLUS_Demo=False \
GROUP_QUOTA_DYNAMIC_Demo.A=0.333 \
GROUP_QUOTA_DYNAMIC_Demo.B=0.333 \
GROUP_QUOTA_DYNAMIC_Demo.C=0.333
</code></pre>

<p>With our accounting group feature created, we can apply it to the machine our negotiator daemon is running on.  Then snapshot our configuration modifications for reference, and activate the new configuration:</p>

<pre><code>$ wallaby add-features-to-node negotiator.node.com AcctGroupDemo
$ wallaby make-snapshot 'new acct group config'
$ wallaby activate
</code></pre>

<p>Now we will demonstrate the new feature in action.  Submit the following file to your pool, which submits 100 jobs each to groups Demo.A, Demo.B and Demo.C, with durations randomly chosen between 25 and 35 seconds:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = $$([25 + random(11)])
transfer_executable = false
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="Demo.A.user1"
queue 100
+AccountingGroup="Demo.B.user1"
queue 100
+AccountingGroup="Demo.C.user1"
queue 100
</code></pre>

<p>Once you make this submission, allow the jobs to negotiate, and you can check to see what accounting groups are running on slots by inspecting the value of RemoteNegotiatingGroup on slot ads.   You should see that subgroups Demo.A, Demo.B and Demo.C are each running 5 jobs.  Note, due to jobs completing between negotiation cycles, these numbers can be &lt;= 5 at certain times (if you have any other slots in the pool, they will show up in the output below as having either 'undefined' negotiating group or possibly some other value if other accounting groups are being used on the pool).</p>

<pre><code>$ condor_status -format "%s\n" 'ifThenElse(RemoteNegotiatingGroup isnt undefined, string(RemoteNegotiatingGroup), "undefined")' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
  5 Demo.A
  5 Demo.B
  5 Demo.C
100 undefined
115 total
</code></pre>

<p>With this accounting group configuration in place, you can play with changing quotas for the accounting groups and observe the numbers of running jobs change in response.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Randomized Sleep Jobs in HTCondor Using Delayed Evaluation]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/10/31/randomized-sleep-jobs-in-htcondor-using-delayed-evaluation/"/>
    <updated>2012-10-31T14:17:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/10/31/randomized-sleep-jobs-in-htcondor-using-delayed-evaluation</id>
    <content type="html"><![CDATA[<p>In some cases, when testing or demonstrating the performance of an HTCondor pool, it is useful to submit a plug of jobs with randomized running times.  The standard technique for controlling run times is to submit a classic 'sleep' job.  However, randomizing the argument to sleep is another matter.  Luckily there is an easy way to do this with a single submit file, using delayed evaluation syntax.</p>

<p>A classad expression placed inside of a special enclosure, like this: <code>$$([ &lt;expr&gt; ])</code>, causes <code>&lt;expr&gt;</code> to be evaluated at the time the job ad is matched with a slot.  You can read more about delayed evaluation <a href="http://research.cs.wisc.edu/condor/manual/v7.8/condor_submit.html#78367">here</a>.  Consider the following example submit file:</p>

<pre><code>universe = vanilla
executable = /bin/sleep

# generate a random sleep duration when job is matched
args = $$([25 + random(11)])

# boilerplate to avoid file transfers and notifications
transfer_executable = false
should_transfer_files = no
when_to_transfer_output = on_exit
notification = never

# generate 100 copies of this job - each will evaluate the
# randomizing expression independently
queue 100
</code></pre>

<p>As you can see in the example above, the value of <code>args</code> is set to the delayed evaluation expression <code>$$([25 + random(11)])</code>, which will evaluate the classad expression <code>25 + random(11)</code> when each job ad matches a slot to run.  The <code>queue 100</code> command generates 100 separate job ads, and so the net effect is 100 jobs, which will each run a sleep job with a duration <em>randomly chosen</em> between 25 and 35.</p>

<p>If we submit this file to a condor pool, and let the jobs run to completion, we can check the pool history file to see how the <code>Args</code> attribute was set on the job ad using the special generative attribute <code>MATCH_EXP_Args</code>, and the <a href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/">cchist tool</a>:</p>

<pre><code>$ cchist condor_history 'MATCH_EXP_Args'
     11 25
      7 26
     10 27
      9 28
      7 29
     13 30
      8 31
      7 32
      8 33
      9 34
     11 35
    100 total
</code></pre>

<p>We can also sanity check our measure of actual run time, to see that those values are close to our values of <code>Args</code>:</p>

<pre><code>$ cchist condor_history 'CompletionDate-JobCurrentStartDate'
      1 25
     11 26
      9 27
      8 28
      9 29
      9 30
     12 31
      4 32
      8 33
     10 34
     12 35
      6 36
      1 37
    100 total
</code></pre>

<p>Have fun with easy random sleep jobs!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hosting a Blog Feed Aggregator With Octopress]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/10/05/hosting-a-blog-feed-aggregator-with-octopress/"/>
    <updated>2012-10-05T12:52:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/10/05/hosting-a-blog-feed-aggregator-with-octopress</id>
    <content type="html"><![CDATA[<p>I have written an Octopress plugin to allow turnkey support for hosting a blog feed aggregator, in Octopress idiomatic style.  I will describe the steps to install it and use it below.  Some of its current features are:</p>

<ul>
<li>Easy configuration and deployment, providing all feed aggregator parameters as yaml front-matter</li>
<li>Turn-key generation of feed aggregator pages, in the configured site style</li>
<li>Optional generation of a 'meta-feed' in atom.xml format, from aggregated feed entries</li>
<li>Automatic removal of duplicate feed list urls, and automatic removal of duplicate posts (e.g. if multiple category feeds from the same author are listed)</li>
<li>Automatic generation of feed author list as an Octopress 'aside'</li>
<li>Inclusion/exclusion of posts based on number of posts and/or post age</li>
<li>Display of full or summary content based on number of posts and/or post age</li>
</ul>


<h3>Install the feed_aggregator.rb plugin</h3>

<p>Currently, you can obtain a copy of "feed_aggregator.rb" here:</p>

<p><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/plugins/feed_aggregator.rb">feed_aggregator.rb</a></p>

<p>Simply copy this file into the plugins directory for your octopress repo:</p>

<pre><code>$ cp feed_aggregator.rb /path/to/your/octopress/repo/plugins
</code></pre>

<h3>Install feed aggregator layout files</h3>

<p>You can obtain a copy of the layout files here:</p>

<ul>
<li><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/.themes/classic/source/_layouts/feed_aggregator.html">feed_aggregator.html</a></li>
<li><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/.themes/classic/source/_layouts/feed_aggregator_page.html">feed_aggregator_page.html</a></li>
<li><a href="https://github.com/erikerlandson/octopress/blob/feed_aggregator/.themes/classic/source/_layouts/feed_aggregator_meta.xml">feed_aggregator_meta.xml</a></li>
</ul>


<p>Copy the layouts files to your '_layouts' directory:</p>

<pre><code>$ cp feed_aggregator.html /path/to/your/octopress/repo/source/_layouts
$ cp feed_aggregator_page.html /path/to/your/octopress/repo/source/_layouts
$ cp feed_aggregator_meta.xml /path/to/your/octopress/repo/source/_layouts
</code></pre>

<h3>Add feedzirra dependency to the Octopress Gemfile</h3>

<p>Octopress wants its dependencies bundled, so you will want to add this dependency to /path/to/your/octopress/repo/Gemfile:</p>

<pre><code>gem 'feedzirra', '~&gt; 0.1.3'
</code></pre>

<p>Then update the bundles:</p>

<pre><code>$ bundle update
</code></pre>

<h3>Create a page for your feed aggregator</h3>

<p>Here is an example feed aggregator:</p>

<pre><code>---
# use the 'feed_aggregator' layout to generate a feed aggregator page
layout: feed_aggregator

# Title to display for the feed
title: My Blog Feed Aggregator

# maximum number of entries from each feed url to display (defaults to 5)
# use '0' for 'no limit'
post_limit: 5

# limit on total posts for feed (defaults to 100)
# use 0 for 'no limit'
post_total_limit: 50

# maximum post age to include: &lt;N&gt; { seconds | minutes | hours | days | weeks | months | years }
# abbreviations and plurals are supported, e.g.  w, week, weeks
# defaults to '1 year'
# use '0 &lt;any-unit&gt;' for 'no limit'
post_age_limit: 6 months

# only render full content for the first &lt;N&gt; posts 
# (default is 'full content for all posts')
# use a limit of 0 to use all summaries
full_post_limit: 10

# use summaries for all posts older than this 
# (default is 'no maximum age')
# works like post_age_limit
full_post_age_limit: 1 month

# generate a 'meta-feed' atom file, with the given name 'atom.xml' (meta feeds are optional)
# (with no directory, generates in same directory as the feed aggregator page)
meta_feed: atom.xml

# list all urls to aggregate here
# You can either specify a single feed url, or explicitly specify 'url', 'author' 
# and/or 'author_url' params for the feed aggregator to use.
# feed_aggregator does its best to supply these values automatically otherwise.
feed_list:
  - http://blog_site_1.com/atom.xml
  - http://blog_site_2.com/atom.xml
  - url: http://www.john_doe.com/feed/feed.rss
    author: John Doe
    author_url: http://www.john_doe.com
---
</code></pre>

<p>As you can see, you only need to supply some yaml front-matter.  Page formatting/rendering is performed automatically from the information in the header.  You must use <code>layout: feed_aggregator</code>, and include the standard <code>title</code> to use for the aggregator title, and the <code>feed_list</code> to supply the individual feeds to aggregate.  Other parameters have default values and behaviors, which are described above.  Various <code>meta_feed</code> path behaviors are described in their own section below.</p>

<p>Once you've created the page, you can publish as usual:</p>

<pre><code>$ rake generate
$ rake deploy
</code></pre>

<p>If you want to update your feed automatically, you can set up a cron job:</p>

<pre><code>cd /path/to/octopress/repo
rake generate
rake deploy
</code></pre>

<h3>Screen Shot</h3>

<p>Here is a screen shot of a feed aggregator.  It respects whatever style theme is configured for the site.  The aggregator title is at the top, and a list of contributing authors is automatically generated as an 'aside'.  Each author name links to the parent blog of the author's feed.  In addition to the standard date, the author's name is also included.  Post titles link back to the original post url.</p>

<p><img src="/assets/feed_aggregator/screen1.png" alt="Aggregator Screen Shot" /></p>

<h3>Meta feed generation</h3>

<p>You may optionally request that a meta feed, created from the aggregated posts, be generated.  The meta feed is created in atom format.  Following are some examples of specifying meta feed files</p>

<pre><code># Generate a meta feed called 'atom.xml' in the same directory as the feed aggregator page
# e.g. if the url for the feed aggregator page is  http://blog.site.com/aggregator/index.html, 
# then the path to the meta-feed will be: http://blog.site.com/aggregator/atom.xml
meta_feed: atom.xml

# Generate a meta feed called 'wilma.xml' in subdirectory 'flintstones' of the website.
# the url for this file will be:   http://blog.site.com/flintstones/wilma.xml
meta_feed: /flintstones/wilma.xml

# url for this will be http://blog.site.com/metafeed.xml
meta_feed: /metafeed.xml

# Supplying no file name is equivalent to 'meta_feed: atom.xml'
meta_feed:
</code></pre>

<h3>To Do</h3>

<ul>
<li>It might be nice to support the display of an avatar/icon for authors</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improved Parse Checking for ClassAd Log Files in Condor]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/09/26/improved-parse-checking-for-classad-log-files-in-condor/"/>
    <updated>2012-09-26T10:06:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/09/26/improved-parse-checking-for-classad-log-files-in-condor</id>
    <content type="html"><![CDATA[<p>Condor maintains certain key transactional information using the ClassAd Log system.  For example, both the negotiator's accountant log ("Accountantnew.log") and the scheduler's job queue log ("job_queue.log") are maintained in ClassAd Log format.</p>

<p>As of <a href="http://www.redhat.com/products/mrg/grid/">Red Hat Grid 2.2</a> (upstream: <a href="http://research.cs.wisc.edu/condor/">condor 7.9.0</a>), the ClassAd Log system provides significantly improved parse checking.  This upgraded format checking allows a much wider variety of log corruptions to be detected, and also provides detailed information on the location of corruptions encountered.</p>

<h3>ClassAd Log Format</h3>

<p>A bit of familiarity with ClassAd Log format will aid in understanding subsequent discussion.  The ClassAd Log system serializes a ClassAd collection history as a sequence of tuples:  <code>opcode, [key, [args]]</code>.  For example, here is an annotated ClassAd log excerpt (NOTE: annotations or comments are illegal in an actual file):</p>

<pre><code>105                               &lt;- open a transaction
103 1.0 LastSuspensionTime 0      &lt;- for classad '1.0', set LastSuspentionTime to 0
103 1.0 CurrentHosts 1            &lt;- for classad '1.0', set CurrentHosts to 1
106                               &lt;- close the transaction
</code></pre>

<p>ClassAd Log parse checking works by detecting any occurrence of an invalid op-code, or any invalid ClassAd expression in the RHS of an attribute update operation (opcode 103, as in the example above)</p>

<h3>Examples of Parse Failure Detection</h3>

<p>Consider a ClassAd Log with a corrupted op-code 'ZMG' (in this case, not even a proper integer):</p>

<pre><code>107 1 CreationTimestamp 1334245749
101 0.0 Job Machine
103 0.0 NextClusterNum 1
105
ZMG 1.0 JobStatus 2                        &lt;- Oh no, a bad opcode!
103 1.0 EnteredCurrentStatus 1334245771
103 1.0 LastSuspensionTime 0
103 1.0 CurrentHosts 1
106
105
103 1.1 LastJobStatus 1
103 1.1 JobStatus 2
</code></pre>

<p>Parse checking will result in the following log message in the scheduler, which provides its assessment of what operation line/tuple it found the corruption, and the following 3 lines for additional context:</p>

<pre><code>09/12/12 15:30:35 WARNING: Encountered corrupt log record 5 (byte offset 89)
09/12/12 15:30:35 Lines following corrupt log record 5 (up to 3):
09/12/12 15:30:35     103 1.0 EnteredCurrentStatus 1334245771
09/12/12 15:30:35     103 1.0 LastSuspensionTime 0
09/12/12 15:30:35     103 1.0 CurrentHosts 1
09/12/12 15:30:35 ERROR "Error: corrupt log record 5 (byte offset 89) occurred inside closed transaction, recovery failed" at line 1136 in file /home/eje/git/grid/src/condor_utils/classad_log.cpp
</code></pre>

<p>Note that here the scheduler halted with an exception, as strict parsing was enabled, and the error was inside a completed transaction.</p>

<p>Here is a second example that contains a badly-formed ClassAd expression:</p>

<pre><code>107 1 CreationTimestamp 1334245749
101 0.0 Job Machine
103 0.0 NextClusterNum 1
105
103 1.0 JobStatus 2
103 1.0 EnteredCurrentStatus 1334245749
103 1.0 LastSuspensionTime 0
103 1.0 CurrentHosts 1
106
105
103 1.1 LastJobStatus 1 + eek!             &lt;- bad ClassAd expr!
103 1.1 JobStatus 2
</code></pre>

<p>Note that parse errors detected in unterminated transactions (the last transaction in a file may be uncompleted) are considered non-fatal:</p>

<pre><code>09/12/12 15:43:29 WARNING: Encountered corrupt log record 11 (byte offset 211)
09/12/12 15:43:29 Lines following corrupt log record 11 (up to 3):
09/12/12 15:43:29     103 1.1 JobStatus 2
09/12/12 15:43:29 Detected unterminated log entry in ClassAd Log /home/eje/condor/local/spool/job_queue.log. Forcing rotation.
</code></pre>

<h3>Disabling Strict Parse Checking</h3>

<p>Strict parse checking means that detected errors are fatal (unless in an unterminated transaction).  One consequence of the former lax error checking for Classad Log files is that some log file output was generated that was not properly formed.  Most such instances have been identified and corrected.  However, in order to accomodate legacy ClassAd Log files and any hidden bugs in log output generation, a condor configuration variable has been provided to disable strict checking:</p>

<pre><code># Disable strict parsing: parse errors will not be fatal
CLASSAD_LOG_STRICT_PARSING = False
</code></pre>

<p>In Red Hat Grid 2.2, <code>CLASSAD_LOG_STRICT_PARSING</code> defaults to <code>False</code>.  In the upstream condor repository, the default value has been set to <code>True</code>, in order to allow strict parsing failures to capture any remaining infrequent bugs in ClassAd log generation.</p>

<p>Note that strict checking can also be disabled or enabled <em>selectively</em>.  For example, this configuration disables strict checking only on the negotiator:</p>

<pre><code>CLASSAD_LOG_STRICT_PARSING = True
NEGOTIATOR.CLASSAD_LOG_STRICT_PARSING = False
</code></pre>

<h3>Categories of Undetectable Corruption</h3>

<p>In the ClassAd Log format, the key is considered an arbitrary string.  Therefore, any corruption that alters a key value is not detectable:</p>

<pre><code>103 1.rats! LastSuspensionTime 0   &lt;- weird key '1.rats!' will go undetected
</code></pre>

<p>Similarly, ClassAd attribute names are by nature arbitrary, and so corruptions to a name can go undetected:</p>

<pre><code>103 1.0 LastOopsie 0   &lt;- LastOopsie is a valid attribute name
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Driving a Condor Job Renice Policy with Accounting Groups]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/27/driving-a-condor-job-renice-policy-with-accounting-groups/"/>
    <updated>2012-07-27T13:50:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/27/driving-a-condor-job-renice-policy-with-accounting-groups</id>
    <content type="html"><![CDATA[<p>Condor can run its jobs with a renice priority level specified by <code>JOB_RENICE_INCREMENT</code>, which defaults simply to 10, but can in fact be any ClassAd expression, and is evaluated in the context of the job ad corresponding to the job being run.</p>

<p>This opens up an opportunity to create a renice <em>policy</em>, driven by accounting groups.  Consider a <a href="http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool/">scenario I discussed previously</a>, where a condor pool caters to mission critical (MC) jobs and regular (R) jobs.</p>

<p>An additional configuration trick we could apply is to add a renice policy that gives a higher renice value (that is, a lower priority) to any jobs that aren't run under the mission-critical (MC) rubric, as in this example configuration:</p>

<pre><code># A convenience expression that extracts group, e.g. "mc.user@domain.com" --&gt; "mc"
SUBMIT_EXPRS = AcctGroupName
AcctGroupName = ifThenElse(my.AccountingGroup =!= undefined, \
                           regexps("^([^@]+)\.[^.]+", my.AccountingGroup, "\1"), "&lt;none&gt;")

NUM_CPUS = 3

# Groups representing mission critical and regular jobs:
GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 2
GROUP_QUOTA_R = 1

# Any group not MC gets a renice increment of 10:
JOB_RENICE_INCREMENT = 10 * (AcctGroupName =!= "MC")
</code></pre>

<p>To demonstrate this policy in action, I wrote a little shell script I called <code>burn</code>, whose only function is to burn cycles for a given number of seconds:</p>

<pre><code>#!/bin/sh

# usage: burn [n]
# where n is number of seconds to burn cycles
s="$1"
if [ -z "$s" ]; then s=60; fi

t0=`date +%s`
while [ 1 ]; do
    x=0
    # burn some cycles:
    while [ $x -lt 10000 ]; do let x=$x+1; done
    t=`date +%s`
    let e=$t-$t0
    # halt when the requested time is up:
    if [ $e -gt $s ]; then exit ; fi
done
</code></pre>

<p>Begin by standing up a condor pool including the configuration above.   Make sure the <code>burn</code> script is readable.  Also, it is preferable to make sure your system is unloaded (load average should be as close to zero as reasonably possible).  Then submit the following, which instantiates two <code>burn</code> jobs running under accounting group <code>MC</code> and a third under group <code>R</code>:</p>

<pre><code>universe = vanilla
cmd = /path/to/burn
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup = "MC.user"
queue 2
+AccountingGroup = "R.user"
queue 1
</code></pre>

<p>Allow the jobs to negotiate and then run for a couple minutes.  You should then see something similar to the following load-average information from the slot ads:</p>

<pre><code>$ condor_status -format "%s" SlotID -format " | %.2f" LoadAvg -format " | %.2f" CondorLoadAvg -format " | %.2f" TotalLoadAvg -format " | %.2f" TotalCondorLoadAvg -format " | %s\n" AccountingGroup | sort
1 | 1.33 | 1.33 | 2.75 | 2.70 | MC.user@localdomain
2 | 1.28 | 1.24 | 2.75 | 2.70 | MC.user@localdomain
3 | 0.13 | 0.13 | 2.77 | 2.72 | R.user@localdomain
</code></pre>

<p>Note, which particular <code>SlotID</code> runs which job may vary.  However, we expect to see that the load averages for the slot running group <code>R</code> are much lower than the load averages for slots running jobs under group <code>MC</code>, as seen above.</p>

<p>We can explicitly verify the renice numbers from our policy to see that our one <code>R</code> job has a nice value of 10 (and is using only a fraction of the cpu):</p>

<pre><code># tell 'ps' to give us (pid, %cpu, nice, cmd+args):
$ ps -eo "%p %C %n %a" | grep 'burn 600'
22403 10.2  10 /bin/sh /home/eje/bin/burn 600
22406 93.2   0 /bin/sh /home/eje/bin/burn 600
22411 90.6   0 /bin/sh /home/eje/bin/burn 600
</code></pre>
]]></content>
  </entry>
  
</feed>
