<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: random sampling | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/random-sampling/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2014-09-11T15:06:48-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
    <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
    <updated>2014-09-11T07:57:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
    <content type="html"><![CDATA[<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode:</p>

<pre><code>sample(data: array, p: real) {
    n = length(data)
    m = floor(p * n)
    for j = 0 to m-1 {
        k = random(j, n-1)
        swap(data[j], data[k])
    }
    emit the first m elements of 'data' to output
}
</code></pre>

<p>We can see that this sampling algorithm is indeed O(np).  However, it makes some nontrivial assumptions about its input data:</p>

<ul>
<li>It is random access</li>
<li>It is writable</li>
<li>Its size is known</li>
<li>It can be destructively modified</li>
</ul>


<p>These assumptions can be violated in several ways.  The input data might not support random access, for example it might be a list, or stream, or an iterator over the same.  We might not know its size a priori.  It might be read-only.  It might be up-cast to some superclass where knowledge about these assumed properties is no longer available.</p>

<p>In cases such as this, there is another common sampling algorithm:</p>

<pre><code>sample(data: sequence, p: real) {
    while not end(data) {
        v = next(data)
        if p &lt; random(0.0, 1.0) then emit v to output
    }
}
</code></pre>

<p>The above algorithm enjoys all the advantage in flexibility.  It requires only linear access, does not require writable input, and makes no assumptions about input size.  However it comes at a price: this algorithm is no longer O(np), it is O(n).  Each element must be traversed directly, and worse yet the random number generagor (RNG) must be invoked on each element.  O(n) invocation of the RNG is a substantial cost -- random number generation is typically very expensive compared to the cost of iterating to the next element in a sequence.</p>

<p>But... does linear sampling truly require us to invoke our RNG on every element?   Consider the pattern of data access, divorced from code.   It looks like a sequence of choices: for each element we either (skip) or (sample):</p>

<pre><code>(skip) (skip) (sample) (skip) (sample) (skip) (sample) (sample) (skip) (skip) (sample) ...
</code></pre>

<p>The number of consecutive (skip) events between each (sample) -- the <em>sampling gap</em> -- can itself be modeled as a random variable.  Each (skip)/(sample) choice is an independent Bernoulli trial, where the probability of (skip) is (1-p).   The PMF of the sampling gap for gap of {0, 1, 2, ...} is therefore the geometric distribution parameterized by (p):  P(k) = p(1-p)<sup>k</sup></p>

<p>This suggests an alternative algorithm for sampling, where we only need to randomly choose sample gaps instead of randomly choosing whether we sample each individual element:</p>

<pre><code>// choose a random sampling gap 'k' from P(k) = p(1-p)^k
// caution: this explodes for p = 0 or p = 1
random_gap(p: real) {
    u = max(random(0.0, 1.0), epsilon)
    return floor(log(u) / log(1-p))
}

sample(data: sequence, p: real) {
    advance(data, random_gap(p))
    while not end(data) {
        emit next(data) to output
        advance(data, random_gap(p))
    }
}
</code></pre>

<p>The above algorithm calls the RNG only once per actual collected sample, and so the cost of RNG calls is O(np).  Note that the algorithm is still O(n), but the cost of the RNG tends to dominate the cost of sequence traversal, and so the resulting efficiency improvement is substantial.  I measured the following performance improvements with gap sampling, compared to traditional linear sequence sampling, on a <a href="https://gist.github.com/erikerlandson/05db1f15c8d623448ff6">Scala prototype testing rig</a>:</p>

<p><head><style>
table, th, td {
border: 1px solid black;
border-collapse: collapse;
}
th, td {
padding: 10px;
}
th {
text-align: center;
}
</style></head></p>

<table>
<tr> <th>Type</th> <th>p</th> <th>linear</th> <th>gap</th> </tr>
<tr> <td>Array</td> <td>0.001</td> <td>2784</td> <td>8</td> </tr>
<tr> <td>Array</td> <td>0.01</td> <td>2817</td> <td>77</td> </tr>
<tr> <td>Array</td> <td>0.1</td> <td>3075</td> <td>686</td> </tr>
<tr> <td>Array</td> <td>0.5</td> <td>4221</td> <td>3756</td> </tr>
<tr> <td>List</td> <td>0.001</td> <td>2662</td> <td>238</td> </tr>
<tr> <td>List</td> <td>0.01</td> <td>2662</td> <td>298</td> </tr>
<tr> <td>List</td> <td>0.1</td> <td>2936</td> <td>301</td> </tr>
<tr> <td>List</td> <td>0.5</td> <td>3995</td> <td>305</td> </tr>
</table>




<br>


<p>In the results above, we see that for the Array data the gap sampling times are linear in (p) -- Arrays are random access, and the operation of skipping sample-gaps is constant time.   Gap sampling on random access data allows marked improvement with small sampling probabities, where there are large sampling gaps that can be skipped in constant time.  The List data timings reflect that the sampling traverses all elements in either case.  The time savings here is due to reduced calls to the RNG.</p>

<p>Note that while the gap sampling numbers for Array are better than linear sampling, they show signs of gaining on the linear sampling times.  I will touch on this again at the end of the post.</p>

<p>The gap-sampling algorithm described above is for sampling <em>without</em> replacement.   However, the same approach can be modified to generate sampling <em>with</em> replacement.</p>

<p>When sampling with replacement, it is useful to consider the <em>replication factor</em> of each element (where a replication factor of zero means the element wasn't sampled).  Pretend for the moment that the actual data size (n) is known.  The sample size (m) = (n)(p).  The probability that each element gets sampled, per trial, is 1/n, with (m) independent trials, and so the replication factor (r) for each element obeys a binomial distribution: Binomial(m, 1/n).  If we substitute (n)(p) for (m), we have Binomial(np, 1/n).  As the (n) grows, the Binomial is <a href="http://en.wikipedia.org/wiki/Binomial_distribution#Poisson_approximation">well approximated by a Poisson distribution</a> Poisson(L), where (L) = (np)(1/n) = (p).  And so for our purposes we may sample from Poisson(p), where P(r) = (p<sup>k</sup> / k!)e<sup>(-p),</sup> for our sampling replication factors.  Note that we have now discarded any dependence on sample size (n), as we desire.</p>

<p>In our gap-sampling context, the sampling gaps are now elements whose replication factor is zero, which occurs with probability P(0) = e<sup>(-p).</sup>  And so our sampling gaps are now drawn from geometric distribution P(k) = (1-q)(q)<sup>k,</sup> where q = e<sup>(-p).</sup>   When we <em>do</em> sample an element, its replication factor is drawn from Poisson(p), however <em>conditioned such that the value is >= 1.</em>  It is straightforward to adapt a <a href="http://en.wikipedia.org/wiki/Poisson_distribution#Generating_Poisson-distributed_random_variables">standard Poisson generator</a>, as shown below.</p>

<p>Given the above, gap sampling with replacement in pseudocode looks like:</p>

<pre><code>// sample 'k' from Poisson(p), conditioned to k &gt;= 1
poisson_ge1(p: real) {
    q = e^(-p)
    // simulate a poisson trial such that k &gt;= 1
    t = q + (1-q)*random(0.0, 1.0)
    k = 1

    // continue standard poisson generation trials
    t = t * random(0.0, 1.0)
    while (t &gt; q) {
        k = k + 1
        t = t * random(0.0, 1.0)
    }
    return k
}

// choose a random sampling gap 'k' from P(k) = p(1-p)^k
// caution: this explodes for p = 0 or p = 1
random_gap(p: real) {
    u = max(random(0.0, 1.0), epsilon)
    return floor(log(u) / -p)
}

sample(data: sequence, p: real) {
    advance(data, random_gap(p))
    while not end(data) {
        rf = poisson_ge1(p)
        v = next(data)
        emit (rf) copies of (v) to output
        advance(data, random_gap(p))
    }
}
</code></pre>

<p>The efficiency improvements I have measured for gap sampling with replacement are shown here:</p>

<table>
<tr> <th>Type</th> <th>p</th> <th>linear</th> <th>gap</th> </tr>
<tr> <td>Array</td> <td>0.001</td> <td>1635</td> <td>12</td> </tr>
<tr> <td>Array</td> <td>0.01</td> <td>1807</td> <td>130</td> </tr>
<tr> <td>Array</td> <td>0.1</td> <td>2087</td> <td>1101</td> </tr>
<tr> <td>Array</td> <td>0.5</td> <td>3258</td> <td>4733</td> </tr>
<tr> <td>List</td> <td>0.001</td> <td>1341</td> <td>233</td> </tr>
<tr> <td>List</td> <td>0.01</td> <td>1400</td> <td>411</td> </tr>
<tr> <td>List</td> <td>0.1</td> <td>1729</td> <td>472</td> </tr>
<tr> <td>List</td> <td>0.5</td> <td>3342</td> <td>361</td> </tr>
</table>




<br>


<p>The table above shows that there is a crossover point in the Array timings where gap sampling costs more time than linear sampling.  Although the random access in Scala Array allows for increasing improvements as sampling probability grows small, there appears to be an increased cost factor that allows for the crossover point.  This is a subject for further investigation, but may have to do with allocation cost of Array iterator data structures in Scala.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pretty Good Random Sampling from Database Queries]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/05/16/pretty-good-random-sampling-from-database-queries/"/>
    <updated>2012-05-16T07:05:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/05/16/pretty-good-random-sampling-from-database-queries</id>
    <content type="html"><![CDATA[<p>Suppose you want to add random sampling to a database query, but your database does not support it.  One known technique is to add a field, say "rk", that contains a random key value in [0,1), index on that field, and add a clause to the query:  <code>("rk" &gt;= x  &amp;&amp;  "rk" &lt; x+p)</code>, where p is your desired random sampling probability and x is randomly chosen from [0,1-p).</p>

<p>This is not bad, but we can see it is not <em>truly</em> randomized, as the sliding window [x,x+p) over the "rk" random key field generates overlap in the samplings.  The larger the value of p, the more significant the overlapping effect will be.</p>

<p>Eliminating this effect absolutely (and maintaining query efficiency) is difficult without direct database support, however we can take steps to significantly reduce it.  Suppose we generated <em>two</em> independently randomized keys "rk0" and "rk1".  We could sample using a slightly more complex clause: <code>(("rk0" &gt;= x0  &amp;&amp; "rk0" &lt; x0+d) || ("rk1" &gt;= x1  &amp;&amp;  "rk1" &lt; x1+d))</code>, where x0 and x1 are randomly selected from [0,1-d).</p>

<p>What value do we use for d to maintain a random sampling factor of p?  As "rk0" and "rk1" are independent random variables, the effective sampling factor p is given by p = d + d - d<sup>2,</sup> where the d<sup>2</sup> accounts for query results present in both the "rk0" and "rk1" subqueries.  Applying the quadratic formula to solve for d gives us: d = 1-sqrt(1-p).</p>

<p>This approach should be useable with any database.  Here is example code I wrote for generating the random sampling portion of a mongodb query in pymongo:</p>

<pre><code>def random_sampling_query(p, rk0="rk0", rk1="rk1", pad = 0):
    d = (1.0 - sqrt(1.0-p)) * (1.0 + pad)
    if d &gt; 1.0: d = 1.0
    if d &lt; 0.0: d = 0.0
    s0 = random.random()*(1.0 - d)
    s1 = random.random()*(1.0 - d)
    return {"$or":[{rk0:{"$gte":s0, "$lt":s0+d}}, {rk1:{"$gte":s1, "$lt":s1+d}}]}
</code></pre>

<p>I included an optional 'pad' parameter to support a case where one might want a particular (integer) sample size s, and so set p = s/(db-table-size), and use padding to mitigate the probability of getting less than s records due to random sampling jitter.  In mongodb one could then append <code>limit(s)</code> to the query return, and get exactly s returns in most cases, with the correct padding.</p>

<p>Here is a pymongo example of using the <code>random_sampling_query()</code> above:</p>

<pre><code># get a query that does random sampling of 1% of the results:
query = random_sampling_query(0.01)
# other query clauses can be added if desired:
query[user] = "eje"
# issue the final query to get results with random sampling:
qres = data.find(query)
</code></pre>

<p>One could extend the logic above by using 3 independent random fields rk0,rk1,rk2 and applying the cubic formula, or four fields and the quartic formula, but I suspect that is passing the point of diminishing returns on storage cost, query cost and algebra.</p>
]]></content>
  </entry>
  
</feed>
