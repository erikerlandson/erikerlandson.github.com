<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Red Hat | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/red-hat/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2018-09-05T06:35:53-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Improved Parse Checking for ClassAd Log Files in Condor]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/09/26/improved-parse-checking-for-classad-log-files-in-condor/"/>
    <updated>2012-09-26T10:06:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/09/26/improved-parse-checking-for-classad-log-files-in-condor</id>
    <content type="html"><![CDATA[<p>Condor maintains certain key transactional information using the ClassAd Log system.  For example, both the negotiator's accountant log ("Accountantnew.log") and the scheduler's job queue log ("job_queue.log") are maintained in ClassAd Log format.</p>

<p>As of <a href="http://www.redhat.com/products/mrg/grid/">Red Hat Grid 2.2</a> (upstream: <a href="http://research.cs.wisc.edu/condor/">condor 7.9.0</a>), the ClassAd Log system provides significantly improved parse checking.  This upgraded format checking allows a much wider variety of log corruptions to be detected, and also provides detailed information on the location of corruptions encountered.</p>

<h3>ClassAd Log Format</h3>

<p>A bit of familiarity with ClassAd Log format will aid in understanding subsequent discussion.  The ClassAd Log system serializes a ClassAd collection history as a sequence of tuples:  <code>opcode, [key, [args]]</code>.  For example, here is an annotated ClassAd log excerpt (NOTE: annotations or comments are illegal in an actual file):</p>

<pre><code>105                               &lt;- open a transaction
103 1.0 LastSuspensionTime 0      &lt;- for classad '1.0', set LastSuspentionTime to 0
103 1.0 CurrentHosts 1            &lt;- for classad '1.0', set CurrentHosts to 1
106                               &lt;- close the transaction
</code></pre>

<p>ClassAd Log parse checking works by detecting any occurrence of an invalid op-code, or any invalid ClassAd expression in the RHS of an attribute update operation (opcode 103, as in the example above)</p>

<h3>Examples of Parse Failure Detection</h3>

<p>Consider a ClassAd Log with a corrupted op-code 'ZMG' (in this case, not even a proper integer):</p>

<pre><code>107 1 CreationTimestamp 1334245749
101 0.0 Job Machine
103 0.0 NextClusterNum 1
105
ZMG 1.0 JobStatus 2                        &lt;- Oh no, a bad opcode!
103 1.0 EnteredCurrentStatus 1334245771
103 1.0 LastSuspensionTime 0
103 1.0 CurrentHosts 1
106
105
103 1.1 LastJobStatus 1
103 1.1 JobStatus 2
</code></pre>

<p>Parse checking will result in the following log message in the scheduler, which provides its assessment of what operation line/tuple it found the corruption, and the following 3 lines for additional context:</p>

<pre><code>09/12/12 15:30:35 WARNING: Encountered corrupt log record 5 (byte offset 89)
09/12/12 15:30:35 Lines following corrupt log record 5 (up to 3):
09/12/12 15:30:35     103 1.0 EnteredCurrentStatus 1334245771
09/12/12 15:30:35     103 1.0 LastSuspensionTime 0
09/12/12 15:30:35     103 1.0 CurrentHosts 1
09/12/12 15:30:35 ERROR "Error: corrupt log record 5 (byte offset 89) occurred inside closed transaction, recovery failed" at line 1136 in file /home/eje/git/grid/src/condor_utils/classad_log.cpp
</code></pre>

<p>Note that here the scheduler halted with an exception, as strict parsing was enabled, and the error was inside a completed transaction.</p>

<p>Here is a second example that contains a badly-formed ClassAd expression:</p>

<pre><code>107 1 CreationTimestamp 1334245749
101 0.0 Job Machine
103 0.0 NextClusterNum 1
105
103 1.0 JobStatus 2
103 1.0 EnteredCurrentStatus 1334245749
103 1.0 LastSuspensionTime 0
103 1.0 CurrentHosts 1
106
105
103 1.1 LastJobStatus 1 + eek!             &lt;- bad ClassAd expr!
103 1.1 JobStatus 2
</code></pre>

<p>Note that parse errors detected in unterminated transactions (the last transaction in a file may be uncompleted) are considered non-fatal:</p>

<pre><code>09/12/12 15:43:29 WARNING: Encountered corrupt log record 11 (byte offset 211)
09/12/12 15:43:29 Lines following corrupt log record 11 (up to 3):
09/12/12 15:43:29     103 1.1 JobStatus 2
09/12/12 15:43:29 Detected unterminated log entry in ClassAd Log /home/eje/condor/local/spool/job_queue.log. Forcing rotation.
</code></pre>

<h3>Disabling Strict Parse Checking</h3>

<p>Strict parse checking means that detected errors are fatal (unless in an unterminated transaction).  One consequence of the former lax error checking for Classad Log files is that some log file output was generated that was not properly formed.  Most such instances have been identified and corrected.  However, in order to accomodate legacy ClassAd Log files and any hidden bugs in log output generation, a condor configuration variable has been provided to disable strict checking:</p>

<pre><code># Disable strict parsing: parse errors will not be fatal
CLASSAD_LOG_STRICT_PARSING = False
</code></pre>

<p>In Red Hat Grid 2.2, <code>CLASSAD_LOG_STRICT_PARSING</code> defaults to <code>False</code>.  In the upstream condor repository, the default value has been set to <code>True</code>, in order to allow strict parsing failures to capture any remaining infrequent bugs in ClassAd log generation.</p>

<p>Note that strict checking can also be disabled or enabled <em>selectively</em>.  For example, this configuration disables strict checking only on the negotiator:</p>

<pre><code>CLASSAD_LOG_STRICT_PARSING = True
NEGOTIATOR.CLASSAD_LOG_STRICT_PARSING = False
</code></pre>

<h3>Categories of Undetectable Corruption</h3>

<p>In the ClassAd Log format, the key is considered an arbitrary string.  Therefore, any corruption that alters a key value is not detectable:</p>

<pre><code>103 1.rats! LastSuspensionTime 0   &lt;- weird key '1.rats!' will go undetected
</code></pre>

<p>Similarly, ClassAd attribute names are by nature arbitrary, and so corruptions to a name can go undetected:</p>

<pre><code>103 1.0 LastOopsie 0   &lt;- LastOopsie is a valid attribute name
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Driving a Condor Job Renice Policy with Accounting Groups]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/27/driving-a-condor-job-renice-policy-with-accounting-groups/"/>
    <updated>2012-07-27T13:50:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/27/driving-a-condor-job-renice-policy-with-accounting-groups</id>
    <content type="html"><![CDATA[<p>Condor can run its jobs with a renice priority level specified by <code>JOB_RENICE_INCREMENT</code>, which defaults simply to 10, but can in fact be any ClassAd expression, and is evaluated in the context of the job ad corresponding to the job being run.</p>

<p>This opens up an opportunity to create a renice <em>policy</em>, driven by accounting groups.  Consider a <a href="http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool/">scenario I discussed previously</a>, where a condor pool caters to mission critical (MC) jobs and regular (R) jobs.</p>

<p>An additional configuration trick we could apply is to add a renice policy that gives a higher renice value (that is, a lower priority) to any jobs that aren't run under the mission-critical (MC) rubric, as in this example configuration:</p>

<pre><code># A convenience expression that extracts group, e.g. "mc.user@domain.com" --&gt; "mc"
SUBMIT_EXPRS = AcctGroupName
AcctGroupName = ifThenElse(my.AccountingGroup =!= undefined, \
                           regexps("^([^@]+)\.[^.]+", my.AccountingGroup, "\1"), "&lt;none&gt;")

NUM_CPUS = 3

# Groups representing mission critical and regular jobs:
GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 2
GROUP_QUOTA_R = 1

# Any group not MC gets a renice increment of 10:
JOB_RENICE_INCREMENT = 10 * (AcctGroupName =!= "MC")
</code></pre>

<p>To demonstrate this policy in action, I wrote a little shell script I called <code>burn</code>, whose only function is to burn cycles for a given number of seconds:</p>

<pre><code>#!/bin/sh

# usage: burn [n]
# where n is number of seconds to burn cycles
s="$1"
if [ -z "$s" ]; then s=60; fi

t0=`date +%s`
while [ 1 ]; do
    x=0
    # burn some cycles:
    while [ $x -lt 10000 ]; do let x=$x+1; done
    t=`date +%s`
    let e=$t-$t0
    # halt when the requested time is up:
    if [ $e -gt $s ]; then exit ; fi
done
</code></pre>

<p>Begin by standing up a condor pool including the configuration above.   Make sure the <code>burn</code> script is readable.  Also, it is preferable to make sure your system is unloaded (load average should be as close to zero as reasonably possible).  Then submit the following, which instantiates two <code>burn</code> jobs running under accounting group <code>MC</code> and a third under group <code>R</code>:</p>

<pre><code>universe = vanilla
cmd = /path/to/burn
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup = "MC.user"
queue 2
+AccountingGroup = "R.user"
queue 1
</code></pre>

<p>Allow the jobs to negotiate and then run for a couple minutes.  You should then see something similar to the following load-average information from the slot ads:</p>

<pre><code>$ condor_status -format "%s" SlotID -format " | %.2f" LoadAvg -format " | %.2f" CondorLoadAvg -format " | %.2f" TotalLoadAvg -format " | %.2f" TotalCondorLoadAvg -format " | %s\n" AccountingGroup | sort
1 | 1.33 | 1.33 | 2.75 | 2.70 | MC.user@localdomain
2 | 1.28 | 1.24 | 2.75 | 2.70 | MC.user@localdomain
3 | 0.13 | 0.13 | 2.77 | 2.72 | R.user@localdomain
</code></pre>

<p>Note, which particular <code>SlotID</code> runs which job may vary.  However, we expect to see that the load averages for the slot running group <code>R</code> are much lower than the load averages for slots running jobs under group <code>MC</code>, as seen above.</p>

<p>We can explicitly verify the renice numbers from our policy to see that our one <code>R</code> job has a nice value of 10 (and is using only a fraction of the cpu):</p>

<pre><code># tell 'ps' to give us (pid, %cpu, nice, cmd+args):
$ ps -eo "%p %C %n %a" | grep 'burn 600'
22403 10.2  10 /bin/sh /home/eje/bin/burn 600
22406 93.2   0 /bin/sh /home/eje/bin/burn 600
22411 90.6   0 /bin/sh /home/eje/bin/burn 600
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LIFO and FIFO Preemption Policies for a Condor Pool]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/19/lifo-and-fifo-preemption-policies-for-a-condor-pool/"/>
    <updated>2012-07-19T13:57:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/19/lifo-and-fifo-preemption-policies-for-a-condor-pool</id>
    <content type="html"><![CDATA[<p>On a Condor pool, a Last In First Out (LIFO) preemption policy favors choosing the longest-running job from the available preemption options.  Correspondingly, a First In First Out (FIFO) policy favors the most-recent job for preemption.</p>

<p>Configuring a LIFO or FIFO policy is easy, using the <code>PREEMPTION_RANK</code> configuration variable.  <code>PREEMPTION_RANK</code> defines a ClassAd expression that is evaluated for all slots that are candidates for claim preemption, and causes those candidates to be sorted so that the candidates with the highest rank value are considered first.   Therefore, to implement a LIFO (or FIFO) preemption policy, one needs reference an expression that represents the claiming job's running time:</p>

<pre><code># LIFO preemption: favor preempting jobs that have been running the longest
PREEMPTION_RANK = TotalJobRunTime
# turn this into FIFO by using (-TotalJobRunTime)
</code></pre>

<p>The attribute <code>TotalJobRunTime</code> represents the amount of time a job has been running on its claim (generally, this is effectively equivalent to total running time, unless your job supports some form of checkpointing), and so ranking preemption candidates by this attribute results in LIFO preemption, and ranking by its negative provides FIFO preemption.</p>

<p>Note that <code>PREEMPTION_RANK</code> applies <em>only</em> to candidates that have already met the requirements defined on <code>PREEMPTION_REQUIREMENTS</code>, or the slot-centric preemption policy defined by <code>RANK</code>.  <code>PREEMPTION_RANK</code> does not itself determine what claimed slots are considered by a job for preemption.</p>

<p>To demonstrate LIFO and FIFO preemption in action, consider the following configuration:</p>

<pre><code># turn off scheduler optimizations, as they can sometimes obscure the
# negotiator/matchmaker behavior
CLAIM_WORKLIFE = 0
CLAIM_PARTITIONABLE_LEFTOVERS = False

# reduce update latencies for faster testing response
UPDATE_INTERVAL = 15
NEGOTIATOR_INTERVAL = 20
SCHEDD_INTERVAL = 15

# for demonstration purposes, make sure basic preemption knobs are 'on'
MAXJOBRETIREMENTTIME = 0
PREEMPTION_REQUIREMENTS = True
NEGOTIATOR_CONSIDER_PREEMPTION = True
RANK = 0.0

# LIFO preemption: favor preempting jobs that have been running the longest
PREEMPTION_RANK = TotalJobRunTime
# turn this into FIFO by using (-TotalJobRunTime)

# define 3 cpus to provide fodder for preemption
NUM_CPUS = 3
</code></pre>

<p>Begin by spinning up a condor pool with the configuration above.  When the pool is operating, fill the three slots with jobs for 'user1', with a delay to ensure that jobs have easily distinguishable values for <code>TotalJobRunTime</code>:</p>

<pre><code>$ cat /tmp/user1.jsub 
universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="user1"
queue 1

$ condor_submit /tmp/user1.jsub ; sleep 30 ; condor_submit /tmp/user1.jsub ; sleep 30 ; condor_submit /tmp/user1.jsub
</code></pre>

<p>Once these jobs have all started running, verify their run times using <a href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/">ccsort</a>:</p>

<pre><code>$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
1.0 | 78 | user1@localdomain
2.0 | 36 | user1@localdomain
3.0 | 16 | user1@localdomain
</code></pre>

<p>to make preemption easy, give user1 a low priority:</p>

<pre><code>$ condor_userprio -setprio user1@localdomain 10
</code></pre>

<p>Now, we will submit some jobs for 'user2': which will be allowed to preempt jobs for 'user1'.  We should see that the longest-running job for user1 is chosen each time:</p>

<pre><code>$ condor_submit /tmp/user2.jsub
Submitting job(s).
1 job(s) submitted to cluster 4.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
2.0 | 81 | user1@localdomain
3.0 | 61 | user1@localdomain
4.0 | 2 | user2@localdomain

$ condor_submit /tmp/user2.jsub
Submitting job(s).
1 job(s) submitted to cluster 5.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 91 | user1@localdomain
4.0 | 32 | user2@localdomain
5.0 | 3 | user2@localdomain
</code></pre>

<p>Now we change LIFO to FIFO and demonstrate.  Switch the sign of <code>TotalJobRunTime</code>:</p>

<pre><code># Now I am FIFO!
PREEMPTION_RANK = -TotalJobRunTime
</code></pre>

<p>And restart the negotiator, and check on our currently running jobs:</p>

<pre><code>$ condor_restart -negotiator

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 151 | user1@localdomain
4.0 | 92 | user2@localdomain
5.0 | 49 | user2@localdomain
</code></pre>

<p>Now, set up 'user2' for easy preemption like user1:</p>

<pre><code>$ condor_userprio -setprio user2@localdomain 10
</code></pre>

<p>And submit some jobs for user3.  Since we reconfigured for FIFO preemption, we should now see the <em>most recent</em> job preempted each time (in this case, these should both be the 'user2' jobs):</p>

<pre><code>$ condor_submit /tmp/user3.jsub
Submitting job(s).
1 job(s) submitted to cluster 6.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 241 | user1@localdomain
4.0 | 182 | user2@localdomain
6.0 | 15 | user3@localdomain

$ condor_submit /tmp/user3.jsub
Submitting job(s).
1 job(s) submitted to cluster 7.

$ ccsort condor_status JobID TotalJobRunTime AccountingGroup
3.0 | 301 | user1@localdomain
6.0 | 75 | user3@localdomain
7.0 | 17 | user3@localdomain
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Configuring Minimum and Maximum Resources for Mission Critical Jobs in a Condor Pool]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool/"/>
    <updated>2012-07-10T15:49:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool</id>
    <content type="html"><![CDATA[<p>Suppose you are administering a Condor pool for a company or organization where you want to support both "mission critical" (MC) jobs and "regular" (R) jobs.  Mission critical jobs might include IT functions such as backups, or payroll, or experiment submissions from high profile internal customers.  Regular jobs encompass any jobs that can be delayed, or preempted, with little or no consequence.</p>

<p>As part of your Condor policy for supporting MC jobs, you may want to ensure that these jobs always have access to a minimum set of resources on the pool.  In order to maintain the peace, you may also wish to set a pool-wide maximum on MC jobs, to leave some number of resources available for R jobs as well.  The following configuration, which I will discuss and demonstrate below, configures a pool-wide minimum <em>and maximum</em> for resources allocated to MC jobs.  Additionally, it shows how to dedicate MC resources on specific nodes in the pool.</p>

<pre><code># turn off scheduler optimizations, as they can sometimes obscure the
# negotiator/matchmaker behavior
CLAIM_WORKLIFE = 0

# turn off adaptive loops in negotiation - these give a single
# 'traditional' one-pass negotiation cycle
GROUP_QUOTA_MAX_ALLOCATION_ROUNDS = 1
GROUP_QUOTA_ROUND_ROBIN_RATE = 1e100

# for demonstration purposes, make sure basic preemption knobs are 'on'
MAXJOBRETIREMENTTIME = 0
PREEMPTION_REQUIREMENTS = True
NEGOTIATOR_CONSIDER_PREEMPTION = True
RANK = 0.0

# extracts the acct group name, e.g. "MC.user@localdomain" --&gt; "MC"
SUBMIT_EXPRS = AcctGroupName CCLimits
AcctGroupName = ifThenElse(my.AccountingGroup =!= undefined, \
                           regexps("^([^@]+)\.[^.]+", my.AccountingGroup, "\1"), "&lt;none&gt;")
CCLimits = ifThenElse(my.ConcurrencyLimits isnt undefined, \
                      my.ConcurrencyLimits, "***")
# note - the "my." scoping in the above is important - 
# these attribute names may also occur in a machine ad

# oversubscribe the machine to simulate 20 nodes on a single box
NUM_CPUS = 20

# accounting groups, each with equal quota
# Mission Critical jobs are associated with group 'MC'
# Regular jobs are associated with group 'R'
GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 10
GROUP_QUOTA_R = 10

# enable 'autoregroup' for groups, which gives all grps
# a chance to compete for resources above their quota
GROUP_AUTOREGROUP = TRUE
GROUP_ACCEPT_SURPLUS = FALSE

# a pool-wide limit on MC job resources
# note this is a "hard" limit - with this example config, MC jobs cannot exceed this
# limit even if there are free resources
MC_JOB_LIMIT = 15

# special slot for MC jobs, effectively reserves
# specific resources for MC jobs on a particular node.
SLOT_TYPE_1 = cpus=1
SLOT_TYPE_1_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_1 = 5

# Allocate any "non-MC" remainders here:
SLOT_TYPE_2 = cpus=1
SLOT_TYPE_2_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_2 = 15

# note - in the above, I declared static slots for the purposes of 
# demonstration, because partitionable slots interfere with clarity of
# APPEND_RANK expr behavior, due to being peeled off 1 slot at a time
# in the negotiation cycle

# A job counts against MC_JOB_LIMIT if and only if it is of the "MC" 
# accounting group, otherwise it won't be run
START = ($(START)) &amp;&amp; (((AcctGroupName =?= "MC") &amp;&amp; (stringListIMember("mc_job", CCLimits))) \
              || ((AcctGroupName =!= "MC") &amp;&amp; !stringListIMember("mc_job", CCLimits)))

# rank from the slot's POV:
# "MC-reserved" slots (slot type 1) prefer MC jobs,
# while other slots have no preference
RANK = ($(RANK)) + 10.0*ifThenElse((SlotTypeID=?=1) || (SlotTypeID=?=-1), \
                                   1.0 * (AcctGroupName =?= "MC"), 0.0)

# rank from the job's POV:
# "MC" jobs prefer any specially allocated per-node resources
# any other jobs prefer other jobs
APPEND_RANK = 10.0*ifThenElse(AcctGroupName =?= "MC", \
              1.0*((SlotTypeID=?=1) || (SlotTypeID=?=-1)), \
              1.0*((SlotTypeID=!=1) &amp;&amp; (SlotTypeID=!=-1)))

# If a job negotiated under "MC", it may not be preempted by a job that did not.
PREEMPTION_REQUIREMENTS = ($(PREEMPTION_REQUIREMENTS)) &amp;&amp; \
                          ((SubmitterNegotiatingGroup =?= "MC") || \
                           (RemoteNegotiatingGroup =!= "MC"))
</code></pre>

<p>Next I will discuss some of the components from this configuration and their purpose.  The first goal of a pool-wide resource minimum is accomplished by declaring accounting groups for MC and R jobs to run against:</p>

<pre><code>GROUP_NAMES = MC, R
GROUP_QUOTA_MC = 10
GROUP_QUOTA_R = 10
</code></pre>

<p>We will enable the autoregroup feature, which allows jobs to also compete for any unused resources <em>without</em> regard for accounting groups, after all jobs have had an opportunity to match under their group.  This is a good way to allow opportunistic resource usage, and also will facilitate demonstration.</p>

<pre><code>GROUP_AUTOREGROUP = TRUE
</code></pre>

<p>A pool-wide maximum on resource usage by MC jobs can be accomplished with a concurrency limit.  Note that this limit is larger than the group quota for MC jobs:</p>

<pre><code>MC_JOB_LIMIT = 15
</code></pre>

<p>It is also desirable to enforce the semantic that MC jobs <em>must</em> 'charge' against the MC_JOB concurrency limit, and conversely that any non-MC jobs are not allowed to charge against that limit.   Adding the following clause to the START expression enforces this semantic by preventing any jobs not following this rule from running:</p>

<pre><code>START = ($(START)) &amp;&amp; (((AcctGroupName =?= "MC") &amp;&amp; (stringListIMember("mc_job", CCLimits))) \
                    || ((AcctGroupName =!= "MC") &amp;&amp; !stringListIMember("mc_job", CCLimits)))
</code></pre>

<p>The final resource related goal for MC jobs is to reserve a certain number of resources on specific machines in the pool.  In the configuration above that is accomplished by declaring a special slot type, as here where we declare 5 slots of slot type 1 (the remaining 15 slots are declared via slot type 2, above):</p>

<pre><code>SLOT_TYPE_1 = cpus=1
SLOT_TYPE_1_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_1 = 5
</code></pre>

<p>Then we add a term to the slot rank expression that will cause any slot of type 1 to preempt a non-MC job in favor of an MC job (the factor of 10.0 is an optional tuning factor to allow this term to either take priority over other terms, or cede priority):</p>

<pre><code>RANK = ($(RANK)) + 10.0*ifThenElse((SlotTypeID=?=1) || (SlotTypeID=?=-1), \
                                   1.0 * (AcctGroupName =?= "MC"), 0.0)
</code></pre>

<p>(Note, slot type -1 would represent a dynamic slot derived from a partitionable slot of type 1.  In this example, all slots are static)</p>

<p>An additional "job side" rank term can also be helpful, to allow MC jobs to try and match special MC reserved slots first, and to allow non-MC jobs to avoid reserved slots if possible:</p>

<pre><code>APPEND_RANK = 10.0*ifThenElse(AcctGroupName =?= "MC", \
              1.0*((SlotTypeID=?=1) || (SlotTypeID=?=-1)), \
              1.0*((SlotTypeID=!=1) &amp;&amp; (SlotTypeID=!=-1)))
</code></pre>

<p>Lastly, preemption policy can be configured to help enforce resource allocations for MC jobs.  Here, a preemption clause is added to prevent any non-MC job from preempting a MC job, and specifically one that <em>negotiated</em> under its group quota (that is, it refers to RemoteNegotiatingGroup):</p>

<pre><code>PREEMPTION_REQUIREMENTS = ($(PREEMPTION_REQUIREMENTS)) &amp;&amp; \
                          ((SubmitterNegotiatingGroup =?= "MC") || \
                           (RemoteNegotiatingGroup =!= "MC"))
</code></pre>

<p>With the example policy configuration unpacked, we can demonstrate its behavior.  Begin by spinning up a pool with the above configuration.  Verify that we have the expected slots (You can refer <a href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/">here to learn more about cchist</a>):</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 undefined | undefined | 1
     15 undefined | undefined | 2
     20 total
</code></pre>

<p>Next, submit 20 Mission Critical jobs (getting enough sleep is critical):</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
concurrency_limits = mc_job
+AccountingGroup="MC.user"
queue 20
</code></pre>

<p>Since we configured a pool-wide maximum of 15 cores, we want to verify that we did not exceed that limit.  Note that 5 slots were negotiated under "&lt;none>", via the autoregroup feature (denoted by the value in RemoteNegotiatingGroup), as the group quota for MC is 10, and the MC jobs were able to match their pool limit of 15:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 MC | MC | 1
      5 MC | MC | 2
      5 MC | &lt;none&gt; | 2
      5 undefined | undefined | 2
     20 total
</code></pre>

<p>Next we set the MC submitter to a lower priority (i.e. higher prio value):</p>

<pre><code>$ condor_userprio -setprio MC.user@localdomain 10
The priority of MC.user@localdomain was set to 10.000000
</code></pre>

<p>Now we submit 15 "regular" R jobs:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="R.user"
queue 15
</code></pre>

<p>The submitter "R.user" currently has higher priority than "MC.user", however our preemption policy will only allow preemption of MC jobs that negotiated under "&lt;none>", as those were matched outside the accounting group's quota.  So we see that jobs with RemoteNegotiatingGroup == "MC" remain un-preempted:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 MC | MC | 1
      5 MC | MC | 2
     10 R | R | 2
     20 total
</code></pre>

<p>The above demonstrates the pool-wide quota and concurrentcy limits for MC jobs.  To demonstrate per-machine resources, we start by clearing all jobs:</p>

<pre><code>$ condor_rm -all
</code></pre>

<p>Submit 20 "R" jobs (similar to above), and verify that they occupy all slots, including the slots with SlotTypeID == 1, which are reserved for MC jobs (but not currently being used):</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 R | &lt;none&gt; | 1
      5 R | &lt;none&gt; | 2
     10 R | R | 2
     20 total
</code></pre>

<p>Submit 10 MC jobs.  "MC.user" does not have sufficient priority to preempt "R.user", however the slot rank expression <em>will</em> preempt non-MC jobs for an MC job on slots of type 1, and so we see that MC jobs <em>do</em> acquire the 5 type-1 slots reserved on this node:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 MC | MC | 1
      5 R | &lt;none&gt; | 2
     10 R | R | 2
     20 total
</code></pre>

<p>Finally, as an encore you can verify that jobs run against the MC accounting group must also charge against the MC_JOB concurrency limit, and non-MC jobs may not charge against it.  Again, start with an empty queue:</p>

<pre><code>$ condor_rm -all
</code></pre>

<p>Now, submit 'bad' jobs that use accounting group "MC" but does not use the "mc_job" concurrency limits:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="MC.user"
queue 10
</code></pre>

<p>And likewise some 'bad' regular jobs that attempt to use the "mc_job" concurrency limits:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 600
should_transfer_files = if_needed
when_to_transfer_output = on_exit
concurrency_limits = mc_job
+AccountingGroup="R.user"
queue 10
</code></pre>

<p>You should see that <em>none</em> of these jobs are allowed to run:</p>

<pre><code>$ cchist condor_status RemoteGroup RemoteNegotiatingGroup SlotTypeID
      5 undefined | undefined | 1
     15 undefined | undefined | 2
     20 total
$ cchist condor_q JobStatus
     20 1
     20 total
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Easy Histograms and Tables from Condor Jobs and Slots]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/"/>
    <updated>2012-06-29T09:46:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots</id>
    <content type="html"><![CDATA[<p>Several <a href="http://research.cs.wisc.edu/condor/">Condor</a> commands, including condor_status, condor_q and condor_history, provide a nice feature for outputting formatted subsets of classad attributes: the <code>-format &lt;format&gt; &lt;attr&gt;</code> option.  In this post, I assume basic familiarity with <code>-format</code>.  You can read more <a href="http://research.cs.wisc.edu/condor/manual/v7.8/condor_status.html#SECTION0011453000000000000000">here</a></p>

<p>The <code>-format</code> option can be used to generate tables and histograms of attributes, in a classic 'unix one-liner' fashion.  For example, supposing I wanted to use condor_status to create a nice histogram of the values for slot type, state, activity and accounting group.  I might issue a one-liner like this:</p>

<pre><code>$ condor_status -format "%s" 'ifThenElse(SlotType =!= undefined, string(SlotType), "undefined")' \
&gt; -format " | %s" 'ifThenElse(State =!= undefined, string(State), "undefined")' \
&gt; -format " | %s" 'ifThenElse(Activity =!= undefined, string(Activity), "undefined")' \
&gt; -format " | %s\n" 'ifThenElse(AccountingGroup =!= undefined, string(AccountingGroup), "undefined")' \
&gt; | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
      3 Static | Claimed | Busy | A.user@localdomain
      2 Static | Claimed | Busy | B.user@localdomain
     10 Static | Unclaimed | Idle | undefined
     15 total
</code></pre>

<p>Note that in this command I was extra pedantic and careful about converting expressions to strings, and using the ClassAd ifThenElse to trap and handle possible undefined values (which do indeed occur for AccountingGroup, when a slot is not in use).</p>

<p>We can see that a lot of this would benefit from some programmatic automation.  To that end I wrote some <a href="https://github.com/erikerlandson/bash_condor_tools">convenience bash functions</a> for automating the tedious portions of this process: <code>cchist</code>, <code>ccsort</code> and <code>ccdump</code>.  For example I could use <code>cchist</code> to generate the histogram from the example above much more cleanly:</p>

<pre><code>$ cchist condor_status SlotType State Activity AccountingGroup
      3 Static | Claimed | Busy | A.user@localdomain
      2 Static | Claimed | Busy | B.user@localdomain
     10 Static | Unclaimed | Idle | undefined
     15 total
</code></pre>

<p>The <code>ccdump</code> command simply dumps the table of values, uncollated, while <code>ccsort</code> outputs the table of values, but sorted:</p>

<pre><code>$ ccdump condor_status SlotType State Activity AccountingGroup
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | B.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Claimed | Busy | A.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Claimed | Busy | B.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
$ ccsort condor_status SlotType State Activity AccountingGroup
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | A.user@localdomain
Static | Claimed | Busy | B.user@localdomain
Static | Claimed | Busy | B.user@localdomain
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
Static | Unclaimed | Idle | undefined
</code></pre>

<p>If you are interested in providing the actual raw unix command that was executed, you can use the <code>-cmd</code> option (note, this currently must appear <em>first</em>)</p>

<pre><code>$ cchist -cmd condor_status SlotType State Activity AccountingGroup
condor_status -format "%s" 'ifThenElse(SlotType isnt undefined, string(SlotType), "undefined")' -format " | %s" 'ifThenElse(State isnt undefined, string(State), "undefined")' -format " | %s" 'ifThenElse(Activity isnt undefined, string(Activity), "undefined")' -format " | %s\n" 'ifThenElse(AccountingGroup isnt undefined, string(AccountingGroup), "undefined")' -constraint 'True' | sort | uniq -c | awk '{ print $0; t += $1 } END { printf("%7d total\n",t) }'
</code></pre>

<p>As you can see, the command condor_status is a parameter.  You can also use the same commands with condor_q and condor_history:</p>

<pre><code>$ cchist condor_q AccountingGroup LastJobStatus
      3 A.user | 1
      2 B.user | 1
      5 total
$ cchist condor_history AccountingGroup LastJobStatus
     18 A.user | 2
     26 B.user | 2
     20 C.user | 2
     64 total
</code></pre>

<p>You can obtain cchist and friends at the <a href="https://github.com/erikerlandson/bash_condor_tools">bash_condor_tools github repo</a></p>
]]></content>
  </entry>
  
</feed>
