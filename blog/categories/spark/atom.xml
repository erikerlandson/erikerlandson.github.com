<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: spark | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2017-03-06T14:40:49-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Efficient Multiplexing for Spark RDDs]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/02/08/efficient-multiplexing-for-spark-rdds/"/>
    <updated>2016-02-08T10:09:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/02/08/efficient-multiplexing-for-spark-rdds</id>
    <content type="html"><![CDATA[<p>In this post I'm going to propose a new abstract operation on <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark RDDs</a> -- <strong>multiplexing</strong> -- that makes some categories of operations on RDDs both easier to program and in many cases much faster.</p>

<p>My main working example will be the operation of splitting a collection of data elements into N randomly-selected subsamples.  This operation is quite common in machine learning, for the purpose of dividing data into a <a href="https://en.wikipedia.org/wiki/Test_set">training and testing set</a>, or the related task of <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics">creating folds for cross-validation</a>).</p>

<p>Consider the current standard RDD method for accomplishing this task, <code>randomSplit()</code>.  This method takes a collection of N weights, and returns N output RDDs, each of which contains a randomly-sampled subset of the input, proportional to the corresponding weight.  The <code>randomSplit()</code> method generates the jth output by running a random number generator (RNG) for each input data element and accepting all elements which are in the corresponding jth (normalized) weight range.  As a diagram, the process looks like this at each RDD partition:</p>

<p><img src="/assets/images/mux/randomsplit.png" title="Figure 1" alt="Figure 1" /></p>

<p>The observation I want to draw attention to is that to produce the N output RDDs, it has to run a random sampling over every element in the input <em>for each output</em>.  So if you are splitting into 10 outputs (e.g. for a 10-fold cross-validation), you are re-sampling your input 10 times, the only difference being that each output is created using a different acceptance range for the RNG output.</p>

<p>To see what this looks like in code, consider a simplified version of random splitting that just takes an integer <code>n</code> and always produces (n) equally-weighted outputs:</p>

<p>```scala
def splitSample<a href="rdd:%20RDD[T],%20n:%20Int,%20seed:%20Long%20=%2042">T :ClassTag</a>: Seq[RDD[T]] = {
  Vector.tabulate(n) { j =></p>

<pre><code>rdd.mapPartitions { data =&gt;
  scala.util.Random.setSeed(seed)
  data.filter { unused =&gt; scala.util.Random.nextInt(n) == j }
}
</code></pre>

<p>  }
}
```</p>

<p>(Note that for this method to operate correctly, the RNG seed must be set to the same value each time, or the data will not be correctly partitioned)</p>

<p>While this approach to random splitting works fine, resampling the same data N times is somewhat wasteful.  However, it is possible to re-organize the computation so that the input data is sampled only once.  The idea is to run the RNG once per data element, and save the element into a randomly-chosen collection.  To make this work in the RDD compute model, all N output collections reside in a single row of an <em>intermediate</em> RDD -- a "manifold" RDD.  Each output RDD then takes its data from the corresponding collection in the manifold RDD, as in this diagram:</p>

<p><img src="/assets/images/mux/multiplex.png" alt="Figure 2" /></p>

<p>If you abstract the diagram above into a generalized operation, you end up with methods that might like the following:</p>

<p>```scala
def muxPartitions<a href="n:%20Int,%20f:%20(Int,%20Iterator[T]">U :ClassTag</a> => Seq[U],
  persist: StorageLevel): Seq[RDD[U]] = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  Vector.tabulate(n) { j => mux.mapPartitions { itr => Iterator.single(itr.next()(j)) } }
}</p>

<p>def flatMuxPartitions<a href="n:%20Int,%20f:%20(Int,%20Iterator[T]">U :ClassTag</a> => Seq[TraversableOnce[U]],
  persist: StorageLevel): Seq[RDD[U]] = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  Vector.tabulate(n) { j => mux.mapPartitions { itr => itr.next()(j).toIterator } }
}
```</p>

<p>Here, the operation of sampling is generalized to any user-supplied function that maps RDD partition data into a sequence of objects that are computed in a single pass, and then multiplexed to the final user-visible outputs.  Note that these functions take a <code>StorageLevel</code> argument that can be used to control the caching level of the internal "manifold" RDD.  This typically defaults to <code>MEMORY_ONLY</code>, so that the computation can be saved and re-used for efficiency.</p>

<p>An efficient split-sampling method based on multiplexing, as described above, might be written using <code>flatMuxPartitions</code> as follows:</p>

<p>```scala
def splitSampleMux<a href="rdd:%20RDD[T],%20n:%20Int,%0A%20%20persist:%20StorageLevel%20=%20MEMORY_ONLY,%0A%20%20seed:%20Long%20=%2042">T :ClassTag</a>: Seq[RDD[T]] =
  rdd.flatMuxPartitions(n, (id: Int, data: Iterator[T]) => {</p>

<pre><code>scala.util.Random.setSeed(id.toLong * seed)
val samples = Vector.fill(n) { scala.collection.mutable.ArrayBuffer.empty[T] }
data.foreach { e =&gt; samples(scala.util.Random.nextInt(n)) += e }
samples
</code></pre>

<p>  }, persist)
```</p>

<p>To test whether multiplexed RDDs actually improve compute efficiency, I collected run-time data at various split values of <code>n</code> (from 1 to 10), for both the non-multiplexing logic (equivalent to the standard <code>randomSplit</code>) and the multiplexed version:</p>

<p><img src="/assets/images/mux/benchmark.png" title="Figure 3" alt="Figure 3" /></p>

<p>As the timing data above show, the computation required to run a non-multiplexed version grows linearly with <code>n</code>, just as predicted.  The multiplexed version, by computing the (n) outputs in a single pass, takes a nearly constant amount of time regardless of how many samples the input is split into.</p>

<p>There are other potential applications for multiplexed RDDs.  Consider the following tuple-based versions of multiplexing:</p>

<p>```scala
def muxPartitions<a href="f:%20(Int,%20Iterator[T]">U1 :ClassTag, U2 :ClassTag</a> => (U1, U2),
  persist: StorageLevel): (RDD[U1], RDD[U2]) = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  val mux1 = mux.mapPartitions(itr => Iterator.single(itr.next.<em>1))
  val mux2 = mux.mapPartitions(itr => Iterator.single(itr.next.</em>2))
  (mux1, mux2)
}</p>

<p>def flatMuxPartitions<a href="f:%20(Int,%20Iterator[T]">U1 :ClassTag, U2 :ClassTag</a> => (TraversableOnce[U1], TraversableOnce[U2]),
  persist: StorageLevel): (RDD[U1], RDD[U2]) = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  val mux1 = mux.mapPartitions(itr => itr.next.<em>1.toIterator)
  val mux2 = mux.mapPartitions(itr => itr.next.</em>2.toIterator)
  (mux1, mux2)
}
```</p>

<p>Suppose you wanted to run an input-validation filter on some data, sending the data that pass validation into one RDD, and data that failed into a second RDD, paired with information about the error that occurred.  Data validation is a potentially expensive operation.  With multiplexing, you can easily write the filter to operate in a single efficient pass to obtain both the valid stream and the stream of error-data:</p>

<p>```scala
def validate<a href="rdd:%20RDD[T],%20validator:%20T%20=>%20Boolean">T :ClassTag</a> = {
  rdd.flatMuxPartitions((id: Int, data: Iterator[T]) => {</p>

<pre><code>val valid = scala.collection.mutable.ArrayBuffer.empty[T]
val bad = scala.collection.mutable.ArrayBuffer.empty[(T, Exception)]
data.foreach { e =&gt;
  try {
    if (!validator(e)) throw new Exception("returned false")
    valid += e
  } catch {
    case err: Exception =&gt; bad += (e, err)
  }
}
(valid, bad)
</code></pre>

<p>  })
}
```</p>

<p>RDD multiplexing is currently a <a href="https://github.com/willb/silex/pull/50">PR against the silex project</a>.  The code I used to run the timing experiments above is <a href="https://github.com/erikerlandson/silex/blob/blog/muxrdd/src/main/scala/com/redhat/et/silex/sample/split.scala#L90">saved for posterity here</a>.</p>

<p>Happy multiplexing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hygienic Closures for Scala Function Serialization]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization/"/>
    <updated>2015-03-31T06:06:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization</id>
    <content type="html"><![CDATA[<p>In most use cases of Scala closures, what you see is what you get, but there are exceptions where looks can be deceiving and this can have a big impact on closure serialization.  Closure serialization is of more than academic interest.  Tools like Apache Spark cannot operate without serializing functions over the network.  In this post I'll describe some scenarios where closures include more than what is evident in the code, and then a technique for preventing unwanted inclusions.</p>

<p>To establish a bit of context, consider this simple example that obtains a function and serializes it to disk, and which <em>does</em> behave as expected:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  object foo {
    val v = 42
    // The returned function includes 'v' in its closure
    def f() = (x: Int) =&gt; v * x
  }

  // The function 'f' will serialize as expected
  val f = foo.f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>When this app is compiled and run, it will serialize <code>f</code> to "/tmp/demo.f1", which of course includes the value of <code>v</code> as part of the closure for <code>f</code>.</p>

<pre><code>$ scalac -d /tmp closures.scala
$ scala -cp /tmp Demo
$ ls /tmp/demo*
/tmp/demo.f
</code></pre>

<p>Now, imagine you wanted to make a straightforward change, where <code>object foo</code> becomes <code>class foo</code>:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  // foo is a class instead of an object
  class foo() {
    val v = 42
    // The returned function includes 'v' in its closure, but also a secret surprise
    def f() = (x: Int) =&gt; v * x
  }

  // This will throw an exception!
  val f = new foo().f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>It would be reasonable to expect that this minor variation behaves exactly as the previous one, but instead it throws an exception!</p>

<pre><code>$ scalac -d /tmp closures.scala
$ scala -cp /tmp Demo
java.io.NotSerializableException: Demo$foo
</code></pre>

<p>If we look at the exception message, we see that it's complaining about not knowing how to serialize objects of class <code>foo</code>.  But we weren't including any values of <code>foo</code> in the closure for <code>f</code>, only a particular member 'v'!  What gives?  Scala is not very helpful with diagnosing this problem, but when a class member value shows up in a closure that is defined <em>inside</em> the class body, the <em>entire instance</em>, including any and all other member values, is included in the closure.  Presumably this is because a class may have any number of instances, and the compiler is including the entire instance in the closure to properly resolve the correct member value.</p>

<p>One straightforward way to fix this is to simply make class <code>foo</code> serializable:</p>

<pre><code>class foo() extends Serializable {
  // ...
}
</code></pre>

<p>If you make this change to the above code, the example with <code>class foo</code> now works correctly, but it is working by serializing the entire <code>foo</code> instance, not just the value of <code>v</code>.</p>

<p>In many cases, this is not a problem and will work fine.  Serializing a few additional members may be inexpensive.  In other cases, however, it can be an impractical or impossible option.  For example, <code>foo</code> might include other very large members, which will be expensive or outright impossible to serialize:</p>

<pre><code>class foo() extends Serializable {
  val v = 42    // easy to serialize
  val w = 4.5   // easy to serialize
  val data = (1 to 1000000000).toList  // serialization landmine hiding in your closure

  // The returned function includes all of 'foo' instance in its closure
  def f() = (x: Int) =&gt; v * x
}
</code></pre>

<p>A variation on the above problem is class members that are small or moderate in size, but serialized many times.  In this case, the serialization cost can become intractable via repetition of unwanted inclusions.</p>

<p>Another potential problem is class members that are not serializable, and perhaps not under your control:</p>

<pre><code>class foo() extends Serializable {
  import some.class.NotSerializable

  val v = 42                      // easy to serialize
  val x = new NotSerializable     // I'll hide in your closure and fail to serialize

  // The returned function includes all of 'foo' instance in its closure
  def f() = (x: Int) =&gt; v * x
}
</code></pre>

<p>There is a relatively painless way to decouple values from their parent instance, so that only desired values are included in a closure.  Passing desired values as parameters to a shim function whose job is to assemble the closure will prevent the parent instance from being pulled into the closure.  In the following example, a shim function named <code>closureFunction</code> is defined for this purpose:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  // apply a generator to create a function with safe decoupled closures
  def closureFunction[E,D,R](enclosed: E)(gen: E =&gt; (D =&gt; R)) = gen(enclosed)

  class NotSerializable {}

  class foo() {
    val v1 = 42
    val v2 = 73
    val n = new NotSerializable

    // use shim function to enclose *only* the values of 'v1' and 'v2'
    def f() = closureFunction((v1, v2)) { enclosed =&gt;
      val (v1, v2) = enclosed
      (x: Int) =&gt; (v1 + v2) * x   // Desired function, with 'v1' and 'v2' enclosed
    }
  }

  // This will work!
  val f = new foo().f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>Being aware of the scenarios where parent instances are pulled into closures, and how to keep your closures clean, can save some frustration and wasted time.  Happy programming!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
    <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
    <updated>2014-09-11T07:57:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
    <content type="html"><![CDATA[<blockquote><p>Update (April 4, 2016): my colleague <a href="http://rnowling.github.io/">RJ Nowling</a> ran across a <a href="http://www.ittc.ku.edu/~jsv/Papers/Vit87.RandomSampling.pdf">paper by J.S. Vitter</a> that shows Vitter developed the trick of accelerating sampling with a sampling-gap distribution in 1987 -- I re-invented Vitter's wheel 30 years after the fact!  I'm surprised it never caught on, as it is not much harder to implement than the naive version.</p></blockquote>

<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode:</p>

<pre><code>sample(data: array, p: real) {
    n = length(data)
    m = floor(p * n)
    for j = 0 to m-1 {
        k = random(j, n-1)
        swap(data[j], data[k])
    }
    emit the first m elements of 'data' to output
}
</code></pre>

<p>We can see that this sampling algorithm is indeed O(np).  However, it makes some nontrivial assumptions about its input data:</p>

<ul>
<li>It is random access</li>
<li>It is writable</li>
<li>Its size is known</li>
<li>It can be destructively modified</li>
</ul>


<p>These assumptions can be violated in several ways.  The input data might not support random access, for example it might be a list, or stream, or an iterator over the same.  We might not know its size a priori.  It might be read-only.  It might be up-cast to some superclass where knowledge about these assumed properties is no longer available.</p>

<p>In cases such as this, there is another common sampling algorithm:</p>

<pre><code>sample(data: sequence, p: real) {
    while not end(data) {
        v = next(data)
        if random(0.0, 1.0) &lt; p then emit v to output
    }
}
</code></pre>

<p>The above algorithm enjoys all the advantage in flexibility.  It requires only linear access, does not require writable input, and makes no assumptions about input size.  However it comes at a price: this algorithm is no longer O(np), it is O(n).  Each element must be traversed directly, and worse yet the random number generagor (RNG) must be invoked on each element.  O(n) invocation of the RNG is a substantial cost -- random number generation is typically very expensive compared to the cost of iterating to the next element in a sequence.</p>

<p>But... does linear sampling truly require us to invoke our RNG on every element?   Consider the pattern of data access, divorced from code.   It looks like a sequence of choices: for each element we either (skip) or (sample):</p>

<pre><code>(skip) (skip) (sample) (skip) (sample) (skip) (sample) (sample) (skip) (skip) (sample) ...
</code></pre>

<p>The number of consecutive (skip) events between each (sample) -- the <em>sampling gap</em> -- can itself be modeled as a random variable.  Each (skip)/(sample) choice is an independent Bernoulli trial, where the probability of (skip) is (1-p).   The PMF of the sampling gap for gap of {0, 1, 2, ...} is therefore a geometric distribution: P(k) = p(1-p)<sup>k</sup></p>

<p>This suggests an alternative algorithm for sampling, where we only need to randomly choose sample gaps instead of randomly choosing whether we sample each individual element:</p>

<pre><code>// choose a random sampling gap 'k' from P(k) = p(1-p)^k
// caution: this explodes for p = 0 or p = 1
random_gap(p: real) {
    u = max(random(0.0, 1.0), epsilon)
    return floor(log(u) / log(1-p))
}

sample(data: sequence, p: real) {
    advance(data, random_gap(p))
    while not end(data) {
        emit next(data) to output
        advance(data, random_gap(p))
    }
}
</code></pre>

<p>The above algorithm calls the RNG only once per actual collected sample, and so the cost of RNG calls is O(np).  Note that the algorithm is still O(n), but the cost of the RNG tends to dominate the cost of sequence traversal, and so the resulting efficiency improvement is substantial.  I measured the following performance improvements with gap sampling, compared to traditional linear sequence sampling, on a <a href="https://gist.github.com/erikerlandson/05db1f15c8d623448ff6">Scala prototype testing rig</a>:</p>

<p><head><style>
table, th, td {
border: 1px solid black;
border-collapse: collapse;
}
th, td {
padding: 10px;
}
th {
text-align: center;
}
</style></head></p>

<table>
<tr> <th>Type</th> <th>p</th> <th>linear</th> <th>gap</th> </tr>
<tr> <td>Array</td> <td>0.001</td> <td>2833</td> <td>29</td> </tr>
<tr> <td>Array</td> <td>0.01</td> <td>2825</td> <td>76</td> </tr>
<tr> <td>Array</td> <td>0.1</td> <td>2985</td> <td>787</td> </tr>
<tr> <td>Array</td> <td>0.5</td> <td>3526</td> <td>3478</td> </tr>
<tr> <td>Array</td> <td>0.9</td> <td>3023</td> <td>6081</td> </tr>
<tr> <td>List</td> <td>0.001</td> <td>2213</td> <td>230</td> </tr>
<tr> <td>List</td> <td>0.01</td> <td>2220</td> <td>265</td> </tr>
<tr> <td>List</td> <td>0.1</td> <td>2337</td> <td>796</td> </tr>
<tr> <td>List</td> <td>0.5</td> <td>2794</td> <td>3151</td> </tr>
<tr> <td>List</td> <td>0.9</td> <td>2513</td> <td>4849</td> </tr>
</table>




<br>


<p>In the results above, we see that the gap sampling times are essentially linear in (p), as expected.  In the case of the linear-access List type, there is a higher baseline time (230 vs 29) due to the constant cost of actual data traversal.  Efficiency improvements are substantial at small sampling probabilities.</p>

<p>We can also see that the cost of gap sampling begins to meet and then exceed the cost of traditinal linear sampling, in the vicinnity (p) = 0.5.  This is due to the fact that the gap sampling logic is about twice the cost (in my test environment) of simply calling the RNG once.  For example, the gap sampling invokes a call to the numeric logarithm code that isn't required in traditional sampling.  And so at (p) = 0.5 the time spent doing the gap sampling approximates the time spent invoking the RNG once per sample, and at higher values of (p) the cost is greater.</p>

<p>This suggests that one should in fact fall back to traditional linear sampling when the sampling probability (p) >= some threshold.  That threshold appears to be about 0.5 or 0.6 in my testing rig, but is likely to depend on underlying numeric libraries, the particular RNG being used, etc, and so I would expect it to benefit from customized tuning on a per-environment basis.  With this in mind, a sample algorithm as deployed would look like this:</p>

<pre><code>// threshold is a tuning parameter
threshold = 0.5

sample(data: sequence, p: real) {
    if (p &lt; threshold) {
        gap_sample(data, p)
    } else {
        traditional_linear_sample(data, p)
    }
}
</code></pre>

<p>The gap-sampling algorithm described above is for sampling <em>without</em> replacement.   However, the same approach can be modified to generate sampling <em>with</em> replacement.</p>

<p>When sampling with replacement, it is useful to consider the <em>replication factor</em> of each element (where a replication factor of zero means the element wasn't sampled).  Pretend for the moment that the actual data size (n) is known.  The sample size (m) = (n)(p).  The probability that each element gets sampled, per trial, is 1/n, with (m) independent trials, and so the replication factor (r) for each element obeys a binomial distribution: Binomial(m, 1/n).  If we substitute (n)(p) for (m), we have Binomial(np, 1/n).  As the (n) grows, the Binomial is <a href="http://en.wikipedia.org/wiki/Binomial_distribution#Poisson_approximation">well approximated by a Poisson distribution</a> Poisson(L), where (L) = (np)(1/n) = (p).  And so for our purposes we may sample from Poisson(p), where P(r) = (p<sup>r</sup> / r!)e<sup>(-p),</sup> for our sampling replication factors.  Note that we have now discarded any dependence on sample size (n), as we desire.</p>

<p>In our gap-sampling context, the sampling gaps are now elements whose replication factor is zero, which occurs with probability P(0) = e<sup>(-p).</sup>  And so our sampling gaps are now drawn from geometric distribution P(k) = (1-q)(q)<sup>k,</sup> where q = e<sup>(-p).</sup>   When we <em>do</em> sample an element, its replication factor is drawn from Poisson(p), however <em>conditioned such that the value is >= 1.</em>  It is straightforward to adapt a <a href="http://en.wikipedia.org/wiki/Poisson_distribution#Generating_Poisson-distributed_random_variables">standard Poisson generator</a>, as shown below.</p>

<p>Given the above, gap sampling with replacement in pseudocode looks like:</p>

<pre><code>// sample 'k' from Poisson(p), conditioned to k &gt;= 1
poisson_ge1(p: real) {
    q = e^(-p)
    // simulate a poisson trial such that k &gt;= 1
    t = q + (1-q)*random(0.0, 1.0)
    k = 1

    // continue standard poisson generation trials
    t = t * random(0.0, 1.0)
    while (t &gt; q) {
        k = k + 1
        t = t * random(0.0, 1.0)
    }
    return k
}

// choose a random sampling gap 'k' from P(k) = p(1-p)^k
// caution: this explodes for p = 0 or p = 1
random_gap(p: real) {
    u = max(random(0.0, 1.0), epsilon)
    return floor(log(u) / -p)
}

sample(data: sequence, p: real) {
    advance(data, random_gap(p))
    while not end(data) {
        rf = poisson_ge1(p)
        v = next(data)
        emit (rf) copies of (v) to output
        advance(data, random_gap(p))
    }
}
</code></pre>

<p>The efficiency improvements I have measured for gap sampling with replacement are shown here:</p>

<table>
<tr> <th>Type</th> <th>p</th> <th>linear</th> <th>gap</th> </tr>
<tr> <td>Array</td> <td>0.001</td> <td>2604</td> <td>45</td> </tr>
<tr> <td>Array</td> <td>0.01</td> <td>3442</td> <td>117</td> </tr>
<tr> <td>Array</td> <td>0.1</td> <td>3653</td> <td>1044</td> </tr>
<tr> <td>Array</td> <td>0.5</td> <td>5643</td> <td>5073</td> </tr>
<tr> <td>Array</td> <td>0.9</td> <td>7668</td> <td>8388</td> </tr>
<tr> <td>List</td> <td>0.001</td> <td>2431</td> <td>233</td> </tr>
<tr> <td>List</td> <td>0.01</td> <td>2450</td> <td>299</td> </tr>
<tr> <td>List</td> <td>0.1</td> <td>2984</td> <td>1330</td> </tr>
<tr> <td>List</td> <td>0.5</td> <td>5331</td> <td>4752</td> </tr>
<tr> <td>List</td> <td>0.9</td> <td>6744</td> <td>7811</td> </tr>
</table>




<br>


<p>As with the results for sampling without replacement, we see that gap sampling cost is linear with (p), which yields large cost savings at small sampling, but begins to exceed traditional linear sampling at higher sampling probabilities.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementing Parallel Prefix Scan as a Spark RDD Transform]]></title>
    <link href="http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform/"/>
    <updated>2014-08-12T11:37:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform</id>
    <content type="html"><![CDATA[<p>In my <a href="/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/">previous post</a>, I described how to implement the Scala <code>scanLeft</code> function as an RDD transform.  By definition <code>scanLeft</code> invokes a sequential-only prefix scan algorithm; it does not assume that either its input function <code>f</code> or its initial-value <code>z</code> can be applied in a parallel fashion.   Its companion function <code>scan</code>, however, computes a <em>parallel</em> prefix scan.  In this post I will describe an implementation of parallel prefix <code>scan</code> as an RDD transform.</p>

<p>As was the case with <code>scanLeft</code>, a basic strategy is to begin by applying <code>scan</code> to each RDD partition.  Provided that appropriate "offsets" <code>{z1, z2, ...}</code> can be computed for each partition, these can be applied to the partial, per-partition results to yield the output.   In fact, the desired <code>{z1, z2, ...}</code> are the parallel prefix scan of the last element in each per-partition scan.  The following diagram illustrates:</p>

<p><img src="/assets/images/rdd_scan/rdd_scan_4.png" alt="image" /></p>

<p>The diagram above glosses over the details of computing <code>scan</code> to obtain <code>{z1, z2, ...}</code>.   I will first describe the implementation I currently use, and then also discuss a possible alternative.  The current implementation takes the approach of encoding the <a href="http://en.wikipedia.org/wiki/Prefix_sum#Parallel_algorithm">logic of a parallel prefix scan</a> directly into an RDD computation DAG.   Each iteration, or "ply," of the parallel algorithm is represented by an RDD.  Each element resides in its own partition, and so the computation dependency for each element is directly representable in the RDD dependency substructure.  This construction is illustrated in the following schematic (for a vector of 8 z-values):</p>

<p><img src="/assets/images/rdd_scan/rdd_scan_5.png" alt="image" /></p>

<p>The parallel prefix scan algorithm executes O(log(n)) plies, which materializes as O(log(n)) RDDs shown in the diagram above.  In this context, (n) is the number of input RDD <em>partitions</em>, not to be confused with the number of data rows in the RDD.   There are O((n)log(n)) partitions, each having a single row containing the z-value for a corresponding output partition.   Some z-values are determined earlier than others.  For example z1 is immediately available in ply(0), and ply(3) can refer directly back to that ply(0) partition in the interest of efficiency, as called out by the red DAG arcs.</p>

<p>This scheme allows each final output partition to obtain its z-value directly from a single dedicated partition, which ensures that minimal data needs to be transferred across worker processes.  Final output partitions can be computed local to their corresponding input partitions.  Data transfer may be limited to the intermediate z-values, which are small single-row affairs by construction.</p>

<p>The code implementing the logic above can be <a href="https://github.com/erikerlandson/spark/blob/rdd_scan_blog/core/src/main/scala/org/apache/spark/rdd/ScanRDDFunctions.scala#L161">viewed here.</a></p>

<p>I will conclude by noting that there is an alternative to this highly distributed computation of <code>{z1, z2, ...}</code>, which is to collect the last-values in the per-partition intermediate scan ouputs into a single array, and run <code>scan</code> directly on that array.   This has the advantage of avoiding the construction of log(n) intermediate RDDs.   It does, however, require a monolithic 'fan-in' of data into a single RDD to receive the collection of values.  That is followed by a fan-out of the array, where each output partition picks its single z-value from the array.  It is for this reason I suspect this alternative incurs substantially more transfer overhead across worker processes.  However, one might also partition the resulting z-values in some optimal way, so that each final output partition needs to request only the partition that contains its z-value.  Future experimentation might show that this can out-perform the current fully-distributed implementation.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementing an RDD scanLeft Transform With Cascade RDDs]]></title>
    <link href="http://erikerlandson.github.com/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/"/>
    <updated>2014-08-09T09:10:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds</id>
    <content type="html"><![CDATA[<p>In Scala, sequence (and iterator) data types support the <code>scanLeft</code> method for computing a sequential prefix scan on sequence elements:</p>

<pre><code>// Use scanLeft to compute the cumulative sum of some integers
scala&gt; List(1, 2, 3).scanLeft(0)(_ + _)
res0: List[Int] = List(0, 1, 3, 6)
</code></pre>

<p>Spark RDDs are logically a sequence of row objects, and so <code>scanLeft</code> is in principle well defined on RDDs.  In this post I will describe how to cleanly implement a <code>scanLeft</code> RDD transform by applying an RDD variation called Cascade RDDs.</p>

<p>A Cascade RDD is an RDD having one partition which is a function of an input RDD partition and an optional predecessor Cascade RDD partition.  You can see that this definition is somewhat recursive, where the basis case is a Cascade RDD having no precedessor.  The following diagram illustrates both cases of Cascade RDD:</p>

<p><img src="/assets/images/rdd_scanleft/rdd_scan_1.png" alt="image" /></p>

<p>As implied by the above diagram, a series of Cascade RDDs falling out of an input RDD will have as many Cascade RDDs as there are input partitions.  This situation begs for an abstraction to re-assemble the cascade back into a single output RDD, and so the method <code>cascadePartitions</code> is defined, as illustrated:</p>

<p><img src="/assets/images/rdd_scanleft/rdd_scan_3.png" alt="image" /></p>

<p>The <code>cascadePartitions</code> method takes a function argument <code>f</code>, with the signature:</p>

<pre><code>f(input: Iterator[T], cascade: Option[Iterator[U]]): Iterator[U]
</code></pre>

<p>in a manner somewhat analogous to <code>mapPartitions</code>.  The function <code>f</code> must address the fact that <code>cascade</code> is optional and will be <code>None</code> in case where there is no predecessor Cascade RDD.  The interested reader can examine the details of how the <code>CascadeRDD</code> class and its companion method <code>cascadePartitions</code> are <a href="https://github.com/erikerlandson/spark/blob/rdd_scan_blog/core/src/main/scala/org/apache/spark/rdd/CascadeRDDFunctions.scala">implemented here.</a></p>

<p>With Cascade RDDs it is now straightforward to define a <code>scanLeft</code> transform for RDDs.  We wish to run <code>scanLeft</code> on each input partition, with the condition that we want to start where the previous input partition left off.  The Scala <code>scanLeft</code> function makes this easy, as the starting point is its first parameter (z): <code>scanLeft(z)(f)</code>.  The following figure illustrates what this looks like:</p>

<p><img src="/assets/images/rdd_scanleft/rdd_scan_2.png" alt="image" /></p>

<p>As the above schematic demonstrates, almost all the work is accomplished with a single call to <code>cascadePartitions</code>, using a thin wrapper around <code>f</code> which determines where to start the next invocation of Scala <code>scanLeft</code> -- either the input parameter <code>z</code>, or the last output element of the previous cascade.   One final transform must be applied to remove the initial element that Scala <code>scanLeft</code> inserts into its output, excepting the first output partition, where it is kept to be consistent with the <code>scanLeft</code> definition.</p>

<p>All computation is accomplished in the standard RDD formalism, and so <code>scanLeft</code> is a proper lazy RDD transform.</p>

<p>The actual implementation is as compact as the above description implies, and you can see the <a href="https://github.com/erikerlandson/spark/blob/rdd_scan_blog/core/src/main/scala/org/apache/spark/rdd/ScanRDDFunctions.scala#L144">code here.</a></p>
]]></content>
  </entry>
  
</feed>
