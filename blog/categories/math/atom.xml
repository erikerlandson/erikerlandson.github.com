<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: math | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2015-08-14T14:48:03-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Generalizing Kendall's Tau]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau/"/>
    <updated>2015-08-14T14:35:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau</id>
    <content type="html"><![CDATA[<p>Recently I have been applying <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient">Kendall's Tau</a> as an evaluation metric to assess how well a regression model ranks input samples, with respect to a known correct ranking.</p>

<p>The process of implementing the Kendall's Tau statistic, with my software engineer's hat on, caused me to reflect a bit on how it could be generalized beyond the traditional application of ranking numeric pairs; what follows is a brief discussion of these reflections.</p>

<h5>A Review of Kendall's Tau</h5>

<p>I'll start with a brief review of Kendall's Tau.  For more depth, a good place to start is the Wikipedia article at the link above.</p>

<p>Consider a sequence of (n) observations where each observation is a pair (x,y), where we wish to measure how well a ranking by x-values agrees with a ranking by the y-values.  Informally, Kendall's Tau (aka the Kendall Rank Correlation Coefficient) is the difference between number of observation-pairs (pairs of pairs, if you will) whose ordering <em>agrees</em> ("concordant" pairs) and the number of such pairs whose ordering <em>disagrees</em> ("discordant" pairs).  This difference is divided by the total number of observation pairs.</p>

<p>The commonly-used formulation of Kendall's Tau is the "Tau-B" statistic, which accounts for observed pairs having tied values in either x or y as being neither concordant nor discordant:</p>

<h6>Figure 1: Kendall's Tau-B</h6>

<p><img src="/assets/images/kendalls_tau/figure_1.png" title="Kendall's Tau" alt="Kendall's Tau" /></p>

<p>The formulation above has quadratic complexity, with respect to data size (n).  It is possible to rearrange this computation in a way that can be computed in (n)log(n) time[1]:</p>

<h6>Figure 2: An (n)log(n) formulation of Kendall's Tau-B</h6>

<p><img src="/assets/images/kendalls_tau/figure_2.png" title="Kendall's Tau" alt="Kendall's Tau" /></p>

<p>The details of performing this computation can be found at [1] or on the <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Algorithms">Wikipedia entry</a>.  For my purposes, I'll note that it requires two (n)log(n) sorts of the data, which becomes relevant below.</p>

<h5>Generalizing to Non-Numeric Values</h5>

<p>Generalizing Kendall's Tau to non-numeric values is mostly just making the observation that the definition of "concordant" and "discordant" pairs is purely based on comparing x-values and y-values (and, in the (n)log(n) formulation, performing sorts on the data).  From the software engineer's perspective this means that the computations are well defined on any data type with an ordering relation, which includes numeric types but also chars, strings, sequences of any element supporting an ordering, etc.  Significantly, most programming languages support the concept of defining ordering relations on arbitrary data types, which means that <em><em>Kendall's Tau can, in principle, be computed on literally any kind of data structure</em></em>, provided you provide it with a well defined ordering.  Furthermore, an examination of the algorithms shows that values of x and y need not even be of the same type, nor do they require the same ordering.</p>

<h5>Generalizing to Partial Orderings</h5>

<p>When I brought this observation up, my colleague <a href="http://chapeau.freevariable.com/">Will Benton</a> asked the very interesting question of whether it's also possible to compute Kendall's Tau on objects that have only a <em>partial ordering</em>.  It turns out that you <em><em>can</em></em> define Kendall's Tau on partially ordered data, by defining the case of two non-comparable x-values, or y-values, as another kind of tie.</p>

<p>The big caveat with this definition is that the (n)log(n) optimization does not apply.  Firstly, the optimized algorithm relies heavily on (n)log(n) sorting, and there is no unique full sorting of elements that are only partially ordered.  Secondly, the formula's definition of the quantities n1, n2 and n3 is founded on the assumption that element equality is transitive; this is why you can count a number of tied values, t, and use t(t-1)/2 as the corresponding number of tied pairs.  But in a partial ordering, this assumption is violated. Consider the case where (a) &lt; (b), but (a) is non-comparable to (c) and (b) is also non-comparable to (c).  By our definition, (a) is tied with (c), and (c) is tied with (b), but transitivity is violated, as (a) &lt; (b).</p>

<p>So how <em>can</em> we compute Tau in this case?  Consider (n1) and (n2), in Figure-1.  These values represent the number of pairs that were tied wrt (x) and (y), respectively.  We can't use the shortcut formulas for (n1) and (n2), but we can count them directly, pair by pair, simply by conducting the traditional quadratic iteration over pairs, and incrementing (n1) whenever two x-values are noncomparable, and incrementing (n2) whenever two y-values are non-comparable, just as we increment (nc) and (nd) to count concordant and discordant pairs.  With this modification, we can apply the formula in Figure-1 as-is.</p>

<h5>Conclusions</h5>

<p>I made these observations without any particular application in mind. However, my instincts as a software engineer tell me that making generalizations in this way often paves the way for new ideas, once the generalized concept is made available.  With luck, it will inspire either me or somebody else to apply Kendall's Tau in interesting new ways.</p>

<h5>References</h5>

<p>[1] Knight, W. (1966). "A Computer Method for Calculating Kendall's Tau with Ungrouped Data". Journal of the American Statistical Association 61 (314): 436â€“439. doi:10.2307/2282833. JSTOR 2282833.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Smooth Gradients for Cubic Hermite Splines]]></title>
    <link href="http://erikerlandson.github.com/blog/2013/03/16/smooth-gradients-for-cubic-hermite-splines/"/>
    <updated>2013-03-16T07:39:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2013/03/16/smooth-gradients-for-cubic-hermite-splines</id>
    <content type="html"><![CDATA[<p>One of the advantages of cubic Hermite splines is that their interval interpolation formula is an explicit function of gradients \( m_0, m_1, ... m_{n-1} \) at knot-points:</p>

<div markdown="0">
\\[
y(t) = h_{00}(t) y_j + h_{10}(t) m_j + h_{01}(t) y_{j+1} + h_{11}(t) m_{j+1} \\\\
\\]
</div>


<p>where the Hermite bases are:</p>

<div markdown="0">
\\[
h_{00} = 2t^3 - 3t^2 + 1 \\\\
h_{10} = t^3 - 2t^2 + t \\\\
h_{01} = -2t^3 + 3t^2 \\\\
h_{11} = t^3 - t^2 \\\\
\\]
</div>


<p>(For now, I will be using the unit-interval form of the interpolation, where t runs from 0 to 1 on each interval.  I will also discuss the non-uniform interval equations below)</p>

<p>This formulation allows one to explicitly specify the interpolation gradient at each knot point, and to choose from various gradient assignment policies, for example <a href="http://en.wikipedia.org/wiki/Cubic_Hermite_spline#Interpolating_a_data_set">those listed here</a>, even supporting policies for <a href="http://en.wikipedia.org/wiki/Monotone_cubic_interpolation">enforcing monotonic interpolations</a>.</p>

<p>One important caveat with cubic Hermite splines is that although the gradient \( y'(t) \) is guaranteed to be continuous, it is <em>not</em> guaranteed to be smooth (that is, differentiable) <em>across</em> the knots (it is of course smooth <em>inside</em> each interval). Therefore, another useful category of gradient policy is to obtain gradients \( m_0, m_1, ... m_{n-1} \) such that \( y'(t) \) is also smooth across knots.</p>

<p>(I feel sure that what follows was long since derived elsewhere, but my attempts to dig the formulation up on the internet failed, and so I decided the derivation might make a useful blog post)</p>

<p>To ensure smooth gradient across knot points, we want the 2nd derivative \( y"(t) \) to be equal at the boundaries of adjacent intervals:</p>

<div markdown="0">
\\[
h_{00}^"(t) y_{j-1} + h_{10}^"(t) m_{j-1} + h_{01}^"(t) y_j + h_{11}^"(t) m_j \\\\
= \\\\
h_{00}^"(t) y_j + h_{10}^"(t) m_j + h_{01}^"(t) y_{j+1} + h_{11}^"(t) m_{j+1}
\\]
</div>


<p>or substituting the 2nd derivative of the basis definitions above:</p>

<div markdown="0">
\\[
\\left( 12 t - 6 \\right) y_{j-1} + \\left( 6 t - 4 \\right) m_{j-1}  + \\left( 6 - 12 t \\right) y_j + \\left( 6 t - 2 \\right) m_j \\\\
= \\\\
\\left( 12 t - 6 \\right) y_{j} + \\left( 6 t - 4 \\right) m_{j}  + \\left( 6 - 12 t \\right) y_{j+1} + \\left( 6 t - 2 \\right) m_{j+1}
\\]
</div>


<p>Observe that t = 1 on the left hand side of this equation, and t = 0 on the right side, and so we have:</p>

<div markdown="0">
\\[
6 y_{j-1} + 2 m_{j-1} - 6 y_j + 4 m_j
=
-6 y_j - 4 m_j + 6 y_{j+1} - 2 m_{j+1}
\\]
</div>


<p>which we can rearrange as:</p>

<div markdown="0">
\\[
2 m_{j-1} + 8 m_j + 2 m_{j+1}
=
6 \\left( y_{j+1} - y_{j-1} \\right)
\\]
</div>


<p>Given n knot points, the above equation holds for j = 1 to n-2 (using zero-based indexing, as nature intended).  Once we define equations for j = 0 and j = n-1, we will have a system of equations to solve.  There are two likely choices.  The first is to simply specify the endpoint gradients \( m_0 = G \) and \( m_{n-1} = H \) directly, which yields the following <a href="http://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm">tri-diagonal matrix equation:</a></p>

<div markdown="0">
\\[
\\left( \\begin{array} {ccccc}
1 &   &   &   &   \\\\
2 & 8 & 2 &   &   \\\\
  & 2 & 8 & 2 &   \\\\
  &   & \\vdots &   &   \\\\
  &   & 2 & 8 & 2 \\\\ 
  &   &   &   & 1 \\\\
\\end{array} \\right)

\\left( \\begin{array} {c}
m_0 \\\\
m_1 \\\\
 \\\\
\\vdots \\\\
 \\\\
m_{n-1}
\\end{array} \\right)
=
\\left( \\begin{array} {c}
G \\\\
6 \\left( y_2 - y_0 \\right) \\\\
6 \\left( y_3 - y_1 \\right) \\\\
\\vdots \\\\
6 \\left( y_{n-1} - y_{n-3} \\right) \\\\
H \\\\
\\end{array} \\right)
\\]
</div>


<p>The second common endpoint policy is to set the 2nd derivative equal to zero -- the "natural spline."   Setting the 2nd derivative to zero at the left-end knot (and t = 0) gives us:</p>

<div markdown="0">
\\[
4 m_0 + 2 m_1   =   6 \\left( y_1 - y_0 \\right)
\\]
</div>


<p>Similarly, at the right-end knot (t = 1), we have:</p>

<div markdown="0">
\\[
2 m_0 + 4 m_1   =   6 \\left( y_{n-1} - y_{n-2} \\right)
\\]
</div>


<p>And so for a natural spline endpoint policy the matrix equation looks like this:</p>

<div markdown="0">
\\[
\\left( \\begin{array} {ccccc}
4 & 2 &   &   &   \\\\
2 & 8 & 2 &   &   \\\\
  & 2 & 8 & 2 &   \\\\
  &   & \\vdots &   &   \\\\
  &   & 2 & 8 & 2 \\\\ 
  &   &   & 2 & 4 \\\\
\\end{array} \\right)

\\left( \\begin{array} {c}
m_0 \\\\
m_1 \\\\
 \\\\
\\vdots \\\\
 \\\\
m_{n-1}
\\end{array} \\right)
=
\\left( \\begin{array} {c}
6 \\left( y_1 - y_0 \\right) \\\\
6 \\left( y_2 - y_0 \\right) \\\\
6 \\left( y_3 - y_1 \\right) \\\\
\\vdots \\\\
6 \\left( y_{n-1} - y_{n-3} \\right) \\\\
6 \\left( y_{n-1} - y_{n-2} \\right) \\\\
\\end{array} \\right)
\\]
</div>


<p>The derivation above is for uniform (and unit) intervals, where t runs from 0 to 1 on each knot interval.  I'll now discuss the variation where knot intervals are non-uniform.   The non-uniform form of the interpolation equation is:</p>

<div markdown="0">
\\[
y(x) = h_{00}(t) y_j + h_{10}(t) d_j m_j + h_{01}(t) y_{j+1} + h_{11}(t) d_j m_{j+1} \\\\
\\text{ } \\\\
\\text{where:} \\\\
\\text{ }  \\\\
d_j = x_{j+1} - x_j \\text{  for  } j = 0, 1, ... n-2 \\\\
t = (x - x_j) / d_j
\\]
</div>


<p>Taking \( t = t(x) \) and applying the chain rule, we see that 2nd derivative equation now looks like:</p>

<div markdown="0">
\\[
y"(x) = \\frac { \\left( 12 t - 6 \\right) y_{j} + \\left( 6 t - 4 \\right) d_j m_{j}  + \\left( 6 - 12 t \\right) y_{j+1} + \\left( 6 t - 2 \\right) d_j m_{j+1} } { d_j^2 }
\\]
</div>


<p>Applying a derivation similar to the above, we find that our (interior) equations look like this:</p>

<div markdown="0">
\\[
\\frac {2} { d_{j-1} }  m_{j-1} + \\left( \\frac {4} { d_{j-1} } + \\frac {4} { d_j } \\right) m_j + \\frac {2} {d_j} m_{j+1}
=
\\frac { 6 \\left( y_{j+1} - y_{j} \\right) } { d_j^2 } + \\frac { 6 \\left( y_{j} - y_{j-1} \\right) } { d_{j-1}^2 }
\\]
</div>


<p>and natural spline endpoint equations are:</p>

<div markdown="0">
\\[
\\text{left:  } \\frac {4} {d_0} m_0 + \\frac {2} {d_0} m_1   =   \\frac {6 \\left( y_1 - y_0 \\right)} {d_0^2} \\\\
\\text{right: } \\frac {2} {d_{n-2}} m_0 + \\frac {4} {d_{n-2}} m_1   =   \\frac {6 \\left( y_{n-1} - y_{n-2} \\right)} {d_{n-2}^2}
\\]
</div>


<p>And so the matrix equation for specified endpoint gradients is:</p>

<div markdown="0">
\\[
\\scriptsize
\\left( \\begin{array} {ccccc}
\\normalsize 1 \\scriptsize &   &   &   &   \\\\
\\frac{2}{d_0} & \\frac{4}{d_0} {+} \\frac{4}{d_1} & \\frac{2}{d_1} &   &   \\\\
  & \\frac{2}{d_1} & \\frac{4}{d_1} {+} \\frac{4}{d_2} & \\frac{2}{d_2} &   \\\\
  &   & \\vdots &   &   \\\\
  &   & \\frac{2}{d_{n-3}} & \\frac{4}{d_{n-3}} {+} \\frac{4}{d_{n-2}} & \\frac{2}{d_{n-2}} \\\\ 
  &   &   &   & \\normalsize 1 \\scriptsize \\\\
\\end{array} \\right)

\\left( \\begin{array} {c}
m_0 \\\\
m_1 \\\\
 \\\\
\\vdots \\\\
 \\\\
m_{n-1}
\\end{array} \\right)
=
\\left( \\begin{array} {c}
G \\\\
6 \\left( \\frac{y_2 {-} y_1}{d_1^2} {+} \\frac{y_1 {-} y_0}{d_0^2} \\right) \\\\
6 \\left( \\frac{y_3 {-} y_2}{d_2^2} {+} \\frac{y_2 {-} y_1}{d_1^2} \\right)  \\\\
\\vdots \\\\
6 \\left( \\frac{y_{n-1} {-} y_{n-2}}{d_{n-2}^2} {+} \\frac{y_{n-2} {-} y_{n-3}}{d_{n-3}^2} \\right) \\\\
H \\\\
\\end{array} \\right)
\\normalsize
\\]
</div>


<p>And the equation for natural spline endpoints is:</p>

<div markdown="0">
\\[
\\scriptsize
\\left( \\begin{array} {ccccc}
\\frac{4}{d_0} & \\frac{2}{d_0}  &   &   &   \\\\
\\frac {2} {d_0} & \\frac {4} {d_0} {+} \\frac {4} {d_1} & \\frac{2}{d_1} &   &   \\\\
  & \\frac{2}{d_1} & \\frac{4}{d_1} {+} \\frac{4}{d_2} & \\frac{2}{d_2} &   \\\\
  &   & \\vdots &   &   \\\\
  &   & \\frac{2}{d_{n-3}} & \\frac{4}{d_{n-3}} {+} \\frac{4}{d_{n-2}} & \\frac{2}{d_{n-2}} \\\\ 
  &   &   & \\frac{2}{d_{n-2}} & \\frac{4}{d_{n-2}} \\\\
\\end{array} \\right)

\\left( \\begin{array} {c}
m_0 \\\\
m_1 \\\\
 \\\\
\\vdots \\\\
 \\\\
m_{n-1}
\\end{array} \\right)
=
\\left( \\begin{array} {c}
\\frac{6 \\left( y_1 {-} y_0 \\right)}{d_0^2} \\\\
6 \\left( \\frac{y_2 {-} y_1}{d_1^2}  {+}  \\frac{y_1 {-} y_0}{d_0^2} \\right) \\\\
6 \\left( \\frac{y_3 {-} y_2}{d_2^2}  {+}  \\frac{y_2 {-} y_1}{d_1^2} \\right)  \\\\
\\vdots \\\\
6 \\left( \\frac{y_{n-1} {-} y_{n-2}}{d_{n-2}^2}  {+}  \\frac{y_{n-2} {-} y_{n-3}}{d_{n-3}^2} \\right) \\\\
\\frac{6 \\left( y_{n-1} {-} y_{n-2} \\right)}{d_{n-2}^2} \\\\
\\end{array} \\right)
\\normalsize
\\]
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Examining the Modulus of Random Variables]]></title>
    <link href="http://erikerlandson.github.com/blog/2013/03/15/examining-the-modulus-of-random-variables/"/>
    <updated>2013-03-15T12:03:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2013/03/15/examining-the-modulus-of-random-variables</id>
    <content type="html"><![CDATA[<h3>Motivation</h3>

<p>The original motivation for these experiments was consideration of the impact of negotiator cycle cadence (i.e. the time between the start of one cycle and the start of the next) on HTCondor pool loading.  Specifically, any HTCondor job that completes and vacates its resource may leave that resource unloaded until it can be re-matched on the next cycle.  Therefore, the duration of resource vacancies (and hence, pool loading) can be thought of as a function of job durations <em>modulo</em> the cadence of the negotiator cycle.  In general, the aggregate behavior of job durations on a pool is useful to model as a random variable.  And so, it seemed worthwhile to build up a little intuition about the behavior of a random variable when you take its modulus.</p>

<h3>Methodology</h3>

<p>I took a Monte Carlo approach to this study because a tractable theoretical framework eluded me, and you do not have to dive very deep to show that <a href="http://erikerlandson.github.com/blog/2013/01/02/the-mean-of-the-modulus-does-not-equal-the-modulus-of-the-mean/">even trivial random variable behavior under a modulus is dependent on the distribution</a>.   A Monte Carlo framework for the study also allows for other underlying distributions to be easily studied, by altering the random variable being sampled.   In the interest of getting right into results, I'll briefly discuss the tools I used at the end of this post.</p>

<h3>Modulus and Variance</h3>

<p>Consider what happens to a random variable's modulus as its variance increases.  This sequence of plots shows that the modulus of a normal distribution tends toward a uniform distribution over the modulus interval, as the underlying variance increases:</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.20.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.30.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.40.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.50.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<p>From the above plots, we can see that in the case of a normal distribution, its modulus tends toward uniform rather quickly - by the time the underlying variance is half of the modulus interval.</p>

<p>The following plots demonstrate the same effect with a one-tailed distribution (the exponential) -- it requires a larger variance for the effect to manifest.</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_01.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_04.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_10.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_20.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<p>A third example, using a log-normal distribution.   The variance of the log-normal increases as a function of both \( \mu \) and \( \sigma \).  In this example \( \mu \) is increased systematically, holding \( \sigma \) constant at 1:</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_1.0.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.5_1.0.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_1.0_1.0.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_2.0_1.0.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<p>For a final examination of variance, I will again use log-normals and this time vary \( \sigma \), while holding \( \mu \) constant at 0.  Here we see that the effect of increasing the log-normal variance via \( \sigma \) does <em>not</em> follow the pattern in previous examples -- the distribution does not 'spread' and its modulus does not evolve toward a uniform distribution!</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_0.5.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_1.0.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_1.5.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_2.0.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<h3>Modulus and Mean</h3>

<p>The following table of plots demonstrates the decreasing effect that a distribution's location (mean) has, as its spread increases and its modulus approaches uniformity.   In fact, we see that <em>any</em> distribution in the 'uniform modulus' parameter region is indistinguishable from any other, with respect to its modulus -- all changes to mean or variance <em>within</em> this region have no affect on the distribution's modulus!</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.0_0.3.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.5_0.3.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_1.0_0.3.png" width="260" height="260">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.0_0.4.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.5_0.4.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_1.0_0.4.png" width="260" height="260">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.0_0.5.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.5_0.5.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_1.0_0.5.png" width="260" height="260">  |</td>
</tr>
</tbody>
</table>


<br>


<h3>Conclusions</h3>

<p>Generally, as the spread of a distribution increases, its modulus tends toward a uniform distribution on the modulus interval.   Although it was tempting to state this in terms of increasing variance, we see from the 2nd log-normal experiment that variance can increase without increasing 'spread' in a way that causes the trend toward uniform modulus.   Currently, I'm not sure what the true invariant is, that properly distinguishes the 2nd log-normal scenario from the others.</p>

<p>For any distribution that <em>does</em> reside in the 'uniform-modulus' parameter space, we see that neither changes to location nor spread (nor even category of distribution) can be distinguished by the distribution modulus.</p>

<h3>Tools</h3>

<p>I used the following software widgets:</p>

<ul>
<li><a href="https://github.com/erikerlandson/condor_tools/blob/cad8773da36fa7f3c60c93895a428d6f1fae6752/bin/rv_modulus_study">rv_modulus_study</a> -- the jig for Monte Carlo sampling of underlying distributions and their corresponding modulus</li>
<li><a href="https://github.com/erikerlandson/dtools/wiki/dplot">dplot</a> -- a simple cli wrapper around <code>matplotlib.pyplot</code> functionality</li>
<li><a href="https://github.com/willb/capricious/">Capricious</a> -- a library for random sampling of various distribution types</li>
<li><a href="https://github.com/erikerlandson/capricious/blob/c8ec13f1f49880bb3573034de59971f84d15f7c1/lib/capricious/spline_distribution.rb">Capricious::SplineDistribution</a> -- a ruby class for estimating PDF and CDF of a distribution from sampled data, using cubic Hermite splines (note, at the time of this writing, I'm using an experimental variation on my personal repo fork, at the link)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deriving an Incremental Form of the Polynomial Regression Equations]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations/"/>
    <updated>2012-07-05T19:46:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations</id>
    <content type="html"><![CDATA[<p>Incremental, or on-line, algorithms are increasingly popular as data set sizes explode and web enabled applications create environments where new data arrive continuously (that is, incrementally) from clients out on the internet.</p>

<p>Recently I have been doing some <a href="https://github.com/erikerlandson/ratorade">experiments</a> with applying one of the <em>oldest</em> incremental algorithms to the task of rating predictions: computing a linear regression with a coefficient of correlation.  The incremental formulae look like this:</p>

<div markdown="0">
To find coefficients \( a_0, a_1 \) of the linear predictor \( y = a_0 + a_1 x \):
\\[
a_1 = \\frac {n \\Sigma x y - \\Sigma x \\Sigma y} {n \\Sigma x^2 - \\left( \\Sigma x \\right) ^2 }
\\hspace{1 cm}
a_0 = \\frac { \\Sigma y - a_1 \\Sigma x } {n}
\\]
The correlation coefficient of this predictor is given by:
\\[
\\rho (x,y) = \\frac {n \\Sigma x y - \\Sigma x \\Sigma y} {\\sqrt {n \\Sigma x^2 - \\left( \\Sigma x \\right) ^ 2 } \\sqrt {n \\Sigma y^2 - \\left( \\Sigma y \\right) ^ 2 } }
\\]
</div>


<p>As you can see from the formulae above, it is sufficient to maintain running sums</p>

<div markdown="0"> \\[ n, \\Sigma x, \\Sigma y, \\Sigma x^2, \\Sigma y^2, \\Sigma x y \\] </div>


<p>and so any new data can be included incrementally - that is, the model can be updated without revisiting any previous data.</p>

<p>Working with these models caused me to wonder if there was a way to generalize them to obtain incremental formulae for a quadratic predictor, or generalized polynomials.  As it happens, there is.  To show how, I'll derive an incremental formula for the coefficients of the quadratic predictor:</p>

<div markdown="0">
\\[
y = a_0 + a_1 x + a_2 x^2
\\]
</div>


<p>Recall the <a href="http://en.wikipedia.org/wiki/Polynomial_regression#Matrix_form_and_calculation_of_estimates">matrix formula</a> for polynomial regression:</p>

<div markdown="0">
\\[ \\vec{a} = \\left( X^T X \\right) ^ {-1} X^T \\vec{y} \\]

where, in the quadratic case:

\\[
\\vec{a} = \\left( \\begin{array} {c}
a_0 \\\\
a_1 \\\\
a_2 \\\\
\\end{array} \\right)
\\hspace{1 cm}
X = \\left( \\begin{array} {ccc}
1 & x_1 & x_1^2 \\\\
1 & x_2 & x_2^2 \\\\
  &  \vdots  & \\\\
1 & x_n & x_n^2 \\\\
\\end{array} \\right)
\\hspace{1 cm}
\\vec{y} = \\left( \\begin{array} {c}
y_1 \\\\
y_2 \\\\
\vdots \\\\
y_n \\\\
\\end{array} \\right)
\\]

Note that we can apply the definition of matrix multiplication and express the two products \\( X^T X \\) and \\( X^T \\vec{y} \\) from the above formula like so:
\\[
X^T X = 
\\left( \\begin{array} {ccc}
n & \\Sigma x & \\Sigma x^2 \\\\
\\Sigma x & \\Sigma x^2 & \\Sigma x^3 \\\\
\\Sigma x^2 & \\Sigma x^3 & \\Sigma x^4 \\\\
\\end{array} \\right)
\\hspace{1 cm}
X^T \\vec{y} =
\\left( \\begin{array} {c}
\\Sigma y \\\\
\\Sigma x y \\\\
\\Sigma x^2 y \\\\
\\end{array} \\right)
\\]
</div>


<p>And so now we can express the formula for our quadratic coefficients in this way:</p>

<div markdown="0">
\\[
\\left( \\begin{array} {c}
a_0 \\\\
a_1 \\\\
a_2 \\\\
\\end{array} \\right)
=
\\left( \\begin{array} {ccc}
n & \\Sigma x & \\Sigma x^2 \\\\
\\Sigma x & \\Sigma x^2 & \\Sigma x^3 \\\\
\\Sigma x^2 & \\Sigma x^3 & \\Sigma x^4 \\\\
\\end{array} \\right)
^ {-1}
\\left( \\begin{array} {c}
\\Sigma y \\\\
\\Sigma x y \\\\
\\Sigma x^2 y \\\\
\\end{array} \\right)
\\]
</div>


<p>Note that we now have a matrix formula that is expressed entirely in sums of various terms in x and y, which means that it can be maintained incrementally, as we desired.  If you have access to a matrix math package, you might very well declare victory right here, as you can easily construct these matrices and do the matrix arithmetic at will to obtain the model coefficients.  However, as an additional step I applied <a href="http://www.sagemath.org/">sage</a> to do the symbolic matrix inversion and multiplication to give:</p>

<div markdown="0">
\\[
\\small
a_0 =
\\frac {1} {Z}
\\left( 
- \\left( \\Sigma x^3 \\Sigma x - \\left( \\Sigma x^2 \\right)^2 \\right) \\Sigma x^2 y  +  \\left( \\Sigma x^4  \\Sigma x - \\Sigma x^3 \\Sigma x^2 \\right) \\Sigma x y  -  \\left( \\Sigma x^4 \\Sigma x^2 - \\left( \\Sigma x^3 \\right)^2 \\right) \\Sigma y 
\\right)
\\normalsize
\\]
\\[
\\small
a_1 =
\\frac {1} {Z}
\\left( 
\\left( n \\Sigma x^3  - \\Sigma x^2 \\Sigma x \\right) \\Sigma x^2 y  -  \\left( n \\Sigma x^4 - \\left( \\Sigma x^2 \\right) ^2 \\right) \\Sigma x y  +  \\left( \\Sigma x^4 \\Sigma x - \\Sigma x^3 \\Sigma x^2 \\right) \\Sigma y
\\right)
\\normalsize
\\]
\\[
\\small
a_2 =
\\frac {1} {Z}
\\left( 
- \\left( n \\Sigma x^2 - \\left( \\Sigma x \\right) ^2 \\right) \\Sigma x^2 y  +  \\left( n \\Sigma x^3 - \\Sigma x^2 \\Sigma x \\right) \\Sigma x y  -  \\left( \\Sigma x^3 \\Sigma x - \\left( \\Sigma x^2 \\right) ^2 \\right) \\Sigma y 
\\right)
\\normalsize
\\]
where:
\\[
Z = n \\left( \\Sigma x^3 \\right) ^ 2 - 2 \\Sigma x^3 \\Sigma x^2 \\Sigma x + \\left( \\Sigma x^2 \\right) ^3 - \\left( n \\Sigma x^2 - \\left( \\Sigma x \\right) ^2  \\right) \\Sigma x^4
\\]
</div>


<p>Inspecting the quadratic derivation above, it is now fairly easy to see that the general form of the incremental matrix formula for the coefficients of a degree-m polynomial looks like this:</p>

<div markdown="0">
\\[
\\left( \\begin{array} {c}
a_0 \\\\
a_1 \\\\
\vdots \\\\
a_m \\\\
\\end{array} \\right)
=
\\left( \\begin{array} {cccc}
n & \\Sigma x & \\cdots & \\Sigma x^m \\\\
\\Sigma x & \\Sigma x^2 & \\cdots & \\Sigma x^{m+1} \\\\
\\vdots & & \\ddots & \\vdots \\\\
\\Sigma x^m & \\Sigma x^{m+1} & \\cdots & \\Sigma x^{2 m} \\\\
\\end{array} \\right)
^ {-1}
\\left( \\begin{array} {c}
\\Sigma y \\\\
\\Sigma x y \\\\
\\vdots \\\\
\\Sigma x^m y \\\\
\\end{array} \\right)
\\]
</div>


<p>Having an incremental formula for generalized polynomial regression leaves open the question of how one might generalize the correlation coefficient.  There is such a generalization, called the <a href="http://en.wikipedia.org/wiki/Multiple_correlation">coefficient of multiple determination</a>, which is defined:</p>

<div markdown="0">
\\[
r = \\sqrt { \\vec{c} ^ T  R^{-1}  \\vec{c} }
\\]
Where
\\[
\\vec{c} = 
\\left ( \\begin{array} {c}
\\rho (x,y) \\\\
\\rho (x^2,y) \\\\
\\vdots \\\\
\\rho (x^m,y) \\\\
\\end{array} \\right)
\\hspace{1 cm}
R =
\\left( \\begin{array} {cccc}
1 & \\rho (x,x^2) & \\cdots & \\rho(x,x^m) \\\\
\\rho (x^2,x) & 1 & \\cdots & \\rho(x^2,x^m) \\\\
\\vdots & & \\ddots & \\vdots \\\\
\\rho (x^m,x) & \\rho (x^m,x^2) & \\cdots & 1 \\\\
\\end{array} \\right)
\\]
and \\( \\rho (x,y) \\) is the traditional pairwise correlation coefficient.
</div>


<p>But we already have an incremental formula for any pairwise correlation coefficient, which is defined above.  And so we can maintain the running sums needed to fill the matrix entries, and compute the coefficient of multiple determination for our polynomial model at any time.</p>

<p>So we now have incremental formulae to maintain any polynomial model in an on-line environment where we either can't or prefer not to store the data history, and also incrementally evaluate the 'generalized correlation coefficient' for that model.</p>

<p>Readers familiar with linear regression may notice that there is also nothing special about polynomial regression, in the sense that powers of x may also be replaced with arbitrary functions of x, and the same regression equations hold.  And so we might generalize the incremental matrix formulae further to replace products of powers of x with products of functions of x:</p>

<div markdown="0">
for a linear regression model \\( y = a_1 f_1 (x) + a_2 f_2 (x) + \\cdots + a_m f_m(x) \\) :
\\[
\\left( \\begin{array} {c}
a_1 \\\\
a_2 \\\\
\vdots \\\\
a_m \\\\
\\end{array} \\right)
=
\\left( \\begin{array} {cccc}
\\Sigma f_1 (x) f_1 (x) & \\Sigma f_1 (x) f_2 (x) & \\cdots & \\Sigma f_1 (x) f_m (x) \\\\
\\Sigma f_2 (x) f_1 (x) & \\Sigma f_2 (x) f_2 (x) & \\cdots & \\Sigma f_2 (x) f_m (x) \\\\
\\vdots & & \\ddots & \\vdots \\\\
\\Sigma f_m (x) f_1 (x) & \\Sigma f_m (x) f_2 (x) & \\cdots & \\Sigma f_m (x) f_m (x) \\\\
\\end{array} \\right)
^ {-1}
\\left( \\begin{array} {c}
\\Sigma y f_1 (x) \\\\
\\Sigma y f_2 (x) \\\\
\\vdots \\\\
\\Sigma y f_m (x) \\\\
\\end{array} \\right)
\\]
</div>


<p>The coefficient of multiple determination generalizes in the analogous way.</p>
]]></content>
  </entry>
  
</feed>
