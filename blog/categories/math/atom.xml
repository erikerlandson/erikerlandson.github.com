<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: math | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2017-03-06T14:43:30-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Approximating a PDF of Distances With a Gamma Distribution]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution/"/>
    <updated>2016-07-09T11:25:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution</id>
    <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/">previous post</a> I discussed some unintuitive aspects of the distribution of distances as spatial dimension changes.  To help explain this to myself I derived a formula for this distribution, assuming a unit multivariate Gaussian.  For distance (aka radius) r, and spatial dimension d, the PDF of distances is:</p>

<p><img src="/assets/images/dist_dist/gwwv5a5.png" alt="Figure 1" /></p>

<p>Recall that the form of this PDF is the <a href="https://en.wikipedia.org/wiki/Generalized_gamma_distribution">generalized gamma distribution</a>, with scale parameter <nobr>a=sqrt(2),</nobr> shape parameter p=2, and free shape parameter (d) representing the dimensionality.</p>

<p>I was interested in fitting parameters to such a distribution, using some distance data from a clustering algorithm.  <a href="https://www.scipy.org/">SciPy</a> comes with a predefined method for fitting generalized gamma parameters, however I wished to implement something similar using <a href="http://commons.apache.org/proper/commons-math/">Apache Commons Math</a>, which does not have native support for fitting a generalized gamma PDF.  I even went so far as to start working out <a href="http://erikerlandson.github.io/blog/2016/06/15/computing-derivatives-of-the-gamma-function/">some of the math</a> needed to augment the Commons Math <a href="http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/differentiation/package-summary.html">Automatic Differentiation libraries</a> with Gamma function differentiation needed to numerically fit my parameters.</p>

<p>Meanwhile, I have been fitting a <em>non generalized</em> <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> to the distance data, as a sort of rough cut, using a fast <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Maximum_likelihood_estimation">non-iterative approximation</a> to the parameter optimization.  Consistent with my habit of asking the obvious question last, I tried plotting this gamma approximation against distance data, to see how well it compared against the PDF that I derived.</p>

<p>Surprisingly (at least to me), my approximation using the gamma distribution is a very effective fit for spatial dimensionalities <nobr> >= 2 </nobr>:</p>

<p><img src="/assets/images/gamma_approx/approx_plot.png" alt="Figure 2" /></p>

<p>As the plot shows, only for the 1-dimension case is the gamma approximation substiantially deviating.  In fact, the fit appears to get better as dimensionality increases.  To address the 1D case, I can easily test the fit of a half-gaussian as a possible model.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing Derivatives of the Gamma Function]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/06/15/computing-derivatives-of-the-gamma-function/"/>
    <updated>2016-06-15T16:37:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/06/15/computing-derivatives-of-the-gamma-function</id>
    <content type="html"><![CDATA[<p>In this post I'll describe a simple algorithm to compute the kth derivatives of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a>.</p>

<p>I'll start by showing a simple recursion relation for these derivatives, and then gives its derivation.  The kth derivative of Gamma(x) can be computed as follows:</p>

<p><img src="/assets/images/dgamma/hvqtl52.png" alt="Equation 1" /></p>

<p>The recursive formula for the D<sub>k</sub> functions has an easy inductive proof:</p>

<p><img src="/assets/images/dgamma/h79ued9.png" alt="Equation 2" /></p>

<p>Computing the next value D<sub>k</sub> requires knowledge of D<sub>k-1</sub> but also derivative D'<sub>k-1</sub>.  If we start expanding terms, we see the following:</p>

<p><img src="/assets/images/dgamma/hhvonpa.png" alt="Equation 3" /></p>

<p>Continuing the process above it is not hard to see that we can continue expanding until we are left only with terms of <nobr>D<sub>1</sub><sup>(*)</sup>(x);</nobr> that is, various derivatives of <nobr>D<sub>1</sub>(x)</nobr>.  Furthermore, each layer of substitutions adds an order to the derivatives, so that we will eventually be left with terms involving the derivatives of <nobr>D<sub>1</sub>(x)</nobr> up to the (k-1)th derivative. Note that these will all be successive orders of the <a href="https://en.wikipedia.org/wiki/Polygamma_function">polygamma function</a>.</p>

<p>What we want, to do these computations systematically, is a formula for computing the nth derivative of a term <nobr>D<sub>k</sub>(x)</nobr>.  Examining the first few such derivatives suggests a pattern:</p>

<p><img src="/assets/images/dgamma/jqwqpzy.png" alt="Equation 4" /></p>

<p>Generalizing from the above, we see that the formula for the nth derivative is:</p>

<p><img src="/assets/images/dgamma/jamccnh.png" alt="Equation 5" /></p>

<p>We are now in a position to fill in the triangular table of values, culminating in the value of <nobr>D<sub>k</sub>(x):</nobr></p>

<p><img src="/assets/images/dgamma/jj9ph5l.png" alt="Equation 6" /></p>

<p>As previously mentioned, the basis row of values <nobr>D<sub>1</sub><sup>(*)</sup>(x)</nobr> are the <a href="https://en.wikipedia.org/wiki/Polygamma_function">polygamma functions</a> where <nobr>D<sub>1</sub><sup>(n)</sup>(x) = polygamma<sup>(n)</sup>(x)</nobr>.  The first two polygammas, order 0 and 1, are simply the digamma and trigamma functions, respectively, and are available with most numeric libraries.  Computing the general polygamma is a project, and blog post, for another time, but the standard polynomial approximation for the digamma function can of course be differentiated...  Happy Computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploring the Effects of Dimensionality on a PDF of Distances]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/"/>
    <updated>2016-06-08T20:56:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances</id>
    <content type="html"><![CDATA[<p>Every so often I'm reminded that the effects of changing dimensionality on objects and processes can be surprisingly counterintuitive.  Recently I ran across a great example of this, while I working on a model for the distribution of distances in spaces of varying dimension.</p>

<p>Suppose that I draw some values from a classic one-dimensional Gaussian, with zero mean and unit variance, but that I am actually interested in their corresponding distances from center.  Knowing that my Gaussian is centered on the origin, I can rephrase that as: the distribution of magnitudes of values drawn from that Gaussian.  I can simulate this process by actually samping Gaussian values and taking their absolute value.  When I do, I get the following result:</p>

<p><img src="/assets/images/dist_dist/figure1.png" alt="Figure 1" /></p>

<p>It's easy to see -- and intuitive -- that the resulting distribution is a <a href="https://en.wikipedia.org/wiki/Half-normal_distribution">half-Gaussian</a>, as I confirmed by overlaying the histogrammed samples above with a half-Gaussian PDF (displayed in green).</p>

<p>I wanted to generalize this basic idea into some arbitrary dimensionality, (d), where I draw d-vectors from an <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">d-dimensional Gaussian</a> (again, centered on the origin with unit variances). When I take the magnitudes of these sampled d-vectors, what will the probability distribution of <em>their</em> magnitudes look like?</p>

<p>My intuitive assumption was that these magnitudes would <em>also</em> follow a half-Gaussian distribution.  After all, every multivariate Gaussian is densest at its mean, just like the univariate case I examined above.  In fact I was so confident in this assumption that I built my initial modeling around it.  Great confusion ensued, when I saw how poorly my models were working on my higher-dimensional data!</p>

<p>Eventually it occurred to me to do the obvious thing and generate some visualizations from higher dimensional data.  For example, here is the correponding plot generated from a bivariate Gaussian (d=2):</p>

<p><a name="figure2"></a>
<img src="/assets/images/dist_dist/figure2.png" alt="Figure 2" /></p>

<p>Surprise -- the distribution at d=2 is <em>not even close to half-Gaussian!</em>.  My intuitions couldn't have been more misleading!</p>

<p>Where did I go wrong?</p>

<p>I'll start by observing what happens when I take a multi-dimensional PDF of vectors in (d) dimensions and project it down to a one-dimensional PDF of the corresponding vector magnitudes. To keep things simple, I will be assuming a multi-dimensional PDF <nobr>f<sub>r</sub>(<strong>x</strong><sub>d</sub>)</nobr> that is (1) centered on the origin, and (2) is radially symmetric; the pdf value is the same for all points at a given distance from the origin.  For example, any multivariate Gaussian with <strong>0</strong><sub>d</sub> mean and <strong>I</strong><sub>d</sub> for a covariance matrix satisfies these two assumptions.  With this in mind, you can see that the process of projecting from vectors in <strong>R</strong><sub>d</sub> to their distance from <strong>0</strong><sub>d</sub> (their magnitude) is equivalent to summing all densities <nobr>f<sub>r</sub>(<strong>x</strong><sub>d</sub>)</nobr> along the surface of "d-sphere" radius (r) to obtain a pdf f(r) in distance space.  With assumption (2) we can simplify that integration to just <nobr>f(r)=A<sub>d</sub>(r)f'(r)</nobr>, where f'(r) defines the value of <nobr>f<sub>r</sub>(<strong>x</strong>)</nobr> for all <strong>x</strong> with magnitude of (r), and A<sub>d</sub>(r) is the surface area of a d-sphere with radius (r):</p>

<p><img src="/assets/images/dist_dist/ztrlusa.png" alt="Figure 3" /></p>

<p>The key observation is that this term is a <em>polynomial</em> function of radius (r), with degree (d-1).  When d=1, it is simply a constant multiplier and so we get the half-Gaussian distribution we expect, but when <nobr>d >= 2</nobr>, the term is zero at r=0, and grows with radius.  Hence we see the in the <a href="#figure2">d=2 plot above</a> that the density begins at zero, then grows with radius until the decreasing gaussian density gradually drives it back toward zero again.</p>

<p>The above ideas can be expressed compactly as follows:</p>

<p><img src="/assets/images/dist_dist/jukgy85.png" alt="Figure 4" /></p>

<p>In my experiments, I am using multivariate Gaussians of mean <strong>0</strong><sub>d</sub> and unit covariance matrix <strong>I</strong><sub>d</sub>, and so the form for f(r;d) becomes:</p>

<p><img src="/assets/images/dist_dist/gwwv5a5.png" alt="Figure 4" /></p>

<p>This form is in fact the <a href="https://en.wikipedia.org/wiki/Generalized_gamma_distribution">generalized gamma distribution</a>, with scale parameter <nobr>a=2<sup>1/2</sup>,</nobr> shape parameter p=2, and free shape parameter (d) representing the dimensionality in this context.</p>

<p>I can verify that this PDF is correct by plotting it against randomly sampled data at differing dimensions:</p>

<p><img src="/assets/images/dist_dist/figure3.png" alt="Figure 5" /></p>

<p>This plot demonstrates both that the PDF expression is correct for varying dimensionalities and also illustrates how the shape of the PDF evolves as dimensionality changes.  For me, it was a great example of challenging my intuitions and learning something completely unexpected about the interplay of distances and dimension.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measuring Decision Tree Split Quality with Test Statistic P-Values]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/"/>
    <updated>2016-05-26T14:39:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values</id>
    <content type="html"><![CDATA[<p>When training a <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a> learning model (or an <a href="https://en.wikipedia.org/wiki/Random_forest">ensemble</a> of such models) it is often nice to have a policy for deciding when a tree node can no longer be usefully split.  There are a variety possibilities.  For example, halting when node population size becomes smaller than some threshold is a simple and effective policy.  Another approach is to halt when some measure of node purity fails to increase by some minimum threshold.  <strong>The underlying concept is to have some measure of split <em>quality</em>, and to halt when no candidate split has sufficient quality.</strong></p>

<p>In this post I am going to discuss some advantages to one of my favorite approaches to measuring split quality, which is to use a <a href="https://en.wikipedia.org/wiki/Statistical_significance">test statistic significance</a> -- aka "p-value" -- of the null hypothesis that the left and right sub-populations are the same after the split.  The idea is that if a split is of good quality, then it ought to have caused the sub-populations to the left and right of the split to be <em>meaningfully different</em>.  That is to say: the null hypothesis (that they are the same) should be <em>rejected</em> with high confidence, i.e. a small p-value.  What constitutes "small" is always context dependent, but popular p-values from applied statistics are 0.05, 0.01, 0.005, etc.</p>

<blockquote><p>update -- there is now an Apache Spark <a href="https://issues.apache.org/jira/browse/SPARK-15699">JIRA</a> and a <a href="https://github.com/apache/spark/pull/13440">pull request</a> for this feature</p></blockquote>

<p>The remainder of this post is organized in the following sections:</p>

<p><a href="#consistency">Consistency</a> <br>
<a href="#awareness">Awareness of Sample Sizes</a> <br>
<a href="#results">Training Results</a> <br>
<a href="#conclusion">Conclusion</a> <br></p>

<p><a name="consistency"></a></p>

<h5>Consistency</h5>

<p>Test statistic p-values have some appealing properties as a split quality measure.  The test statistic methodology has the advantage of working essentially the same way regardless of the particular test being used.  We begin with two sample populations; in our case, these are the left and right sub-populations created by a candidate split.  We want to assess whether these two populations have the same distribution (the null hypothesis) or different distributions.  We measure some test statistic 'S' (<a href="https://en.wikipedia.org/wiki/Student's_t-test">Student's t</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a>, etc).  We then compute the probability that |S| >= the value we actually measured.  This probability is commonly referred to as the p-value.  The smaller the p-value, the less likely it is that our two populations are the same.  <strong>In our case, we can interpret this as: a smaller p-value indicates a better quality split.</strong></p>

<p>This consistent methodology has a couple advantages contributing to user experience (UX).  If all measures of split quality work in the same way, then there is a lower cognitive load to move between measures once the user understands the common pattern of use.  A second advantage is better "unit analysis."  Since all such quality measures take the form of p-values, there is no risk of a chosen quality measure getting mis-aligned with a corresponding quality threshold.  They are all probabilities, on the interval [0,1], and "smaller threshold" always means "higher threshold of split quality."   By way of comparison, if an application is measuring <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">entropy</a> and then switches to using <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini impurity</a>, these measures are in differing units and care has to be taken that the correct quality threshold is used in each case or the model training policy will be broken.  Switching between differing statistical tests does not come with the same risk.  <strong>A p-value quality threshold will have the same semantic regardless of which statistical test is being applied:</strong> probability that left and right sub-populations are the same, given the particular statistic being measured.</p>

<p><a name="awareness"></a></p>

<h5>Awareness of Sample Size</h5>

<p>Test statistics have another appealing property: many are "aware" of sample size in a way that captures the idea that the smaller the sample size, the larger the difference between populations should be to conclude a given significance.  For one example, consider <a href="https://en.wikipedia.org/wiki/Welch's_t-test#Statistical_test">Welch's t-test</a>, the two-sample variation of the t distribution that applies well to comparing left and right sub populations of candidate decision tree splits:</p>

<p><img src="/assets/images/pval_halting/figure_1.png" alt="Figure 1" /></p>

<p>Visualizing the effects of sample sizes n1 and n2 on these equations directly is a bit tricky, but assuming equal sample sizes and variances allows the equations to be simplified quite a bit, so that we can observe the effect of sample size:</p>

<p><img src="/assets/images/pval_halting/figure_2.png" alt="Figure 2" /></p>

<p>These simplified equations show clearly that (all else remaining equal) as sample size grows smaller, the measured t-statistic correspondingly grows smaller (proportional to sqrt(n)), and furthermore the corresponding variance of the t distribution to be applied grows larger.  For any given shift in left and right sub-populations, each of these trends yields a larger (i.e. weaker) p-value.   This behavior is desirable for a split quality metric.  <strong>The less data there is at a given candidate split, the less confidence there <em>should</em> be in split quality.</strong>  Put another way: we would like to require a larger difference before a split is measured as being good quality when we have less data to work with, and that is exactly the behavior the t-test provides us.</p>

<p><a name="results"></a></p>

<h5>Training Results</h5>

<p>These propreties are pleasing, but it remains to show that test statistics can actually improve decision tree training in practice.  In the following sections I will compare the effects of training with test statstics with other split quality policies based on entropy and gini index.</p>

<p>To conduct these experiments, I modified a <a href="https://github.com/erikerlandson/spark/blob/pval_halting/mllib/src/main/scala/org/apache/spark/mllib/tree/impurity/ChiSquared.scala">local copy</a> of Apache Spark with the <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a> test statistic for comparing categorical distributions.  The demo script, which I ran in <code>spark-shell</code>, can be viewed <a href="https://github.com/erikerlandson/spark/blob/pval_halting/pval_demo">here</a>.</p>

<p>I generated an example data set that represents a two-class learning problem, where labels may be 0 or 1.  Each sample has 10 clean binary features, such that if the bit is 1, the probability of the label is 90% 1 and 10% 0.  There are 5 noise features, also binary, which are completely random.   There are 50 samples of each clean feature being on, for a total of 500 samples.   There are also 500 samples where all clean features are 0 and the corresponding labels are 90% 0 and 10% 1.  The total number of samples in the data set is 1000.  The shape of the data is illustrated by the following table:</p>

<p><code>
truth |     features 0 - 9 (one on at a time)     |   random noise
------+-------------------------------------------+--------------
90% 1 | 1   0   0   0   0   0   0   0   0   0   0 | 1   0   0   1   0
90% 1 |  ... 50 samples with feature 0 'on' ...   |   ... noise ...
90% 1 | 0   1   0   0   0   0   0   0   0   0   0 | 0   1   1   0   0
90% 1 |  ... 50 samples with feature 1 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 2 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 3 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 4 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 5 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 6 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 7 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 8 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 9 'on' ...   |   ... noise ...
90% 0 | 0   0   0   0   0   0   0   0   0   0   0 | 1   1   0   0   1
90% 0 |  ... 500 samples with all 'off  ...       |   ... noise ...
</code></p>

<p>For the first run I use my customized chi-squared statistic as the split quality measure.  I used a p-value threshold of 0.01 -- that is, I would like my chi-squared test to conclude that the probability of left and right split populations are the same is &lt;= 0.01, or that split will not be used.  Note, this means I can expect that around 1% of the time, it will conclude a split was good, when it was just luck.  This is a reasonable false-positive rate; random forests are by nature robust to noise, including noise in their own split decisions:</p>

<p>```
scala> :load pval_demo
Loading pval_demo...
defined module demo</p>

<p>scala> val rf = demo.train("chisquared", 0.01, noise = 0.1)
  pval= 1.578e-09
gain= 20.2669
  pval= 1.578e-09
gain= 20.2669
  pval= 1.578e-09
gain= 20.2669
  pval= 9.140e-09
gain= 18.5106</p>

<p>... more tree p-value demo output ...</p>

<p>  pval= 0.7429
gain= 0.2971
  pval= 0.9287
gain= 0.0740
  pval= 0.2699
gain= 1.3096
rf: org.apache.spark.mllib.tree.model.RandomForestModel =
TreeEnsembleModel classifier with 1 trees</p>

<p>scala> println(rf.trees(0).toDebugString)
DecisionTreeModel classifier of depth 10 with 21 nodes
  If (feature 5 in {1.0})
   Predict: 1.0
  Else (feature 5 not in {1.0})
   If (feature 6 in {1.0})</p>

<pre><code>Predict: 1.0
</code></pre>

<p>   Else (feature 6 not in {1.0})</p>

<pre><code>If (feature 0 in {1.0})
 Predict: 1.0
Else (feature 0 not in {1.0})
 If (feature 1 in {1.0})
  Predict: 1.0
 Else (feature 1 not in {1.0})
  If (feature 2 in {1.0})
   Predict: 1.0
  Else (feature 2 not in {1.0})
   If (feature 8 in {1.0})
    Predict: 1.0
   Else (feature 8 not in {1.0})
    If (feature 3 in {1.0})
     Predict: 1.0
    Else (feature 3 not in {1.0})
     If (feature 4 in {1.0})
      Predict: 1.0
     Else (feature 4 not in {1.0})
      If (feature 7 in {1.0})
       Predict: 1.0
      Else (feature 7 not in {1.0})
       If (feature 9 in {1.0})
        Predict: 1.0
       Else (feature 9 not in {1.0})
        Predict: 0.0
</code></pre>

<p>scala>
```</p>

<p>The first thing to observe is that <strong>the resulting decision tree used exactly the 10 clean features 0 through 9, and none of the five noise features.</strong>   The tree splits off each of the clean features to obtain an optimally accurate leaf-node (one with 90% 1s and 10% 0s).  A second observation is that the p-values shown in the demo output are extremely small (i.e. strong) values -- around 1e-9 (one part in a billion) -- for good-quality splits.  We can also see "weak" p-values with magnitudes such as 0.7, 0.2, etc.  These are poor quality splits on the noise features that it rejects and does not use in the tree, exactly as we hope to see.</p>

<p>Next, I will show a similar run with the standard available "entropy" quality measure, and a minimum gain threshold of 0.035, which is a value I had to determine by trial and error, as what kind of entropy gains one can expect to see, and where to cut them off, is somewhat unintuitive and likely to be very data dependent.</p>

<p>```
scala> val rf = demo.train("entropy", 0.035, noise = 0.1)
  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
gain= 0.0310
  impurity parent= 0.9970, left= 0.1414 (  50), right= 0.9998 ( 950) weighted= 0.9569
gain= 0.0402
  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
gain= 0.0310</p>

<p>... more demo output ...</p>

<p>rf: org.apache.spark.mllib.tree.model.RandomForestModel =
TreeEnsembleModel classifier with 1 trees</p>

<p>scala> println(rf.trees(0).toDebugString)
DecisionTreeModel classifier of depth 11 with 41 nodes
  If (feature 4 in {1.0})
   If (feature 12 in {1.0})</p>

<pre><code>If (feature 11 in {1.0})
 Predict: 1.0
Else (feature 11 not in {1.0})
 Predict: 1.0
</code></pre>

<p>   Else (feature 12 not in {1.0})</p>

<pre><code>Predict: 1.0
</code></pre>

<p>  Else (feature 4 not in {1.0})
   If (feature 1 in {1.0})</p>

<pre><code>If (feature 12 in {1.0})
 Predict: 1.0
Else (feature 12 not in {1.0})
 Predict: 1.0
</code></pre>

<p>   Else (feature 1 not in {1.0})</p>

<pre><code>If (feature 0 in {1.0})
 If (feature 10 in {0.0})
  If (feature 14 in {1.0})
   Predict: 1.0
  Else (feature 14 not in {1.0})
   Predict: 1.0
 Else (feature 10 not in {0.0})
  If (feature 14 in {0.0})
   Predict: 1.0
  Else (feature 14 not in {0.0})
   Predict: 1.0
Else (feature 0 not in {1.0})
 If (feature 6 in {1.0})
  Predict: 1.0
 Else (feature 6 not in {1.0})
  If (feature 3 in {1.0})
   Predict: 1.0
  Else (feature 3 not in {1.0})
   If (feature 7 in {1.0})
    If (feature 13 in {1.0})
     Predict: 1.0
    Else (feature 13 not in {1.0})
     Predict: 1.0
   Else (feature 7 not in {1.0})
    If (feature 2 in {1.0})
     Predict: 1.0
    Else (feature 2 not in {1.0})
     If (feature 8 in {1.0})
      Predict: 1.0
     Else (feature 8 not in {1.0})
      If (feature 9 in {1.0})
       If (feature 11 in {1.0})
        If (feature 13 in {1.0})
         Predict: 1.0
        Else (feature 13 not in {1.0})
         Predict: 1.0
       Else (feature 11 not in {1.0})
        If (feature 12 in {1.0})
         Predict: 1.0
        Else (feature 12 not in {1.0})
         Predict: 1.0
      Else (feature 9 not in {1.0})
       If (feature 5 in {1.0})
        Predict: 1.0
       Else (feature 5 not in {1.0})
        Predict: 0.0
</code></pre>

<p>scala>
```</p>

<p>The first observation is that <strong>the resulting tree using entropy as a split quality measure is twice the size of the tree trained using the chi-squared policy.</strong>  Worse, it is using the noise features -- its quality measure is yielding many more false positives.  The entropy-based model is less parsimonious and will also have performance problems since the model has included very noisy features.</p>

<p>Lastly, I ran a similar training using the "gini" impurity measure, and a 0.015 quality threshold (again, hopefully optimal value that I had to run multiple experiments to identify).  Its quality is better than the entropy-based measure, but this model is still substantially larger than the chi-squared model, and it still uses some noise features:</p>

<p>```
scala> val rf = demo.train("gini", 0.015, noise = 0.1)
  impurity parent= 0.4999, left= 0.2952 (  50), right= 0.4987 ( 950) weighted= 0.4885
gain= 0.0113
  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
gain= 0.0158
  impurity parent= 0.4999, left= 0.1472 (  50), right= 0.4981 ( 950) weighted= 0.4806
gain= 0.0193
  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
gain= 0.0158</p>

<p>... more demo output ...</p>

<p>rf: org.apache.spark.mllib.tree.model.RandomForestModel =
TreeEnsembleModel classifier with 1 trees</p>

<p>scala> println(rf.trees(0).toDebugString)
DecisionTreeModel classifier of depth 12 with 31 nodes
  If (feature 6 in {1.0})
   Predict: 1.0
  Else (feature 6 not in {1.0})
   If (feature 3 in {1.0})</p>

<pre><code>Predict: 1.0
</code></pre>

<p>   Else (feature 3 not in {1.0})</p>

<pre><code>If (feature 1 in {1.0})
 Predict: 1.0
Else (feature 1 not in {1.0})
 If (feature 8 in {1.0})
  Predict: 1.0
 Else (feature 8 not in {1.0})
  If (feature 2 in {1.0})
   If (feature 14 in {0.0})
    Predict: 1.0
   Else (feature 14 not in {0.0})
    Predict: 1.0
  Else (feature 2 not in {1.0})
   If (feature 5 in {1.0})
    Predict: 1.0
   Else (feature 5 not in {1.0})
    If (feature 7 in {1.0})
     Predict: 1.0
    Else (feature 7 not in {1.0})
     If (feature 0 in {1.0})
      If (feature 12 in {1.0})
       If (feature 10 in {0.0})
        Predict: 1.0
       Else (feature 10 not in {0.0})
        Predict: 1.0
      Else (feature 12 not in {1.0})
       Predict: 1.0
     Else (feature 0 not in {1.0})
      If (feature 9 in {1.0})
       Predict: 1.0
      Else (feature 9 not in {1.0})
       If (feature 4 in {1.0})
        If (feature 10 in {0.0})
         Predict: 1.0
        Else (feature 10 not in {0.0})
         If (feature 14 in {0.0})
          Predict: 1.0
         Else (feature 14 not in {0.0})
          Predict: 1.0
       Else (feature 4 not in {1.0})
        Predict: 0.0
</code></pre>

<p>scala>
```</p>

<p><a name="conclusion"></a></p>

<h5>Conclusion</h5>

<p>In this post I have discussed some advantages of using test statstics and p-values as split quality metrics for decision tree training:</p>

<ul>
<li>Consistency</li>
<li>Awareness of sample size</li>
<li>Higher quality model training</li>
</ul>


<p>I believe they are a useful tool for improved training of decision tree models!  Happy computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing Simplex Vertex Locations From Pairwise Object Distances]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/03/26/computing-simplex-vertex-locations-from-pairwise-vertex-distances/"/>
    <updated>2016-03-26T16:22:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/03/26/computing-simplex-vertex-locations-from-pairwise-vertex-distances</id>
    <content type="html"><![CDATA[<p>Suppose I have a collection of (N) objects, and distances d(j,k) between each pair of objects (j) and (k); that is, my objects are members of a <a href="https://en.wikipedia.org/wiki/Metric_space">metric space</a>.  I have no knowledge about my objects, beyond these pair-wise distances.  These objects could be construed as vertices in an (N-1) dimensional <a href="https://en.wikipedia.org/wiki/Simplex">simplex</a>.  However, since I have no spatial information about my objects, I first need a way to assign spatial locations to each object, in vector space R<sup>(N-1),</sup> with only my object distances to work with.</p>

<p>In this post I will derive an algorithm for assigning vertex locations in R<sup>(N-1)</sup> for each of N objects, using only pairwise object distances.</p>

<p>I will assume that N >= 2, since at least two object are required to define a pairwise distance.  The case N=2 is easy, as I can assign vertex 1 to the origin, and vertex 2 to the point d(1,2), to form a 1-simplex (i.e. a line segment) whose single edge is just the distance between the two objects.  I will also assume that my N objects are distinct; that is, each pair has a non-zero distance.</p>

<p>Next consider an arbitrary N, and suppose I have already added vertices 1 through k.  The next vertex (k+1) must obey the pairwise distance relations, as follows:</p>

<p><img src="http://mathurl.com/jm56vxq.png" alt="figure 1" /></p>

<p>Adding the new vertex (k+1) involves adding another dimension (k) to the simplex.  I define this new kth coordinate x(k) to be zero for the existing k vertices, as annotated above; only the new vertex (k+1) will have a non-zero kth coordinate.  Expanding the quadratic terms on the left yields the following form:</p>

<p><img src="http://mathurl.com/jtm7dpq.png" alt="figure 2" /></p>

<p>The squared terms for the coordinates of the new vertex (k+1) are inconvenient, however I can get rid of them by subtracting pairs of equations above.  For example, if I subtract equation 1 from the remaining k-1 equations (2 through k), these squared terms disappear, leaving me with the following system of k-1 equations, which we can see is linear in the 1st k-1 coordinates of the new vertex.  Therefore, I know I'll be able to solve for those coordinates.  I can solve for the remaining kth coordinate by plugging it into the first distance equation:</p>

<p><img src="http://mathurl.com/haovm32.png" alt="figure 3" /></p>

<p>To clarify matters, the equations above can be re-written as the following matrix equation, solveable by any linear systems library:</p>

<p><img src="http://mathurl.com/h6qdtms.png" alt="figure 4" /></p>

<p>This gives me a recusion relation for adding a new vertex (k+1), given that I have already added the first k vertices.  The basis case of adding the first two vertices was already described above.  And so I can iteratively add all my vertices one at a time by applying the recursion relation.</p>

<p>As a corollary, assume that I have constructed a simplex having k vertices, as shown above, and I would like to assign a spatial location to a new object, (y), given its k distances to each vertex.  The corresponding distance relations are given by:</p>

<p><img src="http://mathurl.com/zdw9uv8.png" alt="figure 5" /></p>

<p>I can apply a derivation very similar to the one above, to obtain the following linear equation for the (k-1) coordinates of (y):</p>

<p><img src="http://mathurl.com/zvr5jre.png" alt="figure 6" /></p>
]]></content>
  </entry>
  
</feed>
