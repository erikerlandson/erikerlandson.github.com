<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: statistics | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2016-06-15T19:55:05-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Measuring Decision Tree Split Quality with Test Statistic P-Values]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/"/>
    <updated>2016-05-26T14:39:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values</id>
    <content type="html"><![CDATA[<p>When training a <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a> learning model (or an <a href="https://en.wikipedia.org/wiki/Random_forest">ensemble</a> of such models) it is often nice to have a policy for deciding when a tree node can no longer be usefully split.  There are a variety possibilities.  For example, halting when node population size becomes smaller than some threshold is a simple and effective policy.  Another approach is to halt when some measure of node purity fails to increase by some minimum threshold.  <strong>The underlying concept is to have some measure of split <em>quality</em>, and to halt when no candidate split has sufficient quality.</strong></p>

<p>In this post I am going to discuss some advantages to one of my favorite approaches to measuring split quality, which is to use a <a href="https://en.wikipedia.org/wiki/Statistical_significance">test statistic significance</a> -- aka "p-value" -- of the null hypothesis that the left and right sub-populations are the same after the split.  The idea is that if a split is of good quality, then it ought to have caused the sub-populations to the left and right of the split to be <em>meaningfully different</em>.  That is to say: the null hypothesis (that they are the same) should be <em>rejected</em> with high confidence, i.e. a small p-value.  What constitutes "small" is always context dependent, but popular p-values from applied statistics are 0.05, 0.01, 0.005, etc.</p>

<blockquote><p>update -- there is now an Apache Spark <a href="https://issues.apache.org/jira/browse/SPARK-15699">JIRA</a> and a <a href="https://github.com/apache/spark/pull/13440">pull request</a> for this feature</p></blockquote>

<p>The remainder of this post is organized in the following sections:</p>

<p><a href="#consistency">Consistency</a> <br>
<a href="#awareness">Awareness of Sample Sizes</a> <br>
<a href="#results">Training Results</a> <br>
<a href="#conclusion">Conclusion</a> <br></p>

<p><a name="consistency"></a></p>

<h5>Consistency</h5>

<p>Test statistic p-values have some appealing properties as a split quality measure.  The test statistic methodology has the advantage of working essentially the same way regardless of the particular test being used.  We begin with two sample populations; in our case, these are the left and right sub-populations created by a candidate split.  We want to assess whether these two populations have the same distribution (the null hypothesis) or different distributions.  We measure some test statistic 'S' (<a href="https://en.wikipedia.org/wiki/Student's_t-test">Student's t</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a>, etc).  We then compute the probability that |S| >= the value we actually measured.  This probability is commonly referred to as the p-value.  The smaller the p-value, the less likely it is that our two populations are the same.  <strong>In our case, we can interpret this as: a smaller p-value indicates a better quality split.</strong></p>

<p>This consistent methodology has a couple advantages contributing to user experience (UX).  If all measures of split quality work in the same way, then there is a lower cognitive load to move between measures once the user understands the common pattern of use.  A second advantage is better "unit analysis."  Since all such quality measures take the form of p-values, there is no risk of a chosen quality measure getting mis-aligned with a corresponding quality threshold.  They are all probabilities, on the interval [0,1], and "smaller threshold" always means "higher threshold of split quality."   By way of comparison, if an application is measuring <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">entropy</a> and then switches to using <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini impurity</a>, these measures are in differing units and care has to be taken that the correct quality threshold is used in each case or the model training policy will be broken.  Switching between differing statistical tests does not come with the same risk.  <strong>A p-value quality threshold will have the same semantic regardless of which statistical test is being applied:</strong> probability that left and right sub-populations are the same, given the particular statistic being measured.</p>

<p><a name="awareness"></a></p>

<h5>Awareness of Sample Size</h5>

<p>Test statistics have another appealing property: many are "aware" of sample size in a way that captures the idea that the smaller the sample size, the larger the difference between populations should be to conclude a given significance.  For one example, consider <a href="https://en.wikipedia.org/wiki/Welch's_t-test#Statistical_test">Welch's t-test</a>, the two-sample variation of the t distribution that applies well to comparing left and right sub populations of candidate decision tree splits:</p>

<p><img src="/assets/images/pval_halting/figure_1.png" alt="Figure 1" /></p>

<p>Visualizing the effects of sample sizes n1 and n2 on these equations directly is a bit tricky, but assuming equal sample sizes and variances allows the equations to be simplified quite a bit, so that we can observe the effect of sample size:</p>

<p><img src="/assets/images/pval_halting/figure_2.png" alt="Figure 2" /></p>

<p>These simplified equations show clearly that (all else remaining equal) as sample size grows smaller, the measured t-statistic correspondingly grows smaller (proportional to sqrt(n)), and furthermore the corresponding variance of the t distribution to be applied grows larger.  For any given shift in left and right sub-populations, each of these trends yields a larger (i.e. weaker) p-value.   This behavior is desirable for a split quality metric.  <strong>The less data there is at a given candidate split, the less confidence there <em>should</em> be in split quality.</strong>  Put another way: we would like to require a larger difference before a split is measured as being good quality when we have less data to work with, and that is exactly the behavior the t-test provides us.</p>

<p><a name="results"></a></p>

<h5>Training Results</h5>

<p>These propreties are pleasing, but it remains to show that test statistics can actually improve decision tree training in practice.  In the following sections I will compare the effects of training with test statstics with other split quality policies based on entropy and gini index.</p>

<p>To conduct these experiments, I modified a <a href="https://github.com/erikerlandson/spark/blob/pval_halting/mllib/src/main/scala/org/apache/spark/mllib/tree/impurity/ChiSquared.scala">local copy</a> of Apache Spark with the <a href="https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data">Chi-Squared</a> test statistic for comparing categorical distributions.  The demo script, which I ran in <code>spark-shell</code>, can be viewed <a href="https://github.com/erikerlandson/spark/blob/pval_halting/pval_demo">here</a>.</p>

<p>I generated an example data set that represents a two-class learning problem, where labels may be 0 or 1.  Each sample has 10 clean binary features, such that if the bit is 1, the probability of the label is 90% 1 and 10% 0.  There are 5 noise features, also binary, which are completely random.   There are 50 samples of each clean feature being on, for a total of 500 samples.   There are also 500 samples where all clean features are 0 and the corresponding labels are 90% 0 and 10% 1.  The total number of samples in the data set is 1000.  The shape of the data is illustrated by the following table:</p>

<p><code>
truth |     features 0 - 9 (one on at a time)     |   random noise
------+-------------------------------------------+--------------
90% 1 | 1   0   0   0   0   0   0   0   0   0   0 | 1   0   0   1   0
90% 1 |  ... 50 samples with feature 0 'on' ...   |   ... noise ...
90% 1 | 0   1   0   0   0   0   0   0   0   0   0 | 0   1   1   0   0
90% 1 |  ... 50 samples with feature 1 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 2 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 3 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 4 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 5 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 6 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 7 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 8 'on' ...   |   ... noise ...
90% 1 |  ... 50 samples with feature 9 'on' ...   |   ... noise ...
90% 0 | 0   0   0   0   0   0   0   0   0   0   0 | 1   1   0   0   1
90% 0 |  ... 500 samples with all 'off  ...       |   ... noise ...
</code></p>

<p>For the first run I use my customized chi-squared statistic as the split quality measure.  I used a p-value threshold of 0.01 -- that is, I would like my chi-squared test to conclude that the probability of left and right split populations are the same is &lt;= 0.01, or that split will not be used.  Note, this means I can expect that around 1% of the time, it will conclude a split was good, when it was just luck.  This is a reasonable false-positive rate; random forests are by nature robust to noise, including noise in their own split decisions:</p>

<p>```
scala> :load pval_demo
Loading pval_demo...
defined module demo</p>

<p>scala> val rf = demo.train("chisquared", 0.01, noise = 0.1)
  pval= 1.578e-09
gain= 20.2669
  pval= 1.578e-09
gain= 20.2669
  pval= 1.578e-09
gain= 20.2669
  pval= 9.140e-09
gain= 18.5106</p>

<p>... more tree p-value demo output ...</p>

<p>  pval= 0.7429
gain= 0.2971
  pval= 0.9287
gain= 0.0740
  pval= 0.2699
gain= 1.3096
rf: org.apache.spark.mllib.tree.model.RandomForestModel =
TreeEnsembleModel classifier with 1 trees</p>

<p>scala> println(rf.trees(0).toDebugString)
DecisionTreeModel classifier of depth 10 with 21 nodes
  If (feature 5 in {1.0})
   Predict: 1.0
  Else (feature 5 not in {1.0})
   If (feature 6 in {1.0})</p>

<pre><code>Predict: 1.0
</code></pre>

<p>   Else (feature 6 not in {1.0})</p>

<pre><code>If (feature 0 in {1.0})
 Predict: 1.0
Else (feature 0 not in {1.0})
 If (feature 1 in {1.0})
  Predict: 1.0
 Else (feature 1 not in {1.0})
  If (feature 2 in {1.0})
   Predict: 1.0
  Else (feature 2 not in {1.0})
   If (feature 8 in {1.0})
    Predict: 1.0
   Else (feature 8 not in {1.0})
    If (feature 3 in {1.0})
     Predict: 1.0
    Else (feature 3 not in {1.0})
     If (feature 4 in {1.0})
      Predict: 1.0
     Else (feature 4 not in {1.0})
      If (feature 7 in {1.0})
       Predict: 1.0
      Else (feature 7 not in {1.0})
       If (feature 9 in {1.0})
        Predict: 1.0
       Else (feature 9 not in {1.0})
        Predict: 0.0
</code></pre>

<p>scala>
```</p>

<p>The first thing to observe is that <strong>the resulting decision tree used exactly the 10 clean features 0 through 9, and none of the five noise features.</strong>   The tree splits off each of the clean features to obtain an optimally accurate leaf-node (one with 90% 1s and 10% 0s).  A second observation is that the p-values shown in the demo output are extremely small (i.e. strong) values -- around 1e-9 (one part in a billion) -- for good-quality splits.  We can also see "weak" p-values with magnitudes such as 0.7, 0.2, etc.  These are poor quality splits on the noise features that it rejects and does not use in the tree, exactly as we hope to see.</p>

<p>Next, I will show a similar run with the standard available "entropy" quality measure, and a minimum gain threshold of 0.035, which is a value I had to determine by trial and error, as what kind of entropy gains one can expect to see, and where to cut them off, is somewhat unintuitive and likely to be very data dependent.</p>

<p>```
scala> val rf = demo.train("entropy", 0.035, noise = 0.1)
  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
gain= 0.0310
  impurity parent= 0.9970, left= 0.1414 (  50), right= 0.9998 ( 950) weighted= 0.9569
gain= 0.0402
  impurity parent= 0.9970, left= 0.3274 (  50), right= 0.9997 ( 950) weighted= 0.9661
gain= 0.0310</p>

<p>... more demo output ...</p>

<p>rf: org.apache.spark.mllib.tree.model.RandomForestModel =
TreeEnsembleModel classifier with 1 trees</p>

<p>scala> println(rf.trees(0).toDebugString)
DecisionTreeModel classifier of depth 11 with 41 nodes
  If (feature 4 in {1.0})
   If (feature 12 in {1.0})</p>

<pre><code>If (feature 11 in {1.0})
 Predict: 1.0
Else (feature 11 not in {1.0})
 Predict: 1.0
</code></pre>

<p>   Else (feature 12 not in {1.0})</p>

<pre><code>Predict: 1.0
</code></pre>

<p>  Else (feature 4 not in {1.0})
   If (feature 1 in {1.0})</p>

<pre><code>If (feature 12 in {1.0})
 Predict: 1.0
Else (feature 12 not in {1.0})
 Predict: 1.0
</code></pre>

<p>   Else (feature 1 not in {1.0})</p>

<pre><code>If (feature 0 in {1.0})
 If (feature 10 in {0.0})
  If (feature 14 in {1.0})
   Predict: 1.0
  Else (feature 14 not in {1.0})
   Predict: 1.0
 Else (feature 10 not in {0.0})
  If (feature 14 in {0.0})
   Predict: 1.0
  Else (feature 14 not in {0.0})
   Predict: 1.0
Else (feature 0 not in {1.0})
 If (feature 6 in {1.0})
  Predict: 1.0
 Else (feature 6 not in {1.0})
  If (feature 3 in {1.0})
   Predict: 1.0
  Else (feature 3 not in {1.0})
   If (feature 7 in {1.0})
    If (feature 13 in {1.0})
     Predict: 1.0
    Else (feature 13 not in {1.0})
     Predict: 1.0
   Else (feature 7 not in {1.0})
    If (feature 2 in {1.0})
     Predict: 1.0
    Else (feature 2 not in {1.0})
     If (feature 8 in {1.0})
      Predict: 1.0
     Else (feature 8 not in {1.0})
      If (feature 9 in {1.0})
       If (feature 11 in {1.0})
        If (feature 13 in {1.0})
         Predict: 1.0
        Else (feature 13 not in {1.0})
         Predict: 1.0
       Else (feature 11 not in {1.0})
        If (feature 12 in {1.0})
         Predict: 1.0
        Else (feature 12 not in {1.0})
         Predict: 1.0
      Else (feature 9 not in {1.0})
       If (feature 5 in {1.0})
        Predict: 1.0
       Else (feature 5 not in {1.0})
        Predict: 0.0
</code></pre>

<p>scala>
```</p>

<p>The first observation is that <strong>the resulting tree using entropy as a split quality measure is twice the size of the tree trained using the chi-squared policy.</strong>  Worse, it is using the noise features -- its quality measure is yielding many more false positives.  The entropy-based model is less parsimonious and will also have performance problems since the model has included very noisy features.</p>

<p>Lastly, I ran a similar training using the "gini" impurity measure, and a 0.015 quality threshold (again, hopefully optimal value that I had to run multiple experiments to identify).  Its quality is better than the entropy-based measure, but this model is still substantially larger than the chi-squared model, and it still uses some noise features:</p>

<p>```
scala> val rf = demo.train("gini", 0.015, noise = 0.1)
  impurity parent= 0.4999, left= 0.2952 (  50), right= 0.4987 ( 950) weighted= 0.4885
gain= 0.0113
  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
gain= 0.0158
  impurity parent= 0.4999, left= 0.1472 (  50), right= 0.4981 ( 950) weighted= 0.4806
gain= 0.0193
  impurity parent= 0.4999, left= 0.2112 (  50), right= 0.4984 ( 950) weighted= 0.4840
gain= 0.0158</p>

<p>... more demo output ...</p>

<p>rf: org.apache.spark.mllib.tree.model.RandomForestModel =
TreeEnsembleModel classifier with 1 trees</p>

<p>scala> println(rf.trees(0).toDebugString)
DecisionTreeModel classifier of depth 12 with 31 nodes
  If (feature 6 in {1.0})
   Predict: 1.0
  Else (feature 6 not in {1.0})
   If (feature 3 in {1.0})</p>

<pre><code>Predict: 1.0
</code></pre>

<p>   Else (feature 3 not in {1.0})</p>

<pre><code>If (feature 1 in {1.0})
 Predict: 1.0
Else (feature 1 not in {1.0})
 If (feature 8 in {1.0})
  Predict: 1.0
 Else (feature 8 not in {1.0})
  If (feature 2 in {1.0})
   If (feature 14 in {0.0})
    Predict: 1.0
   Else (feature 14 not in {0.0})
    Predict: 1.0
  Else (feature 2 not in {1.0})
   If (feature 5 in {1.0})
    Predict: 1.0
   Else (feature 5 not in {1.0})
    If (feature 7 in {1.0})
     Predict: 1.0
    Else (feature 7 not in {1.0})
     If (feature 0 in {1.0})
      If (feature 12 in {1.0})
       If (feature 10 in {0.0})
        Predict: 1.0
       Else (feature 10 not in {0.0})
        Predict: 1.0
      Else (feature 12 not in {1.0})
       Predict: 1.0
     Else (feature 0 not in {1.0})
      If (feature 9 in {1.0})
       Predict: 1.0
      Else (feature 9 not in {1.0})
       If (feature 4 in {1.0})
        If (feature 10 in {0.0})
         Predict: 1.0
        Else (feature 10 not in {0.0})
         If (feature 14 in {0.0})
          Predict: 1.0
         Else (feature 14 not in {0.0})
          Predict: 1.0
       Else (feature 4 not in {1.0})
        Predict: 0.0
</code></pre>

<p>scala>
```</p>

<p><a name="conclusion"></a></p>

<h5>Conclusion</h5>

<p>In this post I have discussed some advantages of using test statstics and p-values as split quality metrics for decision tree training:</p>

<ul>
<li>Consistency</li>
<li>Awareness of sample size</li>
<li>Higher quality model training</li>
</ul>


<p>I believe they are a useful tool for improved training of decision tree models!  Happy computing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generalizing Kendall's Tau]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau/"/>
    <updated>2015-08-14T14:35:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau</id>
    <content type="html"><![CDATA[<p>Recently I have been applying <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient">Kendall's Tau</a> as an evaluation metric to assess how well a regression model ranks input samples, with respect to a known correct ranking.</p>

<p>The process of implementing the Kendall's Tau statistic, with my software engineer's hat on, caused me to reflect a bit on how it could be generalized beyond the traditional application of ranking numeric pairs.  In this post I'll discuss the generalization of Kendall's Tau to non-numeric data, and also generalizing from totally ordered data to partial orderings.</p>

<h5>A Review of Kendall's Tau</h5>

<p>I'll start with a brief review of Kendall's Tau.  For more depth, a good place to start is the Wikipedia article at the link above.</p>

<p>Consider a sequence of (n) observations where each observation is a pair (x,y), where we wish to measure how well a ranking by x-values agrees with a ranking by the y-values.  Informally, Kendall's Tau (aka the Kendall Rank Correlation Coefficient) is the difference between number of observation-pairs (pairs of pairs, if you will) whose ordering <em>agrees</em> ("concordant" pairs) and the number of such pairs whose ordering <em>disagrees</em> ("discordant" pairs).  This difference is divided by the total number of observation pairs.</p>

<p>The commonly-used formulation of Kendall's Tau is the "Tau-B" statistic, which accounts for observed pairs having tied values in either x or y as being neither concordant nor discordant:</p>

<h6>Figure 1: Kendall's Tau-B</h6>

<p><img src="/assets/images/kendalls_tau/figure_1.png" title="Kendall's Tau" alt="Kendall's Tau" /></p>

<p>The formulation above has quadratic complexity, with respect to data size (n).  It is possible to rearrange this computation in a way that can be computed in (n)log(n) time[1]:</p>

<h6>Figure 2: An (n)log(n) formulation of Kendall's Tau-B</h6>

<p><img src="/assets/images/kendalls_tau/figure_2.png" title="Kendall's Tau" alt="Kendall's Tau" /></p>

<p>The details of performing this computation can be found at [1] or on the <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Algorithms">Wikipedia entry</a>.  For my purposes, I'll note that it requires two (n)log(n) sorts of the data, which becomes relevant below.</p>

<h5>Generalizing to Non-Numeric Values</h5>

<p>Generalizing Kendall's Tau to non-numeric values is mostly just making the observation that the definition of "concordant" and "discordant" pairs is purely based on comparing x-values and y-values (and, in the (n)log(n) formulation, performing sorts on the data).  From the software engineer's perspective this means that the computations are well defined on any data type with an ordering relation, which includes numeric types but also chars, strings, sequences of any element supporting an ordering, etc.  Significantly, most programming languages support the concept of defining ordering relations on arbitrary data types, which means that <em><em>Kendall's Tau can, in principle, be computed on literally any kind of data structure</em></em>, provided you supply it with a well defined ordering.  Furthermore, an examination of the algorithms shows that values of x and y need not even be of the same type, nor do they require the same ordering.</p>

<h5>Generalizing to Partial Orderings</h5>

<p>When I brought this observation up, my colleague <a href="http://chapeau.freevariable.com/">Will Benton</a> asked the very interesting question of whether it's also possible to compute Kendall's Tau on objects that have only a <em>partial ordering</em>.  It turns out that you <em><em>can</em></em> define Kendall's Tau on partially ordered data, by defining the case of two non-comparable x-values, or y-values, as another kind of tie.</p>

<p>The big caveat with this definition is that the (n)log(n) optimization does not apply.  Firstly, the optimized algorithm relies heavily on (n)log(n) sorting, and there is no unique full sorting of elements that are only partially ordered.  Secondly, the formula's definition of the quantities n1, n2 and n3 is founded on the assumption that element equality is transitive; this is why you can count a number of tied values, t, and use t(t-1)/2 as the corresponding number of tied pairs.  But in a partial ordering, this assumption is violated. Consider the case where (a) &lt; (b), but (a) is non-comparable to (c) and (b) is also non-comparable to (c).  By our definition, (a) is tied with (c), and (c) is tied with (b), but transitivity is violated, as (a) &lt; (b).</p>

<p>So how <em>can</em> we compute Tau in this case?  Consider (n1) and (n2), in Figure-1.  These values represent the number of pairs that were tied wrt (x) and (y), respectively.  We can't use the shortcut formulas for (n1) and (n2), but we can count them directly, pair by pair, simply by conducting the traditional quadratic iteration over pairs, and incrementing (n1) whenever two x-values are noncomparable, and incrementing (n2) whenever two y-values are non-comparable, just as we increment (nc) and (nd) to count concordant and discordant pairs.  With this modification, we can apply the formula in Figure-1 as-is.</p>

<h5>Conclusions</h5>

<p>I made these observations without any particular application in mind. However, my instincts as a software engineer tell me that making generalizations in this way often paves the way for new ideas, once the generalized concept is made available.  With luck, it will inspire either me or somebody else to apply Kendall's Tau in interesting new ways.</p>

<h5>References</h5>

<p>[1] Knight, W. (1966). "A Computer Method for Calculating Kendall's Tau with Ungrouped Data". Journal of the American Statistical Association 61 (314): 436–439. doi:10.2307/2282833. JSTOR 2282833.</p>
]]></content>
  </entry>
  
</feed>
