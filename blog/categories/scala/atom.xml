<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: scala | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2018-06-03T20:23:54-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    <email><![CDATA[erikerlandson@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Encoding Map-Reduce As A Monoid With Left Folding]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid/"/>
    <updated>2016-09-05T10:31:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid</id>
    <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/">previous post</a> I discussed some scenarios where traditional map-reduce (directly applying a map function, followed by some monoidal reduction) could be inefficient.
To review, the source of inefficiency is in situations where the <code>map</code> operation is creating some non-trivial monoid that represents a single element of the input type.
For example, if the monoidal type is <code>Set[Int]</code>, then the mapping function ('prepare' in algebird) maps every input integer <code>k</code> into <code>Set(k)</code>, which is somewhat expensive.</p>

<p>In that discussion, I was focusing on map-reduce as embodied by the algebird <code>Aggregator</code> type, where <code>map</code> appears as the <code>prepare</code> function.
However, it is easy to see that <em>any</em> map-reduce implementation may be vulnerable to the same inefficiency.</p>

<p>I wondered if there were a way to represent map-reduce using some alternative formulation that avoids this vulnerability.
There is such a formulation, which I will talk about in this post.</p>

<p>I'll begin by reviewing a standard map-reduce implementation.
The following scala code sketches out the definition of a monoid over a type <code>B</code> and a map-reduce interface.
As this code suggests, the <code>map</code> function maps input data of some type <code>A</code> into some <em>monoidal</em> type <code>B</code>, which can be reduced (aka "aggregated") in a way that is amenable to parallelization:</p>

<p>``` scala
trait Monoid[B] {
  // aka 'combine' aka '++'
  def plus: (B, B) => B</p>

<p>  // aka 'empty' aka 'identity'
  def e: B
}</p>

<p>trait MapReduce[A, B] {
  // monoid embodies the reducible type
  def monoid: Monoid[B]</p>

<p>  // mapping function from input type A to reducible type B
  def map: A => B</p>

<p>  // the basic map-reduce operation
  def apply(data: Seq[A]): B = data.map(map).fold(monoid.e)(monoid.plus)</p>

<p>  // map-reduce parallelized over data partitions
  def apply(data: ParSeq[Seq[A]]): B =</p>

<pre><code>data.map { part =&gt;
  part.map(map).fold(monoid.e)(monoid.plus)
}
.fold(monoid.e)(monoid.plus)
</code></pre>

<p>}
```</p>

<p>In the parallel version of map-reduce above, you can see that map and reduce are executed on each data partition (which may occur in parallel) to produce a monoidal <code>B</code> value, followed by a final reduction of those intermediate results.
This is the classic form of map-reduce popularized by tools such as Hadoop and Apache Spark, where inidividual data partitions may reside across highly parallel commodity clusters.</p>

<p>Next I will present an alternative definition of map-reduce.
In this implementation, the <code>map</code> function is replaced by a <code>foldL</code> function, which executes a single "left-fold" of an input object with type <code>A</code> into the monoid object with type <code>B</code>:</p>

<p>``` scala
// a map reduce operation based on a monoid with left folding
trait MapReduceLF[A, B] extends MapReduce[A, B] {
  def monoid: Monoid[B]</p>

<p>  // left-fold an object with type A into the monoid B
  // obeys type law: foldL(b, a) = b ++ foldL(e, a)
  def foldL: (B, A) => B</p>

<p>  // foldL(e, a) embodies the role of map(a) in standard map-reduce
  def map = (a: A) => foldL(monoid.e, a)</p>

<p>  // map-reduce operation is now a single fold-left operation
  override def apply(data: Seq[A]): B = data.foldLeft(monoid.e)(foldL)</p>

<p>  // map-reduce parallelized over data partitions
  override def apply(data: ParSeq[Seq[A]]): B =</p>

<pre><code>data.map { part =&gt;
  part.foldLeft(monoid.e)(foldL)
}
.fold(monoid.e)(monoid.plus)
</code></pre>

<p>}
```</p>

<p>As the comments above indicate, the left-folding function <code>foldL</code> is assumed to obey the law <code>foldL(b, a) = b ++ foldL(e, a)</code>.
This law captures the idea that folding <code>a</code> into <code>b</code> should be the analog of reducing <code>b</code> with a monoid corresponding to the single element <code>a</code>.
Referring to my earlier example, if type <code>A</code> is <code>Int</code> and <code>B</code> is <code>Set[Int]</code>, then <code>foldL(b, a) =&gt; b + a</code>.
Note that <code>b + a</code> is directly inserting single element <code>a</code> into <code>b</code>, which is significantly more efficient than <code>b ++ Set(a)</code>, which is how a typical map-reduce implementation would be required to operate.</p>

<p>This law also gives us the corresponding definition of <code>map(a)</code>, which is <code>foldL(e, a)</code>, or in my example: <code>Set.empty[Int] ++ a</code> or just: <code>Set(a)</code></p>

<p>In this formulation, the basic map-reduce operation is now a single <code>foldLeft</code> operation, instead of a mapping followed by a monoidal reduction.
The parallel version is analoglous.
Each partition uses the new <code>foldLeft</code> operation, and the final reduction of intermediate monoidal results remains the same as before.</p>

<p>The <code>foldLeft</code> function is potentially a much more general operation, and it raises the question of whether this new encoding is indeed parallelizable as before.
I will conclude with a proof that this encoding is also parallelizable;
Note that the law <code>foldL(b, a) = b ++ foldL(e, a)</code> is a significant component of this proof, as it represents the constraint that <code>foldL</code> behaves like an analog of reducing <code>b</code> with a monoidal representation of element <code>a</code>.</p>

<p>In the following proof I used a scala-like pseudo code, described in the introduction:</p>

<p>```
// given an object mr of type MapReduceFL[A, B]
// and using notation:
// f &lt;==> mr.foldL
// for b1,b2 of type B: b1 ++ b2 &lt;==> mr.plus(b1, b2)
// e &lt;==> mr.e
// [...] &lt;==> Seq(...)
// d1, d2 are of type Seq[A]</p>

<p>// Proof that map-reduce with left-folding is parallelizable
// i.e. mr(d1 ++ d2) == mr(d1) ++ mr(d2)
mr(d1 ++ d2)
== (d1 ++ d2).foldLeft(e)(f)  // definition of map-reduce operation
== d1.foldLeft(e)(f) ++ d2.foldLeft(e)(f)  // Lemma A
== mr(d1) ++ mr(d2)  // definition of map-reduce (QED)</p>

<p>// Proof of Lemma A
// i.e. (d1 ++ d2).foldLeft(e)(f) == d1.foldLeft(e)(f) ++ d2.foldLeft(e)(f)</p>

<p>// proof is by induction on the length of data sequence d2</p>

<p>// case d2 where length is zero, i.e. d2 == []
(d1 ++ []).foldLeft(e)(f)
== d1.foldLeft(e)(f)  // definition of empty sequence []
== d1.foldLeft(e)(f) ++ e  // definition of identity e
== d1.foldLeft(e)(f) ++ [].foldLeft(e)(f)  // definition of foldLeft</p>

<p>// case d2 where length is 1, i.e. d2 == [a] for some a of type A
(d1 ++ [a]).foldLeft(e)(f)
== f(d1.foldLeft(e)(f), a)  // definition of foldLeft and f
== d1.foldLeft(e)(f) ++ f(e, a)  // the type-law f(b, a) == b ++ f(e, a)
== d1.foldLeft(e)(f) ++ [a].foldLeft(e)(f)  // definition of foldLeft</p>

<p>// inductive step, assuming proof for d2' of length &lt;= n
// consider d2 of length n+1, i.e. d2 == d2' ++ [a], where d2' has length n
(d1 ++ d2).foldLeft(e)(f)
== (d1 ++ d2' ++ [a]).foldLeft(e)(f)  // definition of d2, d2', [a]
== f((d1 ++ d2').foldLeft(e)(f), a)  // definition of foldLeft and f
== (d1 ++ d2').foldLeft(e)(f) ++ f(e, a)  // type-law f(b, a) == b ++ f(e, a)
== d1.foldLeft(e)(f) ++ d2'.foldLeft(e)(f) ++ f(e, a)  // induction
== d1.foldLeft(e)(f) ++ d2'.foldLeft(e)(f) ++ [a].foldLeft(e)(f)  // def'n of foldLeft
== d1.foldLeft(e)(f) ++ (d2' ++ [a]).foldLeft(e)(f)  // induction
== d1.foldLeft(e)(f) ++ d2.foldLeft(e)(f)  // definition of d2 (QED)
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Supporting Competing APIs in Scala -- Can Better Package Factoring Help?]]></title>
    <link href="http://erikerlandson.github.com/blog/2016/08/31/supporting-competing-apis-in-scala-can-better-package-factoring-help/"/>
    <updated>2016-08-31T17:55:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2016/08/31/supporting-competing-apis-in-scala-can-better-package-factoring-help</id>
    <content type="html"><![CDATA[<p> On and off over the last year, I've been working on a <a href="http://erikerlandson.github.io/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/">library</a> of tree and map classes in Scala that happen to make use of some algebraic structures (mostly monoids or related concepts).
 In my initial implementations, I made use of the popular <a href="https://github.com/twitter/algebird">algebird</a> variations on monoid and friends.
 In their incarnation as an <a href="https://github.com/twitter/algebird/pull/496">algebird PR</a> this was uncontroversial to say the least, but lately I have been re-thinking them as a <a href="https://github.com/isarn/isarn/pull/1">third-party Scala package</a>.</p>

<p>This immediately raised some interesting and thorny questions:
in an ecosystem that contains not just <a href="https://github.com/twitter/algebird">algebird</a>, but other popular alternatives such as <a href="https://github.com/typelevel/cats">cats</a> and <a href="https://github.com/scalaz/scalaz">scalaz</a>, what algebra API should I use in my code?
How best to allow the library user to interoperate with the algebra libray of their choice?
Can I accomplish these things while also avoiding any problematic package dependencies in my library code?</p>

<p>In Scala, the second question is relatively straightforward to answer.
I can write my interface using <a href="http://docs.scala-lang.org/tutorials/tour/implicit-conversions">implicit conversions</a>, and provide sub-packages that provide such conversions from popular algebra libraries into the library I actually use in my code.
A library user can import the predefined implicit conversions of their choice, or if necessary provide their own.</p>

<p>So far so good, but that leads immediately back to the first question -- what API should <strong><em>I</em></strong> choose to use internally in my own library?</p>

<p>One obvious approach is to just pick one of the popular options (I might favor <code>cats</code>, for example) and write my library code using that.
If a library user also prefers <code>cats</code>, great.
Otherwise, they can import the appropritate implicit conversions from their favorite alternative into <code>cats</code> and be on their way.</p>

<p>But this solution is not without drawbacks.
Anybody using my library will now be including <code>cats</code> as a transitive dependency in their project, even if they are already using some other alternative.
Although <code>cats</code> is not an enormous library, that represents a fair amount of code sucked into my users' projects, most of which isn't going to be used at all.
More insidiously, I have now introduced the possiblity that the <code>cats</code> version I package with is out of sync with the version my library users are building against.
Version misalignment in transitive dependencies is a land-mine in project builds and very difficult to resolve.</p>

<p>A second approach I might use is to define some abstract algebraic traits of my own.
I can write my libraries in terms of this new API, and then provide implicit conversions from popular APIs into mine.</p>

<p>This approach has some real advantages over the previous.  Being entirely abstract, my internal API will be lightweight.  I have the option of including only the algebraic concepts I need.  It does not introduce any possibly problematic 3rd-party dependencies that might cause code bloat or versioning problems for my library users.</p>

<p>Although this is an effective solution, I find it dissatisfying for a couple reasons.
Firstly, my new internal API effectively represents <em>yet another competing algebra API</em>, and so I am essentially contributing to the proliferating-standards antipattern.</p>

<p><img src="https://imgs.xkcd.com/comics/standards.png" alt="standards" /></p>

<p>Secondly, it means that I am not taking advantage of community knowledge.
The <code>cats</code> library embodies a great deal of cumulative human expertise in both category theory and Scala library design.
What does a good algebra library API look like?
Well, <em>it's likely to look a lot like <code>cats</code></em> of course!
The odds that I end up doing an inferior job designing my little internal vanity API are rather higher than the odds that I do as well or better.
The best I can hope for is to re-invent the wheel, with a real possibility that my wheel has corners.</p>

<p>Is there a way to resolve this unpalatable situation?
Can we design our projects to both remain flexible about interfacing with multiple 3rd-party alternatives, but avoid effectively writing <em>yet another alternative</em> for our own internal use?</p>

<p>I hardly have any authoritative answers to this problem, but I have one idea that might move toward a solution.
As I alluded to above, when I write my libraries, I am most frequently <em>only</em> interested in the API -- the abstract interface.
If I did go with writing my own algebra API, I would seek to define purely abstract traits.
Since my intention is that my library users would supply their own favorite library alternative, I would have no need or desire to instantiate any of my APIs.
That function would be provided by the separate sub-projects that provide implicit conversions from community alternatives into my API.</p>

<p>On the other hand, what if <code>cats</code> and <code>algebird</code> factored <em>their</em> libraries in a similar way?
What if I could include a sub-package like <code>cats-kernel-api</code>, or <code>algebird-core-api</code>, which contained <em>only</em> pure abstract traits for monoid, semigroup, etc?
Then I could choose my favorite community API, and code against it, with much less code bloat, and a much reduced vulnerability to any versioning drift.
I would still be free to provide implicit conversions and allow <em>my</em> users to make their own choice of library in their projects.</p>

<p>Although I find this idea attractive, it is certainly not foolproof.
For example, there is never a way to <em>guarantee</em> that versioning drift won't break an API.
APIs such as <code>cats</code> and <code>algebird</code> are likely to be unusually amenable to this kind of approach.
After all, their interfaces are primarily driven by underlying mathematical definitions, which are generally as stable as such things ever get.
However, APIs in general tend to be significantly more stable than underlying code.
And the most-stable subsets of APIs might be encoded as traits and exposed this way, allowing other more experimental API components to change at a higher frequency.
Perhaps library packages could even be factored in some way such as <code>library-stable-api</code> and <code>library-unstable-api</code>.
That would clearly add a bit of complication to library trait hierarchies, but the payoff in terms of increased 3rd-party usability might be worth it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The 'prepare' operation considered harmful in Algebird aggregation]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/"/>
    <updated>2015-11-24T16:32:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird</id>
    <content type="html"><![CDATA[<p>I want to make an argument that the Algebird <a href="http://twitter.github.io/algebird/#com.twitter.algebird.Aggregator">Aggregator</a> design, in particular its use of the <code>prepare</code> operation in a map-reduce context, has substantial inefficiencies, compared to an equivalent formulation that is more directly suited to taking advantage of Scala's <a href="http://www.scala-lang.org/api/current/index.html#scala.collection.Seq">aggregate method on collections</a> method.</p>

<p>Consider the definition of aggregation in the Aggregator class:</p>

<p><code>scala
def apply(inputs: TraversableOnce[A]): C = present(reduce(inputs.map(prepare)))
</code></p>

<p>You can see that it is a standard map/reduce operation, where <code>reduce</code> is defined as a monoidal (or semigroup -- more on this later) operation. Under the hood, it boils down to an invocation of Scala's <code>reduceLeft</code> method.  The key thing to notice is that the role of <code>prepare</code> is to map a collection of data elements into the required monoids, which are then aggregated using that monoid's <code>plus</code> operation.  In other words, <code>prepare</code> converts data elements into "singleton" monoids each representing a data element.</p>

<p>Now, if the monoid in question is simple, say some numeric type, this conversion is free, or nearly so.  For example, the conversion of an integer into the "integer monoid" is a no-op.  However, there are other kinds of "non-trivial" monoids, for which the conversion of a data element into its corresponding monoid may be costly.  In this post, I will be using the monoid defined by Scala Set[Int], where the monoid <code>plus</code> operation is set union, and of course the <code>zero</code> element is the empty set.</p>

<p>Consider the process of defining an Algebird aggregator for the task of generating the set of unique elements in a data set.  The corresponding <code>prepare</code> operation is: <code>prepare(e: Int) = Set(e)</code>.  A monoid trait that encodes this idea might look like the following.  (the code I used in this post can be found <a href="https://gist.github.com/erikerlandson/d96dc553bc51e0eb5e4b">here</a>)</p>

<p>```scala
// an algebird-like monoid with the 'prepare' operation
trait PreparedMonoid[M, E] {
  val zero: M
  def plus(m1: M, m2: M): M
  def prepare(e: E): M
}</p>

<p>// a PreparedMonoid for a set of integers.  monoid operator is set union.
object intSetPrepared extends PreparedMonoid[Set[Int], Int] {
  val zero = Set.empty[Int]
  def plus(m1: Set[Int], m2: Set[Int]) = m1 ++ m2
  def prepare(e: Int) = Set(e)
}</p>

<p>implicit class SeqWithMapReduce<a href="seq:%20Seq[E]">E</a> {
  // algebird map/reduce Aggregator model
  def mrPrepared<a href="mon:%20PreparedMonoid[M,%20E]">M</a>: M = {</p>

<pre><code>seq.map(mon.prepare).reduceLeft(mon.plus)
</code></pre>

<p>  }
}
```</p>

<p>If we unpack the above code, as applied to <code>intSetPrepared</code>, we are instantiating a new Set object, containing a single value, for every single input data element.</p>

<p>But there is a potentially better model of aggregation, exemplified by the Scala <code>aggregate</code> method.  This method does not use a <code>prepare</code> operation.  It uses a zero value and a monoidal operator, which the Scala docs refer to as <code>combop</code>, but it also uses an "update" operation, that defines how to update the monoid object, directly, with a single element, referred to as <code>seqop</code> in Scala's documentation.  This idea can also be encoded as a flavor of monoid, enhanced with an <code>update</code> method:</p>

<p>```scala
// an algebird-like monoid with 'update' operation
trait UpdatedMonoid[M, E] {
  val zero: M
  def plus(m1: M, m2: M): M
  def update(m: M, e: E): M
}</p>

<p>// an equivalent UpdatedMonoid for a set of integers
object intSetUpdated extends UpdatedMonoid[Set[Int], Int] {
  val zero = Set.empty[Int]
  def plus(m1: Set[Int], m2: Set[Int]) = m1 ++ m2
  def update(m: Set[Int], e: Int) = m + e
}</p>

<p>implicit class SeqWithMapReduceUpdated<a href="seq:%20Seq[E]">E</a> {
  // map/reduce logic, taking advantage of scala 'aggregate'
  def mrUpdatedAggregate<a href="mon:%20UpdatedMonoid[M,%20E]">M</a>: M = {</p>

<pre><code>seq.aggregate(mon.zero)(mon.update, mon.plus)
</code></pre>

<p>  }
}
```</p>

<p>This arrangement promises more efficiency when aggregating w.r.t. nontrivial monoids, by avoiding the construction of "singleton" monoids for each data element.  The following demo confirms that for the Set-based monoid, it is over 10 times faster:</p>

<p>```scala
scala> :load /home/eje/scala/prepare.scala
Loading /home/eje/scala/prepare.scala...
defined module prepare</p>

<p>scala> import prepare.<em>
import prepare.</em></p>

<p>scala> val data = Vector.fill(1000000) { scala.util.Random.nextInt(10) }
data: scala.collection.immutable.Vector[Int] = Vector(7, 9, 4, 2, 7,...</p>

<p>// Verify that output is the same for both implementations:
scala> data.mrPrepared(intSetPrepared)
res0: Set[Int] = Set(0, 5, 1, 6, 9, 2, 7, 3, 8, 4)</p>

<p>// results are the same
scala> data.mrUpdatedAggregate(intSetUpdated)
res1: Set[Int] = Set(0, 5, 1, 6, 9, 2, 7, 3, 8, 4)</p>

<p>// Compare timings of prepare-based versus update-based aggregation
// (benchmark values are returned in seconds)
scala> benchmark(10) { data.mrPrepared(intSetPrepared) }
res2: Double = 0.2957673056</p>

<p>// update-based aggregation is 10 times faster
scala> benchmark(10) { data.mrUpdatedAggregate(intSetUpdated) }
res3: Double = 0.027041249300000004
```</p>

<p>It is also possible to apply Scala's <code>aggregate</code> to a monoid enhanced with <code>prepare</code>:</p>

<p>```scala
implicit class SeqWithMapReducePrepared<a href="seq:%20Seq[E]">E</a> {
  // using 'aggregate' with prepared op
  def mrPreparedAggregate<a href="mon:%20PreparedMonoid[M,%20E]">M</a>: M = {</p>

<pre><code>seq.aggregate(mon.zero)((m, e) =&gt; mon.plus(m, mon.prepare(e)), mon.plus)
</code></pre>

<p>  }
}
```</p>

<p>Although this turns out to be measurably faster than the literal map-reduce implementation, it is still not nearly as fast as the variation using <code>update</code>:</p>

<p><code>scala
scala&gt; benchmark(10) { data.mrPreparedAggregate(intSetPrepared) }
res2: Double = 0.1754636707
</code></p>

<p>Readers familiar with Algebird may be wondering about my use of monoids above, when the <code>Aggregator</code> interface is actually based on semigroups.  This is important, since building on Scala's <code>aggregate</code> function requires a zero element that semigroups do not have.  Although I believe it might be worth considering changing <code>Aggregator</code> to use monoids, another sensible option is to change the internal logic for the subclass <code>AggregatorMonoid</code>, which does require a monoid, or possibly just define a new <code>AggregatorMonoidUpdated</code> subclass.</p>

<p>A final note on compatability: note that any monoid enhanced with <code>prepare</code> can be converted into an equivalent monoid enhanced with <code>update</code>, as demonstrated by this factory function:</p>

<p>```scala
object UpdatedMonoid {
  // create an UpdatedMonoid from a PreparedMonoid
  def apply<a href="mon:%20PreparedMonoid[M,%20E]">M, E</a> = new UpdatedMonoid[M, E] {</p>

<pre><code>val zero = mon.zero
def plus(m1: M, m2: M) = mon.plus(m1, m2)
def update(m: M, e: E) = mon.plus(m, mon.prepare(e))
</code></pre>

<p>  }
}
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Very Fast Reservoir Sampling]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling/"/>
    <updated>2015-11-20T11:27:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling</id>
    <content type="html"><![CDATA[<p>In this post I will demonstrate how to do reservoir sampling orders of magnitude faster than the traditional "naive" reservoir sampling algorithm, using a fast high-fidelity approximation to the reservoir sampling-gap distribution.</p>

<blockquote><p>The code I used to collect the data for this post can be viewed <a href="https://github.com/erikerlandson/silex/blob/blog/reservoir/src/main/scala/com/redhat/et/silex/sample/reservoir/reservoir.scala">here</a>.  I generated the plots using the <a href="https://github.com/quantifind/wisp">quantifind WISP</a> project.</p>

<p>Update (April 4, 2016): my colleague <a href="http://rnowling.github.io/">RJ Nowling</a> ran across a <a href="http://www.ittc.ku.edu/~jsv/Papers/Vit87.RandomSampling.pdf">paper by J.S. Vitter</a> that shows Vitter developed the trick of accelerating sampling with a sampling-gap distribution in 1987 -- I re-invented Vitter's wheel 30 years after the fact!  I'm surprised it never caught on, as it is not much harder to implement than the naive version.</p></blockquote>

<p>In a <a href="http://erikerlandson.github.io/blog/2014/09/11/faster-random-samples-with-gap-sampling/">previous post</a>, I showed that random Bernoulli and Poisson sampling could be made much faster by modeling the <em>sampling gap distribution</em> for the corresponding sampling distributions.  More recently, I also began exploring whether <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">reservoir sampling</a> might also be optimized using the gap sampling technique, by deriving the <a href="http://erikerlandson.github.io/blog/2015/08/17/the-reservoir-sampling-gap-distribution/">reservoir sampling gap distribution</a>.  For a sampling reservoir of size (R), starting at data element (j), the probability distribution of the sampling gap is:</p>

<p><img src="/assets/images/reservoir1/figure6.png" title="Figure 1" alt="Figure 1" /></p>

<p>Modeling a sampling gap distribution is a powerful tool for optimizing a sampling algorithm, but it presupposes that you can actually draw values from that distribution substantially faster than just applying a random process to drawing each data element.  I was unable to come up with a "direct" algorithm for drawing samples from P(k) above (I suspect none exists), however I also know the CDF F(k), so it <em>is</em> possible to apply <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inversion sampling</a>, which runs in logarithmic time w.r.t the desired accuracy.  Although its logarithmic cost effectively guarantees that it will be a net efficiency win for sufficiently large (j), it still involves a substantial number of computations to yield its samples, and it seems unlikely to be competitive with straight "naive" reservoir sampling over many real-world data sizes, where (j) may never grow very large.</p>

<p>Well, if exact computations are too expensive, we can always look for a fast approximation.  Consider the original "first principles" formula for the sampling gap P(k):</p>

<p><img src="/assets/images/reservoir2/figure2.png" title="Figure 2" alt="Figure 2" /></p>

<p>As the figure above alludes to, if (j) is relatively large compared to (k), then values (j+1),(j+2)...(j+k) are all going to be effectively "close" to (j), and so we can replace them all with (j) as an approximation.  Note that the resulting approximation is just the PMF of the <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a>, with probability of success p=(R/j), and we already saw how to efficiently draw values from a geometric distribution from our experience with Bernoulli sampling.</p>

<p>Do we have any reason to hope that this approximation will be useful?  For reasons that are similar to those for Bernoulli gap sampling, it will only be efficient to employ gap sampling when the probability (R/j) becomes small enough.  From our experiences with Bernoulli sampling that is <em>at least</em> j>=2R.  So, we have some assurance that (j) itself will be never be <em>very</em> small.  What about (k)?  Note that a geometric distribution "favors" smaller values of (k) -- that is, small values of (k) have the highest probabilities.  In fact, the smaller that (j) is, the larger the probability (R/j) is, and so the more likely that (k) values that are small relative to (j) will be the frequent ones.  It is also promising that the true distribution for P(k) <em>also</em> favors smaller values of (k) (in fact it favors them even a bit more strongly than the approximation).</p>

<p>Although it is encouraging, it is also clear that my argument above is limited to heuristic hand-waving.  What does this approximation really <em>look</em> like, compared to the true distribution?  Fortunately, it is easy to plot both distributions numerically, since we now know the formulas for both:</p>

<p><img src="/assets/images/reservoir2/CDFs_R=10.png" title="Figure 3" alt="Figure 3" /></p>

<p>The plot above shows that, in fact, the geometric approximation is a <em>surprisingly good</em> approximation to the true distribution!  Furthermore, the approximation remains good as both (j) and (k) grow larger.</p>

<p>Our numeric eye-balling looks quite promising.  Is there an effective way to <em>measure</em> how good this approximation is?  One useful measure is the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov D statistic</a>, which is just the maximum absolute error between two cumulative distributions.  Here is a plot of the D statistic for reservoir size R=10, as (j) varies across several magnitudes:</p>

<p><img src="/assets/images/reservoir2/R=10.png" title="Figure 4" alt="Figure 4" /></p>

<p>This plot is also good news: we can see that deviation, as measured by D, remains bounded at a small value (less than 0.0262).  As this is for the specific value R=10, we also want to know how things change as reservoir size changes:</p>

<p><img src="/assets/images/reservoir2/R=all.png" title="Figure 5" alt="Figure 5" /></p>

<p>The news is still good!  As reservoir size grows, the approximation only gets better: the D values get smaller as R increases, and remain asymptotically bounded as (j) increases.</p>

<p>Now we have some numeric assurance that the geometric approximation is a good one, and stays good as reservoir size grows and sampling runs get longer.  However, we should also verify that an actual implementation of the approximation works as expected.</p>

<p>Here is pseudocode for an implementation of reservoir sampling using the fast geometric approximation:</p>

<pre><code>// data is array to sample from
// R is the reservoir size
function reservoirFast(data: Array, R: Int) {
  n = data.length
  // Initialize reservoir with first R elements of data:
  res = data[0 until R]
  // Until this threshold, use traditional sampling.  This value may
  // depend on performance characteristics of random number generation and/or
  // numeric libraries:
  t = 4 * R
  j = 1 + R
  while (j &lt; n  &amp;&amp;  j &lt;= t) {
    k = randomInt(j) // random integer &gt;= 0 and &lt; j
    if (k &lt; R) res[k] = data[j]
    j = j + 1
  }
  // Once gaps become significant, it pays to do gap sampling
  while (j &lt; n) {
    // draw gap size (g) from geometric distribution with probability p = R/j
    p = R / j
    u = randomFloat() // random float &gt; 0 and &lt;= 1
    g = floor(log(u) / log(1-p))
    j = j + g
    if (j &lt; n) {
      k = randomInt(R)
      res[k] = data[j]
    }
    j = j + 1
  }
  // return the reservoir
  return res
}
</code></pre>

<p>Following is a plot that shows two-sample D statistics, comparing the distribution in sample gaps between runs of the exact "naive" reservoir sampling with the fast geometric approximation:</p>

<p><img src="/assets/images/reservoir2/D_naive_vs_fast.png" title="Figure 6" alt="Figure 6" /></p>

<p>As expected, the measured difference in sampling characteristics between naive and fast approximation are small, confirming the numeric predictions.</p>

<p>Since the point of this exercise was to achieve faster random sampling, it remains to measure what kind of speed improvements the fast approximation provides.  As a point of reference, here is a plot of run times for reservoir sampling over 10<sup>8</sup> integers:</p>

<p><img src="/assets/images/reservoir2/naive_sample_time_vs_R.png" title="Figure 7" alt="Figure 7" /></p>

<p>As expected, sample time remains constant at around 1.5 seconds, regardless of reservoir size, since the naive algorithm always samples from its RNG per each sample.</p>

<p>Compare this to the corresponding plot for the fast geometric approximation:</p>

<p><img src="/assets/images/reservoir2/gap_sample_times_vs_R.png" title="Figure 8" alt="Figure 8" /></p>

<p>Firstly, we see that the sampling times are <em>much faster</em>, as originally anticipated in my <a href="http://erikerlandson.github.io/blog/2015/08/17/the-reservoir-sampling-gap-distribution/">previous post</a> -- in the neighborhood of 3 orders of magnitude faster.  Secondly, we see that the sampling times do increase as a linear function of reservoir size.  Based on our experience with Bernoulli gap sampling, this is expected; the sampling probabilities are given by (R/j), and therefore the amount of sampling is proportional to R.</p>

<p>Another property anticipated in my previous post was that the efficiency of gap sampling should continue to increase as the amount of data sampled grows; the sampling probability being (R/j), the probability of sampling decreases as j gets larger, and so the corresponding gap sizes grow.  The following plot verifies this property, holding reservoir size R constant, and increasing the data size:</p>

<p><img src="/assets/images/reservoir2/gap_sampling_efficiency.png" title="Figure 9" alt="Figure 9" /></p>

<p>The sampling time (per million elements) decreases as the sample size grows, as predicted by the formula.</p>

<p>In conclusion, I have demonstrated that a geometric distribution can be used as a high quality approximation to the true sampling gap distribution for reservoir sampling, which allows reservoir sampling to be performed much faster than the naive algorithm while still retaining sampling quality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Library of Binary Tree Algorithms as Mixable Scala Traits]]></title>
    <link href="http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/"/>
    <updated>2015-09-26T12:43:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits</id>
    <content type="html"><![CDATA[<p>In this post I am going to describe some work I've done recently on a system of Scala traits that support tree-based collection algorithms prefix-sum, nearest key query and value increment in a mixable format, all backed by Red-Black balanced tree logic, which is also a fully inheritable trait.</p>

<blockquote><p>(update) Since I wrote this post, the code has evolved into a <a href="https://github.com/isarn/isarn-collections">library on the isarn project</a>. The original source files, containing the exact code fragments discussed in the remainder of this post, are preserved for posterity <a href="https://github.com/erikerlandson/silex/tree/blog/rbtraits/src/main/scala/com/redhat/et/silex/maps">here</a>.</p></blockquote>

<p>This post eventually became a bit more sprawling and "tl/dr" than I was expecting, so by way of apology, here is a table of contents with links:</p>

<ol>
<li><a href="#motivation">Motivating Use Case</a></li>
<li><a href="#overview">Library Overview</a></li>
<li><a href="#redblack">A Red-Black Tree Base Class</a></li>
<li><a href="#nodemap">Node Inheritance Example: NodeMap[K,V]</a></li>
<li><a href="#orderedmaplike">Collection Trait Example: OrderedMapLike[K,V,IN,M]</a></li>
<li><a href="#orderedmap">Collection Example: OrderedMap[K,V]</a></li>
<li><a href="#mixing">Finale: Trait Mixing</a></li>
</ol>


<p><a name="motivation"></a></p>

<h5>A Motivating Use Case</h5>

<p>The skeptical programmer may be wondering what the point of Yet Another Map Collection really is, much less an entire class hierarchy.  The use case that inspired this work was <a href="https://github.com/twitter/algebird/pull/495">my project</a> of implementing the <a href="https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf">t-digest algorithm</a>.  Discussion of t-digest is beyond the scope of this post, but suffice it to say that constructing a t-digest requires the maintenance of a collection of "cluster" objects, that needs to satisfy the following several properties:</p>

<ol>
<li>an entry contains one <strong>or more</strong> cluster objects at a given numeric location</li>
<li>entries are maintained in a numeric key order</li>
<li>entries will be frequently inserted and deleted, in arbitrary order</li>
<li>given a numeric key value, be able to find the entry nearest to that value</li>
<li>given a key, compute a <a href="https://en.wikipedia.org/wiki/Prefix_sum">prefix-sum</a> for that value</li>
<li>all of the above should be bounded by logarithmic time complexity</li>
</ol>


<p>Propreties 2,3 and 6 are commonly satisfied by a map structure backed by some variety of balanced tree representation, of which the best-known is the <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">Red-Black tree</a>.</p>

<p>Properties 1, 4 and 5 are more interesting.  Property 1 -- representing a collection of multiple objects at each entry -- can be accomplished in a generalizable way by noting that a collection is representable as a monoid, and so supporting values that can be incremented with respect to a <a href="http://twitter.github.io/algebird/index.html#com.twitter.algebird.Monoid">user-supplied monoid relation</a> can satisfy property-1, but also can support many other kinds of update, including but not limited to classical numeric incrementing operations.</p>

<p>Properties 4 and 5 -- nearest-entry queries and prefix-sum queries -- are also both supportable in logarithmic time using a tree data structure, provided that tree is balanced.  Again, the details of the algorithms are out of the current scope, however they are not extremely complex, and their implementations are available in the code.</p>

<p>A reader with their software engineering hat on will notice that these properties are <em>orthogonal</em>.  A programmer might be interested in a data structure supporting any one of them, or in some mixed combination.   This kind of situation fairly shouts "Scala traits" (or, alternatively, interfaces in Java, etc).  With that idea in mind, I designed a system of Scala collection traits that support all of the above properties, in a pure trait form that is fully "mixable" by the programmer, so that one can use exactly the properties needed, but not pay for anything else.</p>

<p><a name="overview"></a></p>

<h5>Library Overview</h5>

<p>The library consists broadly of 3 kinds of traits:</p>

<ul>
<li>tree node traits -- implement core tree support for some functionality</li>
<li>collection traits -- provide additional collection API methods the user</li>
<li>collections -- instantiate a usable incarnation of a collection</li>
</ul>


<p>For the programmer who wishes to either create a trait mixture, or add new mixable traits, the collections also function as reference implementations.</p>

<p>The three tables that follow summarize the currently available traits of each kind listed above.  They are (at the time of this posting) all under the package namespace <code>com.redhat.et.silex.maps</code>:</p>

<p><head><style>
table, th, td {
border: 1px solid black;
border-collapse: collapse;
}
th, td {
padding: 10px;
}
th {
text-align: center;
}
</style></head></p>

<table>
<caption>Tree Node Traits</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>Node[K]</td> <td>redblack.tree</td><td>Fundamental Red-Black tree functionality</td></tr>
<tr><td>NodeMap[K,V]</td><td>ordered.tree</td><td>Support a mapping from keys to values</td></tr>
<tr><td>NodeNear[K]</td><td>nearest.tree</td><td>Nearest-entry query (key-only)</td></tr>
<tr><td>NodeNearMap[K,V]</td><td>nearest.tree</td><td>Nearest-entry query for key/value maps</td></tr>
<tr><td>NodeInc[K,V]</td><td>increment.tree</td><td>Increment values w.r.t. a monoid</td></tr>
<tr><td>NodePS[K,V,P]</td><td>prefixsum.tree</td><td>Prefix sum queries by key (w.r.t. a monoid)</td></tr>
</table>




<br>


<table>
<caption>Collection Traits</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>OrderedSetLike[K,IN,M]</td><td>ordered</td><td>ordered set of keys</td></tr>
<tr><td>OrderedMapLike[K,V,IN,M]</td><td>ordered</td><td>ordered key/value map</td></tr>
<tr><td>NearestSetLike[K,IN,M]</td><td>nearest</td><td>nearest entry query on keys</td></tr>
<tr><td>NearestMapLike[K,V,IN,M]</td><td>nearest</td><td>nearest entry query on key/value map</td></tr>
<tr><td>IncrementMapLike[K,V,IN,M]</td><td>increment</td><td>increment values w.r.t a monoid</td></tr>
<tr><td>PrefixSumMapLike[K,V,P,IN,M]</td><td>prefixsum</td><td>prefix sum queries w.r.t. a monoid</td></tr>
</table>




<br>


<table>
<caption>Concrete Collections</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>OrderedSet[K]</td><td>ordered</td><td>ordered set</td></tr>
<tr><td>OrderedMap[K,V]</td><td>ordered</td><td>ordered key/value map</td></tr>
<tr><td>NearestSet[K]</td><td>nearest</td><td>ordered set with nearest-entry query</td></tr>
<tr><td>NearestMap[K,V]</td><td>nearest</td><td>ordred map with nearest-entry query</td></tr>
<tr><td>IncrementMap[K,V]</td><td>increment</td><td>ordered map with value increment w.r.t. a monoid</td></tr>
<tr><td>PrefixSumMap[K,V,P]</td><td>prefixsum</td><td>ordered map with prefix sum query w.r.t. a monoid</td></tr>
</table>




<br>


<p>The following diagram summarizes the organization and inheritance relationships of the classes.</p>

<p><img src="/assets/images/rbtraits/rbtraits.png" alt="diagram" /></p>

<p><a name="redblack"></a></p>

<h5>A Red/Black Tree Base Class</h5>

<p>The most fundamental trait in this hierarchy is the trait that embodies Red-Black balancing; a "red-black-ness" trait, as it were.  This trait supplies the axiomatic tree operations of insertion, deletion and key lookup, where the Red-Black balancing operations are encapsulated for insertion (due to <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=44273">Chris Okasaki</a>) and deletion (due to <a href="http://www.cs.kent.ac.uk/people/staff/smk/redblack/rb.html">Stefan Kahrs</a>)  Note that Red-Black trees do not assume a separate value, as in a map, but require only keys (thus implementing an ordered set over the key type):</p>

<p>``` scala
object tree {
  /<em>* The color (red or black) of a node in a Red/Black tree </em>/
  sealed trait Color
  case object R extends Color
  case object B extends Color</p>

<p>  /<em>* Defines the data payload of a tree node </em>/
  trait Data[K] {</p>

<pre><code>/** The axiomatic unit of data for R/B trees is a key */
val key: K
</code></pre>

<p>  }</p>

<p>  /** Base class of a Red/Black tree node</p>

<pre><code>* @tparam K The key type
*/
</code></pre>

<p>  trait Node[K] {</p>

<pre><code>/** The ordering that is applied to key values */
val keyOrdering: Ordering[K]

/** Instantiate an internal node. */
protected def iNode(color: Color, d: Data[K], lsub: Node[K], rsub: Node[K]): INode[K]

// ... declarations for insertion, deletion and key lookup ...

// ... red-black balancing rules ...
</code></pre>

<p>  }</p>

<p>   /<em>* Represents a leaf node in the Red Black tree system </em>/
  trait LNode[K] extends Node[K] {</p>

<pre><code>// ... basis case insertion, deletion, lookup ...
</code></pre>

<p>  }</p>

<p>  /<em>* Represents an internal node (Red or Black) in the Red Black tree system </em>/
  trait INode[K] extends Node[K] {</p>

<pre><code>/** The Red/Black color of this node */
val color: Color
/** Including, but not limited to, the key */
val data: Data[K]
/** The left sub-tree */
val lsub: Node[K]
/** The right sub-tree */
val rsub: Node[K]

// ... implementations for insertion, deletion, lookup ...
</code></pre>

<p>  }
}
<code>``
I will assume most readers are familiar with basic binary tree operations, and the Red-Black rules are described elsewhere (I adapted them from the Scala red-black implementation).  For the purposes of this discussion, the most interesting feature is that this is a _pure Scala trait_.  All</code>val` declarations are abstract.  This trait, by itself, cannot function without a subclass to eventually perform dependency injection.   However, this abstraction allows the trait to be inherited freely -- any programmer can inherit from this trait and get a basic Red-Black balanced tree for (nearly) free, as long as a few basic principles are adhered to for proper dependency injection.</p>

<p>Another detail to call out is the abstraction of the usual <code>key</code> with a <code>Data</code> element.  This element represents any node payload that is moved around as a unit during tree structure manipulations, such as balancing pivots.  In the case of a map-like subclass, <code>Data</code> is extended to include a <code>value</code> field as well as a <code>key</code> field.</p>

<p>The other noteworthy detail is the abstract definition <code>def iNode(color: Color, d: Data[K], lsub: Node[K], rsub: Node[K]): INode[K]</code> - this is the function called to create any new tree node.  In fact, this function, when eventually instantiated, is what performs dependency injection of other tree node fields.</p>

<p><a name="nodemap"></a></p>

<h5>Node Inheritance Example: NodeMap[K,V]</h5>

<p>A relatively simple example of node inheritance is hopefully instructive.  Here is the definition for tree nodes supporting a key/value map:</p>

<p>``` scala
object tree {
  /<em>* Trees that back a map-like object have a value as well as a key </em>/
  trait DataMap[K, V] extends Data[K] {</p>

<pre><code>val value: V
</code></pre>

<p>  }</p>

<p>  /** Base class of ordered K/V tree node</p>

<pre><code>* @tparam K The key type
* @tparam V The value type
*/
</code></pre>

<p>  trait NodeMap[K, V] extends Node[K]</p>

<p>  trait LNodeMap[K, V] extends NodeMap[K, V] with LNode[K]</p>

<p>  trait INodeMap[K, V] extends NodeMap[K, V] with INode[K] {</p>

<pre><code>val data: DataMap[K, V]
</code></pre>

<p>  }
}
```</p>

<p>Note that in this case very little is added to the red/black functionality already provided by <code>Node[K]</code>.  A <code>DataMap[K,V]</code> trait is defined to add a <code>value</code> field in addition to the <code>key</code>, and the internal node <code>INodeMap[K,V]</code> refines the type of its <code>data</code> field to be <code>DataMap[K,V]</code>.  The semantics is little more than "tree nodes now carry a value in addition to a key."</p>

<p>A tree node trait inherits from its own parent class <em>and</em> the corresponding traits for any mixed-in functionality.  So for example <code>INodeMap[K,V]</code> inherits from <code>NodeMap[K,V]</code> but also <code>INode[K]</code>.</p>

<p><a name="orderedmaplike"></a></p>

<h5>Collection Trait Example: OrderedMapLike[K,V,IN,M]</h5>

<p>Continuing with the ordered map example, here is the definition of the collection trait for an ordered map:</p>

<p>``` scala
trait OrderedMapLike[K, V, IN &lt;: INodeMap[K, V], M &lt;: OrderedMapLike[K, V, IN, M]]</p>

<pre><code>extends NodeMap[K, V] with OrderedLike[K, IN, M] {
</code></pre>

<p>  /<em>* Obtain a new map with a (key, val) pair inserted </em>/
  def +(kv: (K, V)) = this.insert(</p>

<pre><code>new DataMap[K, V] {
  val key = kv._1
  val value = kv._2
}).asInstanceOf[M]
</code></pre>

<p>  /<em>* Get the value stored at a key, or None if key is not present </em>/
  def get(k: K) = this.getNode(k).map(_.data.value)</p>

<p>  /<em>* Iterator over (key,val) pairs, in key order </em>/
  def iterator = nodesIterator.map(n => ((n.data.key, n.data.value)))</p>

<p>  /<em>* Container of values, in key order </em>/
  def values = valuesIterator.toIterable</p>

<p>  /<em>* Iterator over values, in key order </em>/
  def valuesIterator = nodesIterator.map(_.data.value)
}
<code>``
You can see that this trait supplies collection API methods that a Scala programmer will recognize as being standard for any map-like collection.  Note that this trait also inherits other standard methods from</code>OrderedLike[K,IN,M]<code>(common to both sets and maps) and _also_ inherits from</code>NodeMap[K,V]<code>: In other words, a collection is effectively yet another kind of tree node, with additional collection API methods mixed in.   Note also the use of "self types" (the type parameter</code>M`), which allows the collection to return objects of its own kind.  This is crucial for allowing operations like data insertion to return an object that also supports node insertion, and to maintain consistency of type across operations.  The collection type is properly "closed" with respect to its own operations.</p>

<p><a name="orderedmap"></a></p>

<h5>Collection Example: OrderedMap[K,V]</h5>

<p>To conclude the ordered map example, consider the task of defining a concrete instantiation of an ordered map:
``` scala
sealed trait OrderedMap[K, V] extends OrderedMapLike[K, V, INodeMap[K, V], OrderedMap[K, V]] {
  override def toString =</p>

<pre><code>"OrderedMap(" +
  nodesIterator.map(n =&gt; s"${n.data.key} -&gt; ${n.data.value}").mkString(", ") +
")"
</code></pre>

<p>}
<code>``
You can see that (aside from a convenience override of</code>toString<code>) the trait</code>OrderedMap[K,V]<code>is nothing more than a vehicle for instantiating a particular concrete</code>OrderedMapLike[K,V,IN,M]<code>subtype, with particular concrete types for internal node (</code>INodeMap[K,V]`) and its own self-type.</p>

<p>Things become a little more interesting inside the companion object <code>OrderedMap</code>:
``` scala
object OrderedMap {
  def key<a href="implicit%20ord:%20Ordering[K]">K</a> = new AnyRef {</p>

<pre><code>def value[V]: OrderedMap[K, V] =
  new InjectMap[K, V](ord) with LNodeMap[K, V] with OrderedMap[K, V]
</code></pre>

<p>  }
}
<code>``
Note that the object returned by the factory method is upcast to</code>OrderedMap[K,V]<code>, but in fact has the more complicated type:</code>InjectMap[K,V] with LNodeMap[K,V] with OrderedMap[K,V]<code>.  There are a couple things going on here.  The trait</code>LNodeMap[K,V]` ensures that the new object is in particular a leaf node, which embodies a new empty tree in the Red-Black tree system.</p>

<p>The type <code>InjectMap[K,V]</code> has an even more interesting purpose.  Here is its definition:
``` scala
class InjectMap<a href="val%20keyOrdering:%20Ordering[K]">K, V</a> {
  def iNode(clr: Color, dat: Data[K], ls: Node[K], rs: Node[K]) =</p>

<pre><code>new InjectMap[K, V](keyOrdering) with INodeMap[K, V] with OrderedMap[K, V] {
  // INode
  val color = clr
  val lsub = ls
  val rsub = rs
  val data = dat.asInstanceOf[DataMap[K, V]]
}
</code></pre>

<p>}
<code>``
Firstly, note that it is a bona fide _class_, as opposed to a trait.  This class is where, finally, all things abstract are made real -- "dependency injection" in the parlance of Scala idioms.  You can see that it defines the implementation of abstract method</code>iNode<code>, and that it does this by returning yet _another_</code>InjectMap[K,V]<code>object, mixed with both</code>INodeMap[K,V]<code>and</code>OrderedMap[K,V]`, thus maintaining closure with respect to all three slices of functionality: dependency injection, the proper type of internal node, and map collection methods.</p>

<p>The various abstract <code>val</code> fields <code>color</code>, <code>data</code>, <code>lsub</code> and <code>rsub</code> are all given concrete values inside of <code>iNode</code>.  Here is where the value of concrete "reference" implementations manifests.  Any fields in the relevant internal-node type must be instantiated here, and the logic of instantiation cannot be inherited while still preserving the ability to mix abstract traits.  Therefore, any programmer wishing to create a new concrete sub-class must replicate the logic for instantiating all inherited in an internal node.</p>

<p>Another example makes the implications more clear.  Here is the definition of injection for a <a href="https://github.com/erikerlandson/silex/blob/blog/rbtraits/src/test/scala/com/redhat/et/silex/maps/mixed.scala">collection that mixes in all three traits</a> for incrementable values, nearest-key queries, and prefix-sum queries:</p>

<p>``` scala
  class Inject[K, V, P](</p>

<pre><code>val keyOrdering: Numeric[K],
val valueMonoid: Monoid[V],
val prefixMonoid: IncrementingMonoid[P, V]) {
  def iNode(clr: Color, dat: Data[K], ls: Node[K], rs: Node[K]) =
  new Inject[K, V, P](keyOrdering, valueMonoid, prefixMonoid)
      with INodeTD[K, V, P] with TDigestMap[K, V, P] {
    // INode[K]
    val color = clr
    val lsub = ls.asInstanceOf[NodeTD[K, V, P]]
    val rsub = rs.asInstanceOf[NodeTD[K, V, P]]
    val data = dat.asInstanceOf[DataMap[K, V]]
    // INodePS[K, V, P]
    val prefix = prefixMonoid.inc(prefixMonoid.plus(lsub.pfs, rsub.pfs), data.value)
    // INodeNear[K, V]
    val kmin = lsub match {
      case n: INodeTD[K, V, P] =&gt; n.kmin
      case _ =&gt; data.key
    }
    val kmax = rsub match {
      case n: INodeTD[K, V, P] =&gt; n.kmax
      case _ =&gt; data.key
    }
  }
</code></pre>

<p>  }
```
Here you can see that all logic for both "basic" internal nodes and also for maintaining prefix sums, and key min/max information for nearest-entry queries, must be supplied.  If there is a singularity in this design here is where it is.  The saving grace is that it is localized into a single well defined place, and any logic can be transcribed from a proper reference implementation of whatever traits are being mixed.</p>

<p><a name="mixing"></a></p>

<h5>Finale: Trait Mixing</h5>

<p>I will conclude by showing the code for mixing tree node traits and collection traits, which is elegant.  Here are type definitions for tree nodes and collection traits that inherit from incrementable values, nearest-key queries, and prefix-sum queries, and there is almost no code except the proper inheritances:</p>

<p>``` scala
object tree {
  import com.redhat.et.silex.maps.increment.tree.<em>
  import com.redhat.et.silex.maps.prefixsum.tree.</em>
  import com.redhat.et.silex.maps.nearest.tree._</p>

<p>  trait NodeTD[K, V, P] extends NodePS[K, V, P] with NodeInc[K, V] with NodeNearMap[K, V]</p>

<p>  trait LNodeTD[K, V, P] extends NodeTD[K, V, P]</p>

<pre><code>  with LNodePS[K, V, P] with LNodeInc[K, V] with LNodeNearMap[K, V]
</code></pre>

<p>  trait INodeTD[K, V, P] extends NodeTD[K, V, P]</p>

<pre><code>  with INodePS[K, V, P] with INodeInc[K, V] with INodeNearMap[K, V] {
val lsub: NodeTD[K, V, P]
val rsub: NodeTD[K, V, P]
</code></pre>

<p>  }
}</p>

<p>// ...</p>

<p>sealed trait TDigestMap[K, V, P]
  extends IncrementMapLike[K, V, INodeTD[K, V, P], TDigestMap[K, V, P]]
  with PrefixSumMapLike[K, V, P, INodeTD[K, V, P], TDigestMap[K, V, P]]
  with NearestMapLike[K, V, INodeTD[K, V, P], TDigestMap[K, V, P]] {</p>

<p>  override def toString = // ...
}
```</p>
]]></content>
  </entry>
  
</feed>
