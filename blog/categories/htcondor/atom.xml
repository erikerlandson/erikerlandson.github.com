<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: htcondor | tool monkey]]></title>
  <link href="http://erikerlandson.github.com/blog/categories/htcondor/atom.xml" rel="self"/>
  <link href="http://erikerlandson.github.com/"/>
  <updated>2013-03-17T21:26:01-07:00</updated>
  <id>http://erikerlandson.github.com/</id>
  <author>
    <name><![CDATA[Erik Erlandson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Examining the Modulus of Random Variables]]></title>
    <link href="http://erikerlandson.github.com/blog/2013/03/15/examining-the-modulus-of-random-variables/"/>
    <updated>2013-03-15T12:03:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2013/03/15/examining-the-modulus-of-random-variables</id>
    <content type="html"><![CDATA[<h3>Motivation</h3>

<p>The original motivation for these experiments was consideration of the impact of negotiator cycle cadence (i.e. the time between the start of one cycle and the start of the next) on HTCondor pool loading.  Specifically, any HTCondor job that completes and vacates its resource may leave that resource unloaded until it can be re-matched on the next cycle.  Therefore, the duration of resource vacancies (and hence, pool loading) can be thought of as a function of job durations <em>modulo</em> the cadence of the negotiator cycle.  In general, the aggregate behavior of job durations on a pool is useful to model as a random variable.  And so, it seemed worthwhile to build up a little intuition about the behavior of a random variable when you take its modulus.</p>

<h3>Methodology</h3>

<p>I took a Monte Carlo approach to this study because a tractable theoretical framework eluded me, and you do not have to dive very deep to show that <a href="http://erikerlandson.github.com/blog/2013/01/02/the-mean-of-the-modulus-does-not-equal-the-modulus-of-the-mean/">even trivial random variable behavior under a modulus is dependent on the distribution</a>.   A Monte Carlo framework for the study also allows for other underlying distributions to be easily studied, by altering the random variable being sampled.   In the interest of getting right into results, I'll briefly discuss the tools I used at the end of this post.</p>

<h3>Modulus and Variance</h3>

<p>Consider what happens to a random variable's modulus as its variance increases.  This sequence of plots shows that the modulus of a normal distribution tends toward a uniform distribution over the modulus interval, as the underlying variance increases:</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.20.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.30.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.40.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.50.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<p>From the above plots, we can see that in the case of a normal distribution, its modulus tends toward uniform rather quickly - by the time the underlying variance is half of the modulus interval.</p>

<p>The following plots demonstrate the same effect with a one-tailed distribution (the exponential) -- it requires a larger variance for the effect to manifest.</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_01.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_04.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_10.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/exponential_20.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<p>A third example, using a log-normal distribution.   The variance of the log-normal increases as a function of both \( \mu \) and \( \sigma \).  In this example \( \mu \) is increased systematically, holding \( \sigma \) constant at 1:</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_1.0.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.5_1.0.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_1.0_1.0.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_2.0_1.0.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<p>For a final examination of variance, I will again use log-normals and this time vary \( \sigma \), while holding \( \mu \) constant at 0.  Here we see that the effect of increasing the log-normal variance via \( \sigma \) does <em>not</em> follow the pattern in previous examples -- the distribution does not 'spread' and its modulus does not evolve toward a uniform distribution!</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_0.5.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_1.0.png" width="375" height="375">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_1.5.png" width="375" height="375">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/lognormal_0.0_2.0.png" width="375" height="375">  |</td>
</tr>
</tbody>
</table>


<br>


<h3>Modulus and Mean</h3>

<p>The following table of plots demonstrates the decreasing effect that a distribution's location (mean) has, as its spread increases and its modulus approaches uniformity.   In fact, we see that <em>any</em> distribution in the 'uniform modulus' parameter region is indistinguishable from any other, with respect to its modulus -- all changes to mean or variance <em>within</em> this region have no affect on the distribution's modulus!</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">  </th>
<th align="center">  </th>
<th align="center">  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.0_0.3.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.5_0.3.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_1.0_0.3.png" width="260" height="260">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.0_0.4.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.5_0.4.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_1.0_0.4.png" width="260" height="260">  |</td>
</tr>
<tr>
<td></td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.0_0.5.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_0.5_0.5.png" width="260" height="260">  </td>
<td align="center">  <img src="/assets/images/rv_mod_study/normal_1.0_0.5.png" width="260" height="260">  |</td>
</tr>
</tbody>
</table>


<br>


<h3>Conclusions</h3>

<p>Generally, as the spread of a distribution increases, its modulus tends toward a uniform distribution on the modulus interval.   Although it was tempting to state this in terms of increasing variance, we see from the 2nd log-normal experiment that variance can increase without increasing 'spread' in a way that causes the trend toward uniform modulus.   Currently, I'm not sure what the true invariant is, that properly distinguishes the 2nd log-normal scenario from the others.</p>

<p>For any distribution that <em>does</em> reside in the 'uniform-modulus' parameter space, we see that neither changes to location nor spread (nor even category of distribution) can be distinguished by the distribution modulus.</p>

<h3>Tools</h3>

<p>I used the following software widgets:</p>

<ul>
<li><a href="https://github.com/erikerlandson/condor_tools/blob/cad8773da36fa7f3c60c93895a428d6f1fae6752/bin/rv_modulus_study">rv_modulus_study</a> -- the jig for Monte Carlo sampling of underlying distributions and their corresponding modulus</li>
<li><a href="https://github.com/erikerlandson/dtools/wiki/dplot">dplot</a> -- a simple cli wrapper around <code>matplotlib.pyplot</code> functionality</li>
<li><a href="https://github.com/willb/capricious/">Capricious</a> -- a library for random sampling of various distribution types</li>
<li><a href="https://github.com/erikerlandson/capricious/blob/c8ec13f1f49880bb3573034de59971f84d15f7c1/lib/capricious/spline_distribution.rb">Capricious::SplineDistribution</a> -- a ruby class for estimating PDF and CDF of a distribution from sampled data, using cubic Hermite splines (note, at the time of this writing, I'm using an experimental variation on my personal repo fork, at the link)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Demonstration of Negotiator-Side Resource Consumption]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/12/03/a-demonstration-of-negotiator-side-resource-consumption/"/>
    <updated>2012-12-03T08:25:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/12/03/a-demonstration-of-negotiator-side-resource-consumption</id>
    <content type="html"><![CDATA[<p>HTCondor supports a notion of aggregate compute resources known as partitionable slots (p-slots), which may be consumed by multiple jobs.   Historically, at most one job could be matched against such a slot in a single negotiation cycle, which limited the rate at which partitionable slot resources could be utilized.  More recently, the scheduler has been enhanced with logic to allow it to acquire multiple claims against a partitionable slot, which increases the p-slot utilization rate. However, as this potentially bypasses the negotiator's accounting of global pool resources such as accounting group quotas and concurrency limits, it places some contraints on what jobs can can safely acquire multiple claims against any particular p-slot: for example, only other jobs on the same scheduler can be considered.  Additionally, candidate job requirements must match the requirements of the job that originally matched in the negotiator.  Another significant impact is that the negotiator is still forced to match an entire p-slot, which may have a large match cost (weight): these large match costs cause <a href="https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3013">accounting difficulties</a> when submitter shares and/or group quotas drop below the cost of a slot.  This particular problem is growing steadily larger, as machines with ever-larger numbers of cores and other resources appear in HTCondor pools.</p>

<p>An alternative approach to scheduler-side resource consumption is to enhance the negotiator with the ability to match multiple jobs against a resource (p-slot) -- negotiator-side resource consumption.   The advantages of negotiator-side consumption are that it places fewer limitations on what jobs can consume a given resource.  The negotiator already handles global resource accounting, and so jobs are not required to adhere to the same requirements expression to safely consume assets from the same resource.  Furthermore, jobs from any scheduler may be considered.  Each match is only charged the cost of resources consumed, and so p-slots with large amounts of resources do not cause difficulties with large match costs.   Another considerable benefit of this approach is that it facilitates the support of <a href="http://spinningmatt.wordpress.com/2012/11/13/no-longer-thinking-in-slots-thinking-in-aggregate-resources-and-consumption-policies/">configurable resource consumption policies</a></p>

<p>I have developed a working draft of negotiator-side resource consumption on my HTCondor github fork, topic branch <a href="https://github.com/erikerlandson/htcondor/tree/V7_9-prototype-negside-pslot-splits">V7_9-prototype-negside-pslot-splits</a> which also implements support for configurable resource consumption policies.   I will briefly demonstrate this implementation and some of its advantages below.</p>

<p>First I will demonstrate an example with a consumption policy that is essentially equivalent to HTCondor's current default policies.  Consider this configuration:</p>

<pre><code># spoof some cores
NUM_CPUS = 10

# configure an aggregate resource (p-slot) to consume
SLOT_TYPE_1 = 100%
SLOT_TYPE_1_PARTITIONABLE = True
# declare multiple claims for negotiator to use
# may also use global: NUM_CLAIMS
SLOT_TYPE_1_NUM_CLAIMS = 20
NUM_SLOTS_TYPE_1 = 1

# turn off schedd-side resource splitting since we're demonstrating neg-side alternative
CLAIM_PARTITIONABLE_LEFTOVERS = False

# turn this off to demonstrate that consumption policy will handle this kind of logic
MUST_MODIFY_REQUEST_EXPRS = False

# configure a consumption policy.   This policy is modeled on
# current 'modify-request-exprs' defaults:
# "my" is resource ad, "target" is job ad
STARTD_EXPRS = ConsumptionCpus, ConsumptionMemory, ConsumptionDisk
ConsumptionCpus = quantize(target.RequestCpus, {1})
ConsumptionMemory = quantize(target.RequestMemory, {128})
ConsumptionDisk = quantize(target.RequestDisk, {1024})
# swap doesn't seem to be actually supported in resource accounting

# keep slot weights enabled for match costing
NEGOTIATOR_USE_SLOT_WEIGHTS = True

# weight used to derive match cost: W(before-consumption) - W(after-consumption)
SlotWeight = Cpus

# for simplicity, turn off preemption, caching, worklife
CLAIM_WORKLIFE=0
MAXJOBRETIREMENTTIME = 3600
PREEMPT = False
RANK = 0
PREEMPTION_REQUIREMENTS = False
NEGOTIATOR_CONSIDER_PREEMPTION = False
NEGOTIATOR_MATCHLIST_CACHING = False

# verbose logging
ALL_DEBUG = D_FULLDEBUG

# reduce daemon update latencies
NEGOTIATOR_INTERVAL = 30
SCHEDD_INTERVAL = 15
</code></pre>

<p>In the above configuration, we declare a typical aggregate (that is, partitionable) resource <code>SLOT_TYPE_1</code>, but then we also configure a <em>consumption policy</em>, by advertising <code>ConsumptionCpus</code>, <code>ConsumptionMemory</code> and <code>ConsumptionDisk</code>.  Note that these are defined with quantizing expressions currently used as default values for the <code>MODIFY_REQUEST_EXPRS</code> behavior.  The startd and the negotiatior will <em>both</em> use these expressions by examining the slot ads.</p>

<p>Next, we submit 15 jobs.  Note that this more than the 10 cores advertised by the p-slot:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 60
should_transfer_files = if_needed
when_to_transfer_output = on_exit
queue 15
</code></pre>

<p>If we watch the negotiator log, we will see that negotiator matches the 10 jobs supported by the p-slot on the next cycle (note that it uses slot1 each time):</p>

<pre><code>$ tail -f NegotiatorLog | grep -e '\-\-\-\-\-'  -e 'matched
12/03/12 11:53:10 ---------- Finished Negotiation Cycle ----------
12/03/12 11:53:25 ---------- Started Negotiation Cycle ----------
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25       Successfully matched with slot1@rorschach
12/03/12 11:53:25 ---------- Finished Negotiation Cycle ----------
</code></pre>

<p>You can use <code>condor_q</code> to verify that the 10 jobs subsequently run.   The jobs run against 10 dynamic slots (d-slots) in the standard way:</p>

<pre><code>$ ccdump condor_status Name TotalSlotCpus
slot1@rorschach | 10
slot1_10@rorschach | 1
slot1_1@rorschach | 1
slot1_2@rorschach | 1
slot1_3@rorschach | 1
slot1_4@rorschach | 1
slot1_5@rorschach | 1
slot1_6@rorschach | 1
slot1_7@rorschach | 1
slot1_8@rorschach | 1
slot1_9@rorschach | 1
</code></pre>

<p>Next we consider altering the resource consumption policy.  As a simple example, suppose we wish to allocate memory more coarsely.  We could alter the configuration above by changing <code>ConsumptionMemory</code> to:</p>

<pre><code>ConsumptionMemory = quantize(target.RequestMemory, {512})
</code></pre>

<p>Perhaps we then also want to express match cost in a memory-centric way, instead of the usual cpu-centric way:</p>

<pre><code>SlotWeight = floor(Memory / 512)
</code></pre>

<p>Here it is worth noting that in this implementation of negotiator-side consumption, the cost of a match is defined as W(S) - W(S'), where W(S) is the weight of the slot <em>prior</em> to consuming resources from the match and consumption policy, and W(S`) is the weight evaluated for the slot <em>after</em> those resources are subtracted.  This modification enables multiple matches to be made against a single p-slot, and furthermore it paves the way to possible avenues for a <a href="http://erikerlandson.github.com/blog/2012/11/26/rethinking-the-semantics-of-group-quotas-and-slot-weights-computing-claim-capacity-from-consumption-policy/">better unit analysis of slot weights and accounting groups</a>.</p>

<p>Continuing the example, if we re-run the example with this new consumption policy, we should see that memory limits reduce the number of jobs matched against <code>slot1</code> to 3:</p>

<pre><code>$ tail -f NegotiatorLog | grep -e '\-\-\-\-\-'  -e 'matched'
12/03/12 12:58:22 ---------- Finished Negotiation Cycle ----------
12/03/12 12:58:37 ---------- Started Negotiation Cycle ----------
12/03/12 12:58:37       Successfully matched with slot1@rorschach
12/03/12 12:58:37       Successfully matched with slot1@rorschach
12/03/12 12:58:37       Successfully matched with slot1@rorschach
12/03/12 12:58:37 ---------- Finished Negotiation Cycle ----------
</code></pre>

<p>Examining the slot memory assets, we see that there is insufficient memory for a fourth match when our consumption policy sets the minimum at 512:</p>

<pre><code>$ ccdump condor_status Name TotalSlotMemory
slot1@rorschach | 1903
slot1_1@rorschach | 512
slot1_2@rorschach | 512
slot1_3@rorschach | 512
</code></pre>

<p>As a final example, I'll demonstrate the positive impact of negotiator side matching on interactions with accounting groups (or submitter shares).  Again returning to my original example, modify the configuration with a simple accounting group policy:</p>

<pre><code>GROUP_NAMES = a
GROUP_QUOTA_a = 1
GROUP_ACCEPT_SURPLUS = False
GROUP_AUTOREGROUP = False
</code></pre>

<p>Now submit 2 jobs against accounting group <code>a</code>:</p>

<pre><code>universe = vanilla
cmd = /bin/sleep
args = 60
should_transfer_files = if_needed
when_to_transfer_output = on_exit
+AccountingGroup="a.u"
queue 2
</code></pre>

<p>We see that accounting groups are respected: one job runs, and it does not suffer from insufficient share to acquire resources from <code>slot1</code> <a href="https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3013">(GT3013)</a>, because match cost is computed using only the individual job's impact on slot weight, instead of being required to match the entire p-slot:</p>

<pre><code>$ tail -f ~/condor/local/log/NegotiatorLog | grep -e '\-\-\-\-\-' -e matched
12/03/12 14:57:50 ---------- Finished Negotiation Cycle ----------
12/03/12 14:58:08 ---------- Started Negotiation Cycle ----------
12/03/12 14:58:08       Successfully matched with slot1@rorschach
12/03/12 14:58:09 ---------- Finished Negotiation Cycle ----------

$ ccdump condor_status Name TotalSlotCpus
slot1@rorschach | 10
slot1_1@rorschach | 1
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Semantics of Group Quotas and Slot Weights: Computing Claim Capacity from Consumption Policy]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/26/rethinking-the-semantics-of-group-quotas-and-slot-weights-computing-claim-capacity-from-consumption-policy/"/>
    <updated>2012-11-26T13:52:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/26/rethinking-the-semantics-of-group-quotas-and-slot-weights-computing-claim-capacity-from-consumption-policy</id>
    <content type="html"><![CDATA[<p>In two previous posts, I made a case to motivate the need for a better definition of slot weights and group quotas that could accommodate use cases involving aggregate resources (partitionable slots) with heterogeneous consumption policies and also provide a principled unit analysis for weights and quotas.  These previous posts can be viewed here:</p>

<ul>
<li><a href="http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources/">Rethinking the Semantics of Group Quotas and Slot Weights for Heterogeneous and Multidimensional Compute Resources</a></li>
<li><a href="http://erikerlandson.github.com/blog/2012/11/15/rethinking-the-semantics-of-group-quotas-and-slot-weights-claim-capacity-model/">Rethinking the Semantics of Group Quotas and Slot Weights: Claim Capacity Model</a></li>
</ul>


<p>As previously mentioned, a Claim Capacity Model of accounting group quotas and slot weights (or "resource consumption costs") requires a resource claiming model that assigns a well defined finite value for the maximum number of claims that each resource and its consumption policy can support.  It must also be re-evaluatable on a resource as its assets are consumed, so that the cost of a proposed claim (or match, in negotiation-speak) can be defined as W(R) - W(R'), were R' embodies the amounts of all assets remaining after the claim has taken its share.  (Here, I will be using the term 'assets' to refer to quantities such as cpus, memory, disk, swap or any <a href="http://spinningmatt.wordpress.com/2012/11/19/extensible-machine-resources/">extensible resources</a> defined, to clarify the difference between an aggregate resource (i.e. a partitionable slot) versus a single resource dimension such as cpus, memory, etc).</p>

<p>This almost immediately raises the question of how best to define such a resource claiming model.  In this post I will briefly describe a few possible approaches, focusing on models which are easy reason about, easy to configure and additionally allow claim capacity for a resource - W(R) - to be computed automatically for the user, thus making a sane relationship between consumption policies and consumption costs possible to enforce.</p>

<h3>Approach 1: fixed claim consumption model</h3>

<p>The simplest-possible approach is arguably to just directly configure a fixed number, M, of claims attached to a resource.  In this model, each match of a job against a resource consumes one of the M claims.   Here, match cost W(R) - W(R') = 1 in all cases, and is independent of the 'size' of assets consumed from a resource.</p>

<p>A possible use case for such a model is that one might wish to declare that a resource can run up to a certain number of jobs, without assigning any particular cost to consuming individual assets.  If the pool users' workload consists of large numbers of resource-cheap jobs that can effectively share cpu, memory, etc, then such a model might be a good fit.</p>

<h3>Approach 2: configure asset quantization levels</h3>

<p>Another approach that makes the relation between consumption policy and claim capacity easy to think about is to configure a quantization level for each resource asset.  For example, here we might quantize memory into 20 levels, i.e. Q(memory) = 20.  Similarly we might define Q(cpus) = 10 (note that HTCondor does not currently handle fractional cpus on resources, but this kind of model would benefit if floating point asset fractions were supported).  At any time, a resource R has some number q(a) left of the original Q(a).  A job requests an amount r(a) for asset (a).   Here, a claim gets a quantized approximation of any requested asset = V(a)(n(a)/Q(a)), where V(a) is the total original value available for asset (a), and n(a) = ceiling(r(a)Q(a)/V(a)).   Here there are two possible sub-policies.  If we demand that each claim consume >= 1 quantum of every asset (i.e. n(a) >= 1), then the claim capacity W(R) is the minimum of q(a), for (a) over all assets.  However, if a claim is allowed to consume a zero quantity of some individual assets (n(a)=0), then the claim capacity is the <em>maximum</em> of the q(a).   In this case, one must address the corner case of a claim attempting to consume (n(a)=0) over all assets.  The resulting resource R' has q'(a) = q(a)-n(a), and W(R') is the minium (or maximum) over the new q'(a).</p>

<h3>Approach 3: configure minimum asset charges</h3>

<p>A third approach is to configure a <em>minimum</em> amount of each asset that any claim must be charged.   For example, we might define a minimum amount of memory C(memory) to charge any claim.   If a job requests an amount r(a), it will always receive max(r(a), C(a)).  As above, q(a) is the number of quanta currently available for asset (a).  Let v(a) be the amount of (a) currently available.  Here we define q(a) for an asset (a) to be floor(v(a)/C(a)).   If we adhere to a reasonable restriction that C(a) must be strictly > 0 for all (a), we are guaranteed a well defined W(R) = min over the q(a).</p>

<p>It is an open question which of these models (or some other completely different options) should be supported.  Conceivably all of them could be provided as options.</p>

<p>Currently my personal preference leans toward Approach 3.  It is easy to reason about and configure.  It yields a well defined W(R) in all circumstances, with no corner cases, that is straightforward to compute and enforce automatically.  It is easy to configure heterogeneous consumption policies that cost different resource assets in different ways, simply by tuning minimum charge C(a) appropriately for each asset.  This includes claim capacity models where jobs are assumed to use very small amounts of any resource, including fractional shares of cpu assets.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Semantics of Group Quotas and Slot Weights: Claim Capacity Model]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/15/rethinking-the-semantics-of-group-quotas-and-slot-weights-claim-capacity-model/"/>
    <updated>2012-11-15T17:22:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/15/rethinking-the-semantics-of-group-quotas-and-slot-weights-claim-capacity-model</id>
    <content type="html"><![CDATA[<p>In my previous post about <a href="http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources">Rethinking the Semantics of Group Quotas and Slot Weights</a>, I proposed a concept for unifying the semantics of accounting group quotas and slot weights across arbitrary resource allocation strategies.</p>

<p>My initial terminology was that the weight of a slot (i.e. resource ad) is a measure of the <em>maximum</em> number of jobs that might match against that ad, given the currently available resource quantities and the allocation policy.  The cost of a match becomes the amount by which that measure is reduced, after the match's resources are removed from the ad.</p>

<p>In the HTCondor vocabulary, a job acquires a <em>claim</em> on resources to actually run after it has been matched.  It has been proposed that it may be beneficial for HTCondor to evolve toward a model where there are (aggregate) resource ads, and claims against those ads, as a simplification of the current model which involves static, partitionable and dynamic slots, with claims.  With this in mind, a preferable terminology for group quota and weight semantics might be that a resource ad (or slot) has a measure of the maximum number of claims it could dispense: a <em>claim capacity</em> measure.  The cost of a claim (or match) is the corresponding reduction of the resource's claim capacity.</p>

<p>So, this semantic model could be referred to as the Claim Capacity Model of group quotas and slot weights.  With this terminology, the shared 'unit' for group quotas and slot weights would be <em>claims</em> instead of <em>jobs</em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rethinking the Semantics of Group Quotas and Slot Weights for Heterogeneous and Multidimensional Compute Resources]]></title>
    <link href="http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources/"/>
    <updated>2012-11-13T15:31:00-07:00</updated>
    <id>http://erikerlandson.github.com/blog/2012/11/13/rethinking-the-semantics-of-group-quotas-and-slot-weights-for-heterogeneous-and-multidimensional-compute-resources</id>
    <content type="html"><![CDATA[<p>The HTCondor semantic for accounting group quotas and slot weights is currently cpu-centric.  This is an artifact of the historic primacy of cpu as the most commonly-considered limiting resource in computations.  For example the <code>SlotWeight</code> attribute is currently defaulted to <code>Cpus</code>, and when slot weights are disabled, there is logic activated in matchmaking to sum the available cpus on slots to avoid 'undercounting' total pool quotas.</p>

<p>However, HTCondor slots -- the core model of computational resources in an HTCondor pool -- manage four resources by default: cpu, memory, disk and swap.  Furthermore, slots may now be configured with arbitrary custom resources.  As recently mentioned by <a href="http://spinningmatt.wordpress.com/2012/11/13/no-longer-thinking-in-slots-thinking-in-aggregate-resources-and-consumption-policies">Matthew Farrellee</a>, there is a growing pressure to provide robust support not just for traditional cpu-centric resource presentation, usage and allocation, but also seamlessly mediated with memory-centric, gpu-centric or '*'-centric resource allocation policies and more generally allocation policies that are simultaneously aware of all resource dimensions.</p>

<p>This goal immediately raises some questions for the semantics of accounting groups and slot weights when matching jobs against slots during matchmaking.</p>

<p>Consider a pool where 50% of the slots are 'weighted' in a traditional cpu-centric way, but the other 50% are intended to be allocated in a memory-centric way.  This is currently possible, as the <code>SlotWeight</code> attribute can be configured appropriately to be a function of either <code>Cpus</code> or <code>Memory</code>.</p>

<p>But in a scenario where slots are weighted as functions of heterogeneous resource dimensions, it raises a semantic question:  when we sum these weights to obtain the pool-wide available quota, what 'real world' quantity does this total represent -- if any?   Is it a purely heuristic numeric value with no well defined unit attached?</p>

<p>This question has import.  Understanding what the answer is, or should be, impacts what story we tell to users about what their accounting group configuration actually means.  When I assign a quota to an accounting group in such a heterogeneous environment, what is that quota regulating?   When a job matches a cpu-centric slot, does the cost of that match have a different meaning than when matching against a memory-centric slot?   When the slots are partitionable, a match implies a certain multi-dimensional slice of resources allocated from that slot.  What is the cost of that slice?  Does the sum of costs add up to the original weight on the partitionable slot?  If not, how does that affect our understanding of quota semantics?</p>

<p>It may be possible unify all of these ideas by adopting the perspective that a slot's weight is a measure of the maximum number of jobs that can be matched against it.  The cost of a match is W(S)-W(S'), where W(S) is the weight function evaluated on the slot prior to match, and W(S') is the corresponding weight after the match has extracted its requested resources.  The pool's total quota is just the sum of W(S), over all slots S in the pool.  Note, this implies that the 'unit' attached to both slot weights and accounting group quotas is 'jobs'.</p>

<p>Consider a simple example from the traditional cpu-centric configuration:   A partitionable slot is configured with 8 cpus, and <code>SlotWeight</code> is just its default <code>Cpus</code>.  Using this model, the allocation policy is: 'each match must use >= 1 cpu", and that other resource requests are assumed to be not limiting.  The maximum number of matches is 8 jobs, each requesting 1 cpu.   However, a job might also request 2 cpus.  In this case, note that the cost of the match is 2, since the remaining slot has 6 slots, and so W(S') now evaluates to 6.   So, the cost of the match is how many fewer possible jobs the original slot can support after the match takes its requested resources.</p>

<p>This approach can be applied equally well to a memory-centric strategy, or a disk centric strategy, or a gpu-based strategy, or any combination simultaneously.  All weights evaluate to a value with unit 'jobs'.   All match costs are differences between weights (before and after match), and so their values are also in units of 'jobs'.  Therefore, the semantics of the sum of weights over a pool is always well defined: it is a number of jobs, and spefically a measure of the maximum number of jobs that might match against all the slots in the pool.  When a match acquires resources that reduce this maximum by more than 1 job, that is not in any way inconsistent.  It means the job used resources that might have supported two or more 'smaller' jobs.   This means that accounting group quotas (and submitter shares) also have a well defined unit and semantic, which is 'how many (or what fraction of) the maximum possible jobs is this group guaranteed by my accounting policy'</p>

<p>One implication of this proposed semantic for quotas and weights is that the measure for the maximum number of jobs that may match against any given slot must be some finite number.   It implies that all resource dimensions are quantized in some way by the allocation policy.   This scheme would not support a real-valued resource dimension that had no minimum quantization.  I do not think that this is a very heavy-weight requirement, and in fact we have already been moving in that direction with features such as MODIFY_REQUEST_EXPRS_xxx.</p>

<p>When a slot's resource allocation policy is defined over all its resources, what bounds this measure of maximum possible matches?  In a case where each job match <em>must</em> use at least one non-zero quantum of each resource dimension, then the limit is the resource with the mimimum quantized levels.   In a case where jobs may request a zero amount of resources, then the limit is the resource with the maximum quantized levels.  (note, it is required that each match use at least one quantum of at least one resource, otherwise the maximum is not properly bounded).</p>
]]></content>
  </entry>
  
</feed>
