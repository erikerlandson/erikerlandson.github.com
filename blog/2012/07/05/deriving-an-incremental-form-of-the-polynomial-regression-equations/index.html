
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Deriving an Incremental Form of the Polynomial Regression Equations - tool monkey</title>
  <meta name="author" content="Erik Erlandson">

  
  <meta name="description" content="Incremental, or on-line, algorithms are increasingly popular as data set sizes explode and web enabled applications create environments where new &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="tool monkey" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

  <!-- enables inclusion of MathJax LaTeX: http://greglus.com/blog/2011/11/29/integrate-MathJax-LaTeX-and-MathML-Markup-in-Octopress/ -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">tool monkey</a></h1>
  
    <h2>adventures of an unfrozen caveman programmer</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:erikerlandson.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Deriving an Incremental Form of the Polynomial Regression Equations</h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-05T19:46:00-07:00" pubdate data-updated="true">Jul 5<span>th</span>, 2012</time>
        
        
         | <a href="#feedback">Feedback</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Incremental, or on-line, algorithms are increasingly popular as data set sizes explode and web enabled applications create environments where new data arrive continuously (that is, incrementally) from clients out on the internet.</p>

<p>Recently I have been doing some <a href="https://github.com/erikerlandson/ratorade">experiments</a> with applying one of the <em>oldest</em> incremental algorithms to the task of rating predictions: computing a linear regression with a coefficient of correlation.  The incremental formulae look like this:</p>

<div markdown="0">
To find coefficients \( a_0, a_1 \) of the linear predictor \( y = a_0 + a_1 x \):
&#92;[
a_1 = &#92;frac {n &#92;Sigma x y - &#92;Sigma x &#92;Sigma y} {n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^2 }
&#92;hspace{1 cm}
a_0 = &#92;frac { &#92;Sigma y - a_1 &#92;Sigma x } {n}
&#92;]
The correlation coefficient of this predictor is given by:
&#92;[
&#92;rho (x,y) = &#92;frac {n &#92;Sigma x y - &#92;Sigma x &#92;Sigma y} {&#92;sqrt {n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^ 2 } &#92;sqrt {n &#92;Sigma y^2 - &#92;left( &#92;Sigma y &#92;right) ^ 2 } }
&#92;]
</div>


<p>As you can see from the formulae above, it is sufficient to maintain running sums</p>

<div markdown="0"> &#92;[ n, &#92;Sigma x, &#92;Sigma y, &#92;Sigma x^2, &#92;Sigma y^2, &#92;Sigma x y &#92;] </div>


<p>and so any new data can be included incrementally - that is, the model can be updated without revisiting any previous data.</p>

<p>Working with these models caused me to wonder if there was a way to generalize them to obtain incremental formulae for a quadratic predictor, or generalized polynomials.  As it happens, there is.  To show how, I&#8217;ll derive an incremental formula for the coefficients of the quadratic predictor:</p>

<div markdown="0">
&#92;[
y = a_0 + a_1 x + a_2 x^2
&#92;]
</div>


<p>Recall the <a href="http://en.wikipedia.org/wiki/Polynomial_regression#Matrix_form_and_calculation_of_estimates">matrix formula</a> for polynomial regression:</p>

<div markdown="0">
&#92;[ &#92;vec{a} = &#92;left( X^T X &#92;right) ^ {-1} X^T &#92;vec{y} &#92;]

where, in the quadratic case:

&#92;[
&#92;vec{a} = &#92;left( &#92;begin{array} {c}
a_0 &#92;&#92;
a_1 &#92;&#92;
a_2 &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
X = &#92;left( &#92;begin{array} {ccc}
1 & x_1 & x_1^2 &#92;&#92;
1 & x_2 & x_2^2 &#92;&#92;
  &  \vdots  & &#92;&#92;
1 & x_n & x_n^2 &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
&#92;vec{y} = &#92;left( &#92;begin{array} {c}
y_1 &#92;&#92;
y_2 &#92;&#92;
\vdots &#92;&#92;
y_n &#92;&#92;
&#92;end{array} &#92;right)
&#92;]

Note that we can apply the definition of matrix multiplication and express the two products &#92;( X^T X &#92;) and &#92;( X^T &#92;vec{y} &#92;) from the above formula like so:
&#92;[
X^T X = 
&#92;left( &#92;begin{array} {ccc}
n & &#92;Sigma x & &#92;Sigma x^2 &#92;&#92;
&#92;Sigma x & &#92;Sigma x^2 & &#92;Sigma x^3 &#92;&#92;
&#92;Sigma x^2 & &#92;Sigma x^3 & &#92;Sigma x^4 &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
X^T &#92;vec{y} =
&#92;left( &#92;begin{array} {c}
&#92;Sigma y &#92;&#92;
&#92;Sigma x y &#92;&#92;
&#92;Sigma x^2 y &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>And so now we can express the formula for our quadratic coefficients in this way:</p>

<div markdown="0">
&#92;[
&#92;left( &#92;begin{array} {c}
a_0 &#92;&#92;
a_1 &#92;&#92;
a_2 &#92;&#92;
&#92;end{array} &#92;right)
=
&#92;left( &#92;begin{array} {ccc}
n & &#92;Sigma x & &#92;Sigma x^2 &#92;&#92;
&#92;Sigma x & &#92;Sigma x^2 & &#92;Sigma x^3 &#92;&#92;
&#92;Sigma x^2 & &#92;Sigma x^3 & &#92;Sigma x^4 &#92;&#92;
&#92;end{array} &#92;right)
^ {-1}
&#92;left( &#92;begin{array} {c}
&#92;Sigma y &#92;&#92;
&#92;Sigma x y &#92;&#92;
&#92;Sigma x^2 y &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>Note that we now have a matrix formula that is expressed entirely in sums of various terms in x and y, which means that it can be maintained incrementally, as we desired.  If you have access to a matrix math package, you might very well declare victory right here, as you can easily construct these matrices and do the matrix arithmetic at will to obtain the model coefficients.  However, as an additional step I applied <a href="http://www.sagemath.org/">sage</a> to do the symbolic matrix inversion and multiplication to give:</p>

<div markdown="0">
&#92;[
&#92;small
a_0 =
&#92;frac {1} {Z}
&#92;left( 
- &#92;left( &#92;Sigma x^3 &#92;Sigma x - &#92;left( &#92;Sigma x^2 &#92;right)^2 &#92;right) &#92;Sigma x^2 y  +  &#92;left( &#92;Sigma x^4  &#92;Sigma x - &#92;Sigma x^3 &#92;Sigma x^2 &#92;right) &#92;Sigma x y  -  &#92;left( &#92;Sigma x^4 &#92;Sigma x^2 - &#92;left( &#92;Sigma x^3 &#92;right)^2 &#92;right) &#92;Sigma y 
&#92;right)
&#92;normalsize
&#92;]
&#92;[
&#92;small
a_1 =
&#92;frac {1} {Z}
&#92;left( 
&#92;left( n &#92;Sigma x^3  - &#92;Sigma x^2 &#92;Sigma x &#92;right) &#92;Sigma x^2 y  -  &#92;left( n &#92;Sigma x^4 - &#92;left( &#92;Sigma x^2 &#92;right) ^2 &#92;right) &#92;Sigma x y  +  &#92;left( &#92;Sigma x^4 &#92;Sigma x - &#92;Sigma x^3 &#92;Sigma x^2 &#92;right) &#92;Sigma y
&#92;right)
&#92;normalsize
&#92;]
&#92;[
&#92;small
a_2 =
&#92;frac {1} {Z}
&#92;left( 
- &#92;left( n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^2 &#92;right) &#92;Sigma x^2 y  +  &#92;left( n &#92;Sigma x^3 - &#92;Sigma x^2 &#92;Sigma x &#92;right) &#92;Sigma x y  -  &#92;left( &#92;Sigma x^3 &#92;Sigma x - &#92;left( &#92;Sigma x^2 &#92;right) ^2 &#92;right) &#92;Sigma y 
&#92;right)
&#92;normalsize
&#92;]
where:
&#92;[
Z = n &#92;left( &#92;Sigma x^3 &#92;right) ^ 2 - 2 &#92;Sigma x^3 &#92;Sigma x^2 &#92;Sigma x + &#92;left( &#92;Sigma x^2 &#92;right) ^3 - &#92;left( n &#92;Sigma x^2 - &#92;left( &#92;Sigma x &#92;right) ^2  &#92;right) &#92;Sigma x^4
&#92;]
</div>


<p>Inspecting the quadratic derivation above, it is now fairly easy to see that the general form of the incremental matrix formula for the coefficients of a degree-m polynomial looks like this:</p>

<div markdown="0">
&#92;[
&#92;left( &#92;begin{array} {c}
a_0 &#92;&#92;
a_1 &#92;&#92;
\vdots &#92;&#92;
a_m &#92;&#92;
&#92;end{array} &#92;right)
=
&#92;left( &#92;begin{array} {cccc}
n & &#92;Sigma x & &#92;cdots & &#92;Sigma x^m &#92;&#92;
&#92;Sigma x & &#92;Sigma x^2 & &#92;cdots & &#92;Sigma x^{m+1} &#92;&#92;
&#92;vdots & & &#92;ddots & &#92;vdots &#92;&#92;
&#92;Sigma x^m & &#92;Sigma x^{m+1} & &#92;cdots & &#92;Sigma x^{2 m} &#92;&#92;
&#92;end{array} &#92;right)
^ {-1}
&#92;left( &#92;begin{array} {c}
&#92;Sigma y &#92;&#92;
&#92;Sigma x y &#92;&#92;
&#92;vdots &#92;&#92;
&#92;Sigma x^m y &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>Having an incremental formula for generalized polynomial regression leaves open the question of how one might generalize the correlation coefficient.  There is such a generalization, called the <a href="http://en.wikipedia.org/wiki/Multiple_correlation">coefficient of multiple determination</a>, which is defined:</p>

<div markdown="0">
&#92;[
r = &#92;sqrt { &#92;vec{c} ^ T  R^{-1}  &#92;vec{c} }
&#92;]
Where
&#92;[
&#92;vec{c} = 
&#92;left ( &#92;begin{array} {c}
&#92;rho (x,y) &#92;&#92;
&#92;rho (x^2,y) &#92;&#92;
&#92;vdots &#92;&#92;
&#92;rho (x^m,y) &#92;&#92;
&#92;end{array} &#92;right)
&#92;hspace{1 cm}
R =
&#92;left( &#92;begin{array} {cccc}
1 & &#92;rho (x,x^2) & &#92;cdots & &#92;rho(x,x^m) &#92;&#92;
&#92;rho (x^2,x) & 1 & &#92;cdots & &#92;rho(x^2,x^m) &#92;&#92;
&#92;vdots & & &#92;ddots & &#92;vdots &#92;&#92;
&#92;rho (x^m,x) & &#92;rho (x^m,x^2) & &#92;cdots & 1 &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
and &#92;( &#92;rho (x,y) &#92;) is the traditional pairwise correlation coefficient.
</div>


<p>But we already have an incremental formula for any pairwise correlation coefficient, which is defined above.  And so we can maintain the running sums needed to fill the matrix entries, and compute the coefficient of multiple determination for our polynomial model at any time.</p>

<p>So we now have incremental formulae to maintain any polynomial model in an on-line environment where we either can&#8217;t or prefer not to store the data history, and also incrementally evaluate the &#8216;generalized correlation coefficient&#8217; for that model.</p>

<p>Readers familiar with linear regression may notice that there is also nothing special about polynomial regression, in the sense that powers of x may also be replaced with arbitrary functions of x, and the same regression equations hold.  And so we might generalize the incremental matrix formulae further to replace products of powers of x with products of functions of x:</p>

<div markdown="0">
for a linear regression model &#92;( y = a_1 f_1 (x) + a_2 f_2 (x) + &#92;cdots + a_m f_m(x) &#92;) :
&#92;[
&#92;left( &#92;begin{array} {c}
a_1 &#92;&#92;
a_2 &#92;&#92;
\vdots &#92;&#92;
a_m &#92;&#92;
&#92;end{array} &#92;right)
=
&#92;left( &#92;begin{array} {cccc}
&#92;Sigma f_1 (x) f_1 (x) & &#92;Sigma f_1 (x) f_2 (x) & &#92;cdots & &#92;Sigma f_1 (x) f_m (x) &#92;&#92;
&#92;Sigma f_2 (x) f_1 (x) & &#92;Sigma f_2 (x) f_2 (x) & &#92;cdots & &#92;Sigma f_2 (x) f_m (x) &#92;&#92;
&#92;vdots & & &#92;ddots & &#92;vdots &#92;&#92;
&#92;Sigma f_m (x) f_1 (x) & &#92;Sigma f_m (x) f_2 (x) & &#92;cdots & &#92;Sigma f_m (x) f_m (x) &#92;&#92;
&#92;end{array} &#92;right)
^ {-1}
&#92;left( &#92;begin{array} {c}
&#92;Sigma y f_1 (x) &#92;&#92;
&#92;Sigma y f_2 (x) &#92;&#92;
&#92;vdots &#92;&#92;
&#92;Sigma y f_m (x) &#92;&#92;
&#92;end{array} &#92;right)
&#92;]
</div>


<p>The coefficient of multiple determination generalizes in the analogous way.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Erik Erlandson</span></span>

      








  


<time datetime="2012-07-05T19:46:00-07:00" pubdate data-updated="true">Jul 5<span>th</span>, 2012</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/computing/'>computing</a>, <a class='category' href='/blog/categories/incremental/'>incremental</a>, <a class='category' href='/blog/categories/linear-regression/'>linear regression</a>, <a class='category' href='/blog/categories/machine-learning/'>machine learning</a>, <a class='category' href='/blog/categories/math/'>math</a>, <a class='category' href='/blog/categories/on-line-learning/'>on-line learning</a>, <a class='category' href='/blog/categories/polynomial-regression/'>polynomial regression</a>
  
</span>


      <br>
<a id="feedback"></a>Feedback • 
 
    <script type="text/javascript" src="//platform.twitter.com/widgets.js"></script>
    
  <a href="https://twitter.com/intent/tweet?text=@manyangled%20re:%20http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations/">Reply to this post on Twitter</a> • 
 
<a href="mailto:erikerlandson@yahoo.com">Email the author</a>

    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations/" data-via="manyangled" data-counturl="http://erikerlandson.github.com/blog/2012/07/05/deriving-an-incremental-form-of-the-polynomial-regression-equations/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2012/06/29/easy-histograms-and-tables-from-condor-jobs-and-slots/" title="Previous Post: Easy Histograms and Tables from Condor Jobs and Slots">&laquo; Easy Histograms and Tables from Condor Jobs and Slots</a>
      
      
        <a class="basic-alignment right" href="/blog/2012/07/10/configuring-minimum-and-maximum-resources-for-mission-critical-jobs-in-a-condor-pool/" title="Next Post: Configuring Minimum and Maximum Resources for Mission Critical Jobs in a Condor Pool">Configuring Minimum and Maximum Resources for Mission Critical Jobs in a Condor Pool &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>About The Author</h1>
  <p>Erik is a software engineer at <a href="http://www.redhat.com">Red Hat</a> where he contributes to the <a href="https://radanalytics.io/">radanalytics.io</a> upstream community.
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">Computing Smooth Max and its Gradients Without Over- and Underflow</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">The Gradient and Hessian of the Smooth Max Over Functions</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/08/23/generalizing-the-concept-of-release-versioning/">Rethinking the Concept of Release Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/19/converging-monoid-addition-for-t-digest/">Converging Monoid Addition for T-Digest</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid/">Encoding Map-Reduce As A Monoid With Left Folding</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/erikerlandson">@erikerlandson</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'erikerlandson',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("manyangled", 4, true);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/manyangled" class="twitter-follow-button" data-show-count="false">Follow @manyangled</a>
  
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Erik Erlandson -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
