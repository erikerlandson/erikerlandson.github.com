
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>tool monkey</title>
  <meta name="author" content="Erik Erlandson">

  
  <meta name="description" content="In my previous post I derived the gradient and Hessian for the smooth max function.
The Notorious JDC wrote a helpful companion post that describes &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://erikerlandson.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="tool monkey" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

  <!-- enables inclusion of MathJax LaTeX: http://greglus.com/blog/2011/11/29/integrate-MathJax-LaTeX-and-MathML-Markup-in-Octopress/ -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">tool monkey</a></h1>
  
    <h2>adventures of an unfrozen caveman programmer</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:erikerlandson.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">Computing Smooth Max and its Gradients Without Over- and Underflow</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2018-05-28T08:13:00-07:00" pubdate data-updated="true">May 28<span>th</span>, 2018</time>
        
        
         | <a href="/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In my <a href="http://erikerlandson.github.io/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">previous post</a> I derived the gradient and Hessian for the smooth max function.
The <a href="https://www.johndcook.com/blog/">Notorious JDC</a> wrote a helpful companion post that describes <a href="https://www.johndcook.com/blog/2010/01/20/how-to-compute-the-soft-maximum/">computational issues</a> of overflow and underflow with smooth max;
values of f<sub>k</sub> don&#8217;t have to grow very large (or small) before floating point limitations start to force their exponentials to +inf or zero.
In JDC&#8217;s post he discusses this topic in terms of a two-valued smooth max.
However it isn&#8217;t hard to generalize the idea to a collection of f<sub>k</sub>.
Start by taking the maximum value over our collection of functions, which I&#8217;ll define as (z):</p>

<p><img src="/assets/images/smoothmax/eq1b.png" alt="eq1" /></p>

<p>As JDC described in his post, this alternative expression for smooth max (m) is computationally stable.
Individual exponential terms may underflow to zero, but they are the ones which are dominated by the other terms, and so approximating them by zero is numerically accurate.
In the limit where one value dominates all others, it will be exactly the value given by (z).</p>

<p>It turns out that we can play a similar trick with computing the gradient:</p>

<p><img src="/assets/images/smoothmax/eq2b.png" alt="eq2" /></p>

<p>Without showing the derivation, we can apply exactly the same manipulation to the terms of the Hessian:</p>

<p><img src="/assets/images/smoothmax/eq3b.png" alt="eq3" /></p>

<p>And so we now have a computationally stable form of the equations for smooth max, its gradient and its Hessian. Enjoy!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">The Gradient and Hessian of the Smooth Max Over Functions</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2018-05-27T09:36:00-07:00" pubdate data-updated="true">May 27<span>th</span>, 2018</time>
        
        
         | <a href="/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Suppose you have a set of functions over a vector space, and you are interested in taking the smooth-maximum over those functions.
For example, maybe you are doing gradient descent, or convex optimization, etc, and you need a variant on &#8220;maximum&#8221; that has a defined gradient.
The smooth maximum function has both a defined gradient and Hessian, and in this post I derive them.</p>

<p>I am using the <a href="https://www.johndcook.com/blog/2010/01/13/soft-maximum/">logarithm-based</a> definition of smooth-max, shown here:</p>

<p><img src="/assets/images/smoothmax/eq1.png" alt="eq1" /></p>

<p>I will use the second variation above, ignoring function arguments, with the hope of increasing clarity.
Applying the chain rule gives the ith partial gradient of smooth-max:</p>

<p><img src="/assets/images/smoothmax/eq2.png" alt="eq2" /></p>

<p>Now that we have an ith partial gradient, we can take the jth partial gradient of <em>that</em> to obtain the (i,j)th element of a Hessian:</p>

<p><img src="/assets/images/smoothmax/eq3.png" alt="eq3" /></p>

<p>This last re-grouping of terms allows us to see that we can express the full gradient and Hessian in the following more compact way:</p>

<p><img src="/assets/images/smoothmax/eq4.png" alt="eq4" /></p>

<p>With a gradient and Hessian, we now have the tools we need to use smooth-max in algorithms such as gradient descent and convex optimization. Happy computing!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/23/generalizing-the-concept-of-release-versioning/">Rethinking the Concept of Release Versioning</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-08-23T17:22:00-07:00" pubdate data-updated="true">Aug 23<span>rd</span>, 2017</time>
        
        
         | <a href="/blog/2017/08/23/generalizing-the-concept-of-release-versioning/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Recently I&#8217;ve been thinking about a few related problems with our current concepts of software release versioning, release dependencies and release building.
These problems apply to software releases in all languages and build systems that I&#8217;ve experienced,
but in the interest of keeping this post as simple as possible I&#8217;m going to limit myself to talking about the Maven ecosystem of release management and build tooling.</p>

<p>Consider how we annotate and refer to release builds for a Scala project:
The <em>version</em> of Scala &#8211; 2.10, 2.11, etc &#8211; that was used to build the project is a <em>qualifier</em> for the release.
For example, if I am building a project using Scala 2.11, and package P is one of my project dependencies, then the maven build tooling (or sbt, etc) looks for a version of P that was <em>also</em> built using Scala 2.11;
the build will fail if no such incarnation of P can be located.
This build constraint propagates recursively throughout the entire dependency tree for a project.</p>

<p>Now consider how we treat the version for the package P dependency itself:
Our build tooling forces us to specify one exact release version x.y.z for P.
This is superficially similar to the constraint for building with Scala 2.11, but <em>unlike</em> the Scala constraint, the knowledge about using P x.y.z is not propagated down the tree.</p>

<p>If the dependency for P appears only once in the depenency tree, everything is fine.
However, as anybody who has ever worked with a large dependency tree for a project knows, package P might very well appear in multiple locations of the dep-tree, as a transitive dependency of different packages.
Worse, these deps may be specified as <em>different versions</em> of P, which may be mutually incompatible.</p>

<p>Transitive dep incompatibilities are a particularly thorny problem to solve, but there are other annoyances related to release versioning.
Often a user would like a &#8220;major&#8221; package dependency built against a particular version of that dep.
For example, packages that use Apache Spark may need to work with a particular build version of Spark (2.1, 2.2, etc).
If I am the package purveyor, I have no very convenient way to build my package against multiple versions of spark, and then annotate those builds in Maven Central.
At best I can bake the spark version into the name.
But what if I want to specify other package dep verions?
Do I create package names with increasingly-long lists of (package,version) pairs hacked into the name?</p>

<p>Finally, there is simply the annoyance of revving my own package purely for the purpose of building it against the latest versions of my dependencies.
None of my code has changed, but I am cutting a new release just to pick up current dependency releases.
And then hoping that my package users will want those particular releases, and that these won&#8217;t break <em>their</em> builds with incompatible transitive deps!</p>

<p>I have been toying with a release and build methodology for avoiding these headaches. What follows is full of vigorous hand-waving,
but I believe something like it could be formalized in a useful way.</p>

<p>The key idea is that a release <em>build</em> is defined by a <em>build signature</em> which is the union of all <code>(dep, ver)</code> pairs.
This includes:</p>

<ol>
<li>The actual release version of the package code, e.g. <code>(mypackage, 1.2.3)</code></li>
<li>The <code>(dep, ver)</code> for all dependencies (taken over all transitive deps, recursively)</li>
<li>The <code>(tool, ver)</code> for all impactful build tooling, e.g. <code>(scala, 2.11)</code>, <code>(python, 3.5)</code>, etc</li>
</ol>


<p>For example, if I maintain a package <code>P</code>, whose latest code release is <code>1.2.3</code>,
built with dependencies <code>(A, 0.5)</code>, <code>(B, 2.5.1)</code> and <code>(C, 1.7.8)</code>, and dependency <code>B</code> built against <code>(Q, 6.7)</code> and <code>(R, 3.3)</code>,
and <code>C</code> built against <code>(Q, 6.7)</code>
and all compiled with <code>(scala, 2.11)</code>, then the build signature will be:</p>

<p><code>{ (P, 1.2.3), (A, 0.5), (B, 2.5.1), (C, 1.7.8), (Q, 6.7), (R, 3.3), (scala, 2.11) }</code></p>

<p>Identifying a release build in this way makes several interesting things possible.
First, it can identify a build with a transitive dependency problem.
For example, if <code>C</code> had been built against <code>(Q, 7.0)</code>,
then the resulting build signature would have <em>two</em> pairs for <code>Q</code>; <code>(Q, 6.7)</code> and <code>(Q, 7.0)</code>,
which is an immediate red flag for a potential problem.</p>

<p>More intriguingly, it could provide a foundation for <em>avoiding</em> builds with incompatible dependencies.
Suppose that I redefine my build logic so that I only specify dependency package names, and not specific versions.
Whenever I build a project, the build system automatically searches for the most-recent version of each dependency.
This already addresses some of the release headaches above.
As a project builder, I can get the latest versions of packages when I build.
As a package maintainer, I do not have to rev a release just to update my package deps;
projects using my package will get the latest by default.
Moreover, because the latest package release is always pulled, I never get multiple incompatible dependency releases
in a build.</p>

<p>Suppose that for some reason I <em>need</em> a particular release of some dependency.
From the example above, imagine that I must use <code>(Q, 6.7)</code>.
We can imagine augmenting the build specification to allow overriding the default behavior of pulling the most recent release.
We might either specify a specific version as we do currently, or possibly specify a range of releases, as systems like brew or ruby gemfiles allow.
In the case where some constraint is placed on releases, this constraint would be propagaged down the tree (or possibly up from the leaves),
in essentially the same way that the constraint of scala version is already.
In the event that the total set of constraints over the whole dependency tree is not satisfiable, then the build will fail.</p>

<p>With a build annotation system like the one I just described, one could imagine a new role for registries like Maven Central,
where different builds are automatically cached.
The registry could maybe even automatically run CI testing to identify the most-recent versions of package dependencies that satisfy
any given package build,
or perhaps valid dependency release ranges.</p>

<p>To conclude, I believe that re-thinking how we describe the dependencies used to build and annotate package releases,
by generalizing release version to include the release version of all transitive deps (including build tooling as deps),
may enable more flexible ways to both build software releases and specify them for pulling.</p>

<p>Happy Computing!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/12/19/converging-monoid-addition-for-t-digest/">Converging Monoid Addition for T-Digest</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-12-19T13:29:00-07:00" pubdate data-updated="true">Dec 19<span>th</span>, 2016</time>
        
        
         | <a href="/blog/2016/12/19/converging-monoid-addition-for-t-digest/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><blockquote><p>In the days when Sussman was a novice,
Minsky once came to him as he sat hacking at the PDP-6.
&#8220;What are you doing?&#8221;, asked Minsky.
&#8220;I am training a randomly wired neural net to play Tic-tac-toe&#8221;, Sussman replied.
&#8220;Why is the net wired randomly?&#8221;, asked Minsky.
&#8220;I do not want it to have any preconceptions of how to play&#8221;, Sussman said.
Minsky then shut his eyes.
&#8220;Why do you close your eyes?&#8221; Sussman asked his teacher.
&#8220;So that the room will be empty.&#8221;
At that moment, Sussman was enlightened.</p></blockquote>

<p>Recently I&#8217;ve been doing some work with the <a href="https://github.com/isarn/isarn-sketches">t-digest sketching</a> algorithm, from the
<a href="https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf">paper by Ted Dunning and Omar Ertl</a>.
One of the appealing properties of t-digest sketches is that you can &#8220;add&#8221; them together in the monoid sense to produce a combined sketch from two separate sketches.
This property is crucial for sketching data across data partitions in scale-out parallel computing platforms such as Apache Spark or Map-Reduce.</p>

<p>In the original Dunning/Ertl paper, they describe an algorithm for monoidal combination of t-digests based on randomized cluster recombination.  The clusters of the two input sketches are collected together, then randomly shuffled, and inserted into a new t-digest in that randomized order.  In Scala code, this algorithm might look like the following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">combine</span><span class="o">(</span><span class="n">ltd</span><span class="k">:</span> <span class="kt">TDigest</span><span class="o">,</span> <span class="n">rtd</span><span class="k">:</span> <span class="kt">TDigest</span><span class="o">)</span><span class="k">:</span> <span class="kt">TDigest</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// randomly shuffle input clusters and re-insert to a new t-digest</span>
</span><span class='line'>  <span class="n">shuffle</span><span class="o">(</span><span class="n">ltd</span><span class="o">.</span><span class="n">clusters</span><span class="o">.</span><span class="n">toVector</span> <span class="o">++</span> <span class="n">rtd</span><span class="o">.</span><span class="n">clusters</span><span class="o">.</span><span class="n">toVector</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="nc">TDigest</span><span class="o">.</span><span class="n">empty</span><span class="o">)((</span><span class="n">d</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">d</span> <span class="o">+</span> <span class="n">e</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>I implemented this algorithm and used it until I noticed that a sum over multiple sketches seemed to behave noticeably differently than either the individual inputs, or the nominal underlying distribution.</p>

<p>To get a closer look at what was going on, I generated some random samples from a Normal distribution ~N(0,1).
I then generated t-digest sketches of each sample, took a cumulative monoid sum, and kept track of how closely each successive sum adhered to the original ~N(0,1) distribution.
As a measure of the difference between a t-digest sketch and the original distribution, I computed the Kolmogorov-Smirnov <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov.E2.80.93Smirnov_statistic">D-statistic</a>, which yields a distance between two cumulative distribution functions.
(Code for my data collections can be viewed <a href="https://github.com/erikerlandson/isarn-sketches-algebird-api/blob/blog/t_digest_sum/src/main/scala/org/isarnproject/sketchesAlgebirdAPI/AlgebirdFactory.scala#L65">here</a>)
I ran multiple data collections and subsequent cumulative sums and used those multiple measurements to generate the following box-plot.
The result was surprising and a bit disturbing:</p>

<p><img src="/assets/images/tdsum/plot1.png" alt="plot1" /></p>

<p>As the plot shows, the t-digest sketch distributions are gradually <em>diverging</em> from the underlying &#8220;true&#8221; distribution ~N(0,1).
This is a potentially significant problem for the stability of monoidal t-digest sums, and by extension any parallel sketching based on combining the partial sketches on data partitions in map-reduce-like environments.</p>

<p>Seeing this divergence motivated me to think about ways to avoid it.
One property of t-digest insertion logic is that the results of inserting new data can differ depending on what clusters are already present.
I wondered if the results might be more stable if the largest clusters were inserted first.
The t-digest algorithm allows clusters closest to the distribution median to grow the largest.
Combining input clusters from largest to smallest would be like building the combined distribution from the middle outwards, toward the distribution tails.
In the case where one t-digest had larger weights, it would also somewhat approximate inserting the smaller sketch into the larger one.
In Scala code, this alternative monoid addition looks like so:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">combine</span><span class="o">(</span><span class="n">ltd</span><span class="k">:</span> <span class="kt">TDigest</span><span class="o">,</span> <span class="n">rtd</span><span class="k">:</span> <span class="kt">TDigest</span><span class="o">)</span><span class="k">:</span> <span class="kt">TDigest</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// insert clusters from largest to smallest</span>
</span><span class='line'>  <span class="o">(</span><span class="n">ltd</span><span class="o">.</span><span class="n">clusters</span><span class="o">.</span><span class="n">toVector</span> <span class="o">++</span> <span class="n">rtd</span><span class="o">.</span><span class="n">clusters</span><span class="o">.</span><span class="n">toVector</span><span class="o">).</span><span class="n">sortWith</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">&gt;</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="nc">TDigest</span><span class="o">.</span><span class="n">empty</span><span class="o">(</span><span class="n">delta</span><span class="o">))((</span><span class="n">d</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">d</span> <span class="o">+</span> <span class="n">e</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As a second experiment, for each data sampling I compared the original monoid addition with the alternative method using largest-to-smallest cluster insertion.
When I plotted the resulting progression of D-statistics side-by-side, the results were surprising:</p>

<p><img src="/assets/images/tdsum/plot2a.png" alt="plot2a" /></p>

<p>As the plot demonstrates, not only was large-to-small insertion more stable, its D-statistics appeared to be getting <em>smaller</em> instead of larger.
To see if this trend was sustained over longer cumulative sums, I plotted the D-stats for cumulative sums over 100 samples:</p>

<p><img src="/assets/images/tdsum/plot2.png" alt="plot2" /></p>

<p>The results were even more dramatic;
These longer sums show that the standard randomized-insertion method continues to diverge,
but in the case of large-to-small insertion the cumulative t-digest sums continue to converge
towards the underlying distribution!</p>

<p>To test whether this effect might be dependent on particular shapes of distribution, I ran similar experiments using a Uniform distribution (no &#8220;tails&#8221;) and an Exponential distribution (one tail).
I included the corresponding plots in the appendix.
The convergence of this alternative monoid addition doesn&#8217;t seem to be sensitive to shape of distribution.</p>

<p>I have upgraded my <a href="https://github.com/isarn/isarn-sketches#t-digest">implementation of t-digest sketching</a> to use this new definition of monoid addition for t-digests.
As you can see, it is easy to change one implementation for another.
One or two lines of code may be sufficient.
I hope this idea may be useful for any other implementations in the community.
Happy sketching!</p>

<h3>Appendix: Plots with Alternate Distributions</h3>

<p><img src="/assets/images/tdsum/plot3.png" alt="plot3" /></p>

<p><img src="/assets/images/tdsum/plot4.png" alt="plot4" /></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid/">Encoding Map-Reduce As A Monoid With Left Folding</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-09-05T10:31:00-07:00" pubdate data-updated="true">Sep 5<span>th</span>, 2016</time>
        
        
         | <a href="/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In a <a href="http://erikerlandson.github.io/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/">previous post</a> I discussed some scenarios where traditional map-reduce (directly applying a map function, followed by some monoidal reduction) could be inefficient.
To review, the source of inefficiency is in situations where the <code>map</code> operation is creating some non-trivial monoid that represents a single element of the input type.
For example, if the monoidal type is <code>Set[Int]</code>, then the mapping function (&#8216;prepare&#8217; in algebird) maps every input integer <code>k</code> into <code>Set(k)</code>, which is somewhat expensive.</p>

<p>In that discussion, I was focusing on map-reduce as embodied by the algebird <code>Aggregator</code> type, where <code>map</code> appears as the <code>prepare</code> function.
However, it is easy to see that <em>any</em> map-reduce implementation may be vulnerable to the same inefficiency.</p>

<p>I wondered if there were a way to represent map-reduce using some alternative formulation that avoids this vulnerability.
There is such a formulation, which I will talk about in this post.</p>

<p>I&#8217;ll begin by reviewing a standard map-reduce implementation.
The following scala code sketches out the definition of a monoid over a type <code>B</code> and a map-reduce interface.
As this code suggests, the <code>map</code> function maps input data of some type <code>A</code> into some <em>monoidal</em> type <code>B</code>, which can be reduced (aka &#8220;aggregated&#8221;) in a way that is amenable to parallelization:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">Monoid</span><span class="o">[</span><span class="kt">B</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// aka &#39;combine&#39; aka &#39;++&#39;</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">plus</span><span class="k">:</span> <span class="o">(</span><span class="kt">B</span><span class="o">,</span> <span class="kt">B</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">B</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// aka &#39;empty&#39; aka &#39;identity&#39;</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">e</span><span class="k">:</span> <span class="kt">B</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">trait</span> <span class="nc">MapReduce</span><span class="o">[</span><span class="kt">A</span>, <span class="kt">B</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// monoid embodies the reducible type</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">monoid</span><span class="k">:</span> <span class="kt">Monoid</span><span class="o">[</span><span class="kt">B</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// mapping function from input type A to reducible type B</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">map</span><span class="k">:</span> <span class="kt">A</span> <span class="o">=&gt;</span> <span class="n">B</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// the basic map-reduce operation</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">map</span><span class="o">).</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// map-reduce parallelized over data partitions</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">ParSeq</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">]])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span>
</span><span class='line'>    <span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">part</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="n">part</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">map</span><span class="o">).</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>In the parallel version of map-reduce above, you can see that map and reduce are executed on each data partition (which may occur in parallel) to produce a monoidal <code>B</code> value, followed by a final reduction of those intermediate results.
This is the classic form of map-reduce popularized by tools such as Hadoop and Apache Spark, where inidividual data partitions may reside across highly parallel commodity clusters.</p>

<p>Next I will present an alternative definition of map-reduce.
In this implementation, the <code>map</code> function is replaced by a <code>foldL</code> function, which executes a single &#8220;left-fold&#8221; of an input object with type <code>A</code> into the monoid object with type <code>B</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// a map reduce operation based on a monoid with left folding</span>
</span><span class='line'><span class="k">trait</span> <span class="nc">MapReduceLF</span><span class="o">[</span><span class="kt">A</span>, <span class="kt">B</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">MapReduce</span><span class="o">[</span><span class="kt">A</span>, <span class="kt">B</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">monoid</span><span class="k">:</span> <span class="kt">Monoid</span><span class="o">[</span><span class="kt">B</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// left-fold an object with type A into the monoid B</span>
</span><span class='line'>  <span class="c1">// obeys type law: foldL(b, a) = b ++ foldL(e, a)</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">foldL</span><span class="k">:</span> <span class="o">(</span><span class="kt">B</span><span class="o">,</span> <span class="kt">A</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">B</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// foldL(e, a) embodies the role of map(a) in standard map-reduce</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">map</span> <span class="k">=</span> <span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">A</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">foldL</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// map-reduce operation is now a single fold-left operation</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">foldL</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// map-reduce parallelized over data partitions</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">ParSeq</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">A</span><span class="o">]])</span><span class="k">:</span> <span class="kt">B</span> <span class="o">=</span>
</span><span class='line'>    <span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">part</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="n">part</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">foldL</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="n">monoid</span><span class="o">.</span><span class="n">e</span><span class="o">)(</span><span class="n">monoid</span><span class="o">.</span><span class="n">plus</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As the comments above indicate, the left-folding function <code>foldL</code> is assumed to obey the law <code>foldL(b, a) = b ++ foldL(e, a)</code>.
This law captures the idea that folding <code>a</code> into <code>b</code> should be the analog of reducing <code>b</code> with a monoid corresponding to the single element <code>a</code>.
Referring to my earlier example, if type <code>A</code> is <code>Int</code> and <code>B</code> is <code>Set[Int]</code>, then <code>foldL(b, a) =&gt; b + a</code>.
Note that <code>b + a</code> is directly inserting single element <code>a</code> into <code>b</code>, which is significantly more efficient than <code>b ++ Set(a)</code>, which is how a typical map-reduce implementation would be required to operate.</p>

<p>This law also gives us the corresponding definition of <code>map(a)</code>, which is <code>foldL(e, a)</code>, or in my example: <code>Set.empty[Int] ++ a</code> or just: <code>Set(a)</code></p>

<p>In this formulation, the basic map-reduce operation is now a single <code>foldLeft</code> operation, instead of a mapping followed by a monoidal reduction.
The parallel version is analoglous.
Each partition uses the new <code>foldLeft</code> operation, and the final reduction of intermediate monoidal results remains the same as before.</p>

<p>The <code>foldLeft</code> function is potentially a much more general operation, and it raises the question of whether this new encoding is indeed parallelizable as before.
I will conclude with a proof that this encoding is also parallelizable;
Note that the law <code>foldL(b, a) = b ++ foldL(e, a)</code> is a significant component of this proof, as it represents the constraint that <code>foldL</code> behaves like an analog of reducing <code>b</code> with a monoidal representation of element <code>a</code>.</p>

<p>In the following proof I used a scala-like pseudo code, described in the introduction:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// given an object mr of type MapReduceFL[A, B]</span>
</span><span class='line'><span class="c1">// and using notation:</span>
</span><span class='line'><span class="c1">// f &lt;==&gt; mr.foldL</span>
</span><span class='line'><span class="c1">// for b1,b2 of type B: b1 ++ b2 &lt;==&gt; mr.plus(b1, b2)</span>
</span><span class='line'><span class="c1">// e &lt;==&gt; mr.e</span>
</span><span class='line'><span class="c1">// [...] &lt;==&gt; Seq(...)</span>
</span><span class='line'><span class="c1">// d1, d2 are of type Seq[A]</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Proof that map-reduce with left-folding is parallelizable</span>
</span><span class='line'><span class="c1">// i.e. mr(d1 ++ d2) == mr(d1) ++ mr(d2)</span>
</span><span class='line'><span class="n">mr</span><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of map-reduce operation</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// Lemma A</span>
</span><span class='line'><span class="o">==</span> <span class="n">mr</span><span class="o">(</span><span class="n">d1</span><span class="o">)</span> <span class="o">++</span> <span class="n">mr</span><span class="o">(</span><span class="n">d2</span><span class="o">)</span>  <span class="c1">// definition of map-reduce (QED)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Proof of Lemma A</span>
</span><span class='line'><span class="c1">// i.e. (d1 ++ d2).foldLeft(e)(f) == d1.foldLeft(e)(f) ++ d2.foldLeft(e)(f)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// proof is by induction on the length of data sequence d2</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// case d2 where length is zero, i.e. d2 == []</span>
</span><span class='line'><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="o">[]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of empty sequence []</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">e</span>  <span class="c1">// definition of identity e</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">[].</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of foldLeft</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// case d2 where length is 1, i.e. d2 == [a] for some a of type A</span>
</span><span class='line'><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="n">f</span><span class="o">(</span><span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">),</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// definition of foldLeft and f</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">f</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// the type-law f(b, a) == b ++ f(e, a)</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">].</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of foldLeft</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// inductive step, assuming proof for d2&#39; of length &lt;= n</span>
</span><span class='line'><span class="c1">// consider d2 of length n+1, i.e. d2 == d2&#39; ++ [a], where d2&#39; has length n</span>
</span><span class='line'><span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>
</span><span class='line'><span class="o">==</span> <span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of d2, d2&#39;, [a]</span>
</span><span class='line'><span class="o">==</span> <span class="n">f</span><span class="o">((</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">),</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// definition of foldLeft and f</span>
</span><span class='line'><span class="o">==</span> <span class="o">(</span><span class="n">d1</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">f</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// type-law f(b, a) == b ++ f(e, a)</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">f</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="n">a</span><span class="o">)</span>  <span class="c1">// induction</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="err">&#39;</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">].</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// def&#39;n of foldLeft</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="o">(</span><span class="n">d2</span><span class="err">&#39;</span> <span class="o">++</span> <span class="o">[</span><span class="kt">a</span><span class="o">]).</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// induction</span>
</span><span class='line'><span class="o">==</span> <span class="n">d1</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span> <span class="o">++</span> <span class="n">d2</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">e</span><span class="o">)(</span><span class="n">f</span><span class="o">)</span>  <span class="c1">// definition of d2 (QED)</span>
</span></code></pre></td></tr></table></div></figure>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/08/31/supporting-competing-apis-in-scala-can-better-package-factoring-help/">Supporting Competing APIs in Scala &#8211; Can Better Package Factoring Help?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-08-31T17:55:00-07:00" pubdate data-updated="true">Aug 31<span>st</span>, 2016</time>
        
        
         | <a href="/blog/2016/08/31/supporting-competing-apis-in-scala-can-better-package-factoring-help/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p> On and off over the last year, I&#8217;ve been working on a <a href="http://erikerlandson.github.io/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/">library</a> of tree and map classes in Scala that happen to make use of some algebraic structures (mostly monoids or related concepts).
 In my initial implementations, I made use of the popular <a href="https://github.com/twitter/algebird">algebird</a> variations on monoid and friends.
 In their incarnation as an <a href="https://github.com/twitter/algebird/pull/496">algebird PR</a> this was uncontroversial to say the least, but lately I have been re-thinking them as a <a href="https://github.com/isarn/isarn/pull/1">third-party Scala package</a>.</p>

<p>This immediately raised some interesting and thorny questions:
in an ecosystem that contains not just <a href="https://github.com/twitter/algebird">algebird</a>, but other popular alternatives such as <a href="https://github.com/typelevel/cats">cats</a> and <a href="https://github.com/scalaz/scalaz">scalaz</a>, what algebra API should I use in my code?
How best to allow the library user to interoperate with the algebra libray of their choice?
Can I accomplish these things while also avoiding any problematic package dependencies in my library code?</p>

<p>In Scala, the second question is relatively straightforward to answer.
I can write my interface using <a href="http://docs.scala-lang.org/tutorials/tour/implicit-conversions">implicit conversions</a>, and provide sub-packages that provide such conversions from popular algebra libraries into the library I actually use in my code.
A library user can import the predefined implicit conversions of their choice, or if necessary provide their own.</p>

<p>So far so good, but that leads immediately back to the first question &#8211; what API should <strong><em>I</em></strong> choose to use internally in my own library?</p>

<p>One obvious approach is to just pick one of the popular options (I might favor <code>cats</code>, for example) and write my library code using that.
If a library user also prefers <code>cats</code>, great.
Otherwise, they can import the appropritate implicit conversions from their favorite alternative into <code>cats</code> and be on their way.</p>

<p>But this solution is not without drawbacks.
Anybody using my library will now be including <code>cats</code> as a transitive dependency in their project, even if they are already using some other alternative.
Although <code>cats</code> is not an enormous library, that represents a fair amount of code sucked into my users&#8217; projects, most of which isn&#8217;t going to be used at all.
More insidiously, I have now introduced the possiblity that the <code>cats</code> version I package with is out of sync with the version my library users are building against.
Version misalignment in transitive dependencies is a land-mine in project builds and very difficult to resolve.</p>

<p>A second approach I might use is to define some abstract algebraic traits of my own.
I can write my libraries in terms of this new API, and then provide implicit conversions from popular APIs into mine.</p>

<p>This approach has some real advantages over the previous.  Being entirely abstract, my internal API will be lightweight.  I have the option of including only the algebraic concepts I need.  It does not introduce any possibly problematic 3rd-party dependencies that might cause code bloat or versioning problems for my library users.</p>

<p>Although this is an effective solution, I find it dissatisfying for a couple reasons.
Firstly, my new internal API effectively represents <em>yet another competing algebra API</em>, and so I am essentially contributing to the proliferating-standards antipattern.</p>

<p><img src="https://imgs.xkcd.com/comics/standards.png" alt="standards" /></p>

<p>Secondly, it means that I am not taking advantage of community knowledge.
The <code>cats</code> library embodies a great deal of cumulative human expertise in both category theory and Scala library design.
What does a good algebra library API look like?
Well, <em>it&#8217;s likely to look a lot like <code>cats</code></em> of course!
The odds that I end up doing an inferior job designing my little internal vanity API are rather higher than the odds that I do as well or better.
The best I can hope for is to re-invent the wheel, with a real possibility that my wheel has corners.</p>

<p>Is there a way to resolve this unpalatable situation?
Can we design our projects to both remain flexible about interfacing with multiple 3rd-party alternatives, but avoid effectively writing <em>yet another alternative</em> for our own internal use?</p>

<p>I hardly have any authoritative answers to this problem, but I have one idea that might move toward a solution.
As I alluded to above, when I write my libraries, I am most frequently <em>only</em> interested in the API &#8211; the abstract interface.
If I did go with writing my own algebra API, I would seek to define purely abstract traits.
Since my intention is that my library users would supply their own favorite library alternative, I would have no need or desire to instantiate any of my APIs.
That function would be provided by the separate sub-projects that provide implicit conversions from community alternatives into my API.</p>

<p>On the other hand, what if <code>cats</code> and <code>algebird</code> factored <em>their</em> libraries in a similar way?
What if I could include a sub-package like <code>cats-kernel-api</code>, or <code>algebird-core-api</code>, which contained <em>only</em> pure abstract traits for monoid, semigroup, etc?
Then I could choose my favorite community API, and code against it, with much less code bloat, and a much reduced vulnerability to any versioning drift.
I would still be free to provide implicit conversions and allow <em>my</em> users to make their own choice of library in their projects.</p>

<p>Although I find this idea attractive, it is certainly not foolproof.
For example, there is never a way to <em>guarantee</em> that versioning drift won&#8217;t break an API.
APIs such as <code>cats</code> and <code>algebird</code> are likely to be unusually amenable to this kind of approach.
After all, their interfaces are primarily driven by underlying mathematical definitions, which are generally as stable as such things ever get.
However, APIs in general tend to be significantly more stable than underlying code.
And the most-stable subsets of APIs might be encoded as traits and exposed this way, allowing other more experimental API components to change at a higher frequency.
Perhaps library packages could even be factored in some way such as <code>library-stable-api</code> and <code>library-unstable-api</code>.
That would clearly add a bit of complication to library trait hierarchies, but the payoff in terms of increased 3rd-party usability might be worth it.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/08/03/x-medoids-using-minimum-description-length-to-identify-the-k-in-k-medoids/">Using Minimum Description Length to Optimize the &#8216;K&#8217; in K-Medoids</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-08-03T20:00:00-07:00" pubdate data-updated="true">Aug 3<span>rd</span>, 2016</time>
        
        
         | <a href="/blog/2016/08/03/x-medoids-using-minimum-description-length-to-identify-the-k-in-k-medoids/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Applying many popular clustering models, for example <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-Means</a>, <a href="https://en.wikipedia.org/wiki/K-medoids">K-Medoids</a> and <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture">Gaussian Mixtures</a>, requires an up-front choice of the number of clusters &#8211; the &#8216;K&#8217; in K-Means, as it were.
Anybody who has ever applied these models is familiar with the inconvenient task of guessing what an appropriate value for K might actually be.
As the size and dimensionality of data grows, estimating a good value for K rapidly becomes an exercise in wild guessing and multiple iterations through the free-parameter space of possible K values.</p>

<p>There are some varied approaches in the community for addressing the task of identifying a good number of clusters in a data set.  In this post I want to focus on an approach that I think deserves more attention than it gets: <a href="https://en.wikipedia.org/wiki/Minimum_description_length">Minimum Description Length</a>.</p>

<p>Many years ago I ran across a <a href="#cite1">superb paper</a> by Stephen J. Roberts on anomaly detection that described a method for <em>automatically</em> choosing a good value for the number of clusters based on the principle of Minimum Description Length.
Minimum Description Length (MDL) is an elegant framework for evaluating the parsimony of a model.
The Description Length of a model is defined as the amount of information needed to encode that model, plus the encoding-length of some data, <em>given</em> that model.
Therefore, in an MDL framework, a good model is one that allows an efficient (i.e. short) encoding of the data, but whose <em>own</em> description is <em>also</em> efficient
(This suggests connections between MDL and the idea of <a href="https://en.wikipedia.org/wiki/Data_compression#Machine_learning">learning as a form of data compression</a>).</p>

<p>For example, a model that directly memorizes all the data may allow for a very short description of the data, but the model itself will cleary require at least the size of the raw data to encode, and so direct memorization models generaly stack up poorly with respect to MDL.
On the other hand, consider a model of some Gaussian data.  We can describe these data in a length proportional to their log-likelihood under the Gaussian density.  Furthermore, the description length of the Gaussian model itself is very short; just the encoding of its mean and standard deviation.  And so in this case a Gaussian distribution represents an efficient model with respect to MDL.</p>

<p><strong>In summary, an MDL framework allows us to mathematically capture the idea that we only wish to consider increasing the complexity of our models if that buys us a corresponding increase in descriptive power on our data.</strong></p>

<p>In the case of <a href="#cite1">Roberts&#8217; paper</a>, the clustering model in question is a <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture">Gaussian Mixture Model</a> (GMM), and the description length expression to be optimized can be written as:</p>

<p><img src="/assets/images/xmedoids/mdl_gm_eq.png" alt="EQ-1" /></p>

<p>In this expression, X represents the vector of data elements.
The first term is the (negative) log-likelihood of the data, with respect to a candidate GMM having some number (K) of Gaussians; p(x) is the GMM density at point (x).
This term represents the cost of encoding the data, given that GMM.
The second term is the cost of encoding the GMM itself.
The value P is the number of free parameters needed to describe that GMM.
Assuming a dimensionality D for the data, then <nobr>P = K(D + D(D+1)/2):</nobr> D values for each mean vector, and <nobr>D(D+1)/2</nobr> values for each covariance matrix.</p>

<p>I wanted to apply this same MDL principle to identifying a good value for K, in the case of a <a href="https://en.wikipedia.org/wiki/K-medoids">K-Medoids</a> model.
How best to adapt MDL to K-Medoids poses some problems.
In the case of K-Medoids, the <em>only</em> structure given to the data is a distance metric.
There is no vector algebra defined on data elements, much less any ability to model the points as a Gaussian Mixture.</p>

<p>However, any candidate clustering of my data <em>does</em> give me a corresponding distribution of distances from each data element to it&#8217;s closest medoid.
I can evaluate an MDL measure on these distance values.
If adding more clusters (i.e. increasing K) does not sufficiently tighten this distribution, then its description length will start to increase at larger values of K, thus indicating that more clusters are not improving our model of the data.
Expressing this idea as an MDL formulation produces the following description length formula:</p>

<p><img src="/assets/images/xmedoids/mdl_km_eq.png" alt="EQ-2" /></p>

<p>Note that the first two terms are similar to the equation above; however, the underlying distribution <nobr>p(||x-c<sub>x</sub>||)</nobr> is now a distribution over the distances of each data element (x) to its closest medoid <nobr>c<sub>x</sub></nobr>, and P is the corresponding number of free parameters for this distribution (more on this below).
There is now an additional third term, representing the cost of encoding the K medoids.
Each medoid is a data element, and specifying each data element requires log|X| bits (or <a href="http://mathworld.wolfram.com/Nat.html">nats</a>, since I generally use natural logarithms), yielding an additional <nobr>(K)log|X|</nobr> in description length cost.</p>

<p>And so, an MDL-based algorithm for automatically identifying a good number of clusters (K) in a K-Medoids model is to run a K-Medoids clustering on my data, for some set of potential K values, and evaluate the MDL measure above for each, and choose the model whose description length L(X) is the smallest!</p>

<p>As I mentioned above, there is also an implied task of choosing a form (or a set of forms) for the distance distribution <nobr>p(||x-c<sub>x</sub>||)</nobr>.
At the time of this writing, I am fitting a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> to the distance data, and <a href="https://github.com/erikerlandson/silex/blob/blog/xmedoids/src/main/scala/com/redhat/et/silex/cluster/KMedoids.scala#L578">using this gamma distribution</a> to compute log-likelihood values.
A gamma distribution has two free parameters &#8211; a shape parameter and a location parameter &#8211; and so currently the value of P is always 2 in my implementations.
I elaborated on some back-story about how I arrived at the decision to use a gamma distribution <a href="http://erikerlandson.github.io/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution/">here</a> and <a href="http://erikerlandson.github.io/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/">here</a>.
An additional reason for my choice is that the gamma distribution does have a fairly good shape coverage, including two-tailed, single-tailed, and/or exponential-like shapes.</p>

<p>Another observation (based on my blog posts mentioned above) is that my use of the gamma distribution implies a bias toward cluster distributions that behave (more or less) like Gaussian clusters, and so in this respect its current behavior is probably somewhat analogous to the <a href="#cite2">G-Means algorithm</a>, which identifies clusterings that yield Gaussian disributions in each cluster.
Adding other candidates for distance distributions is a useful subject for future work, since there is no compelling reason to either favor or assume Gaussian-like cluster distributions over <em>all</em> kinds of metric spaces.
That said, I am seeing reasonable results even on data with clusters that I suspect are not well modeled as Gaussian distributions.
Perhaps the shape-coverage of the gamma distribution is helping to add some robustness.</p>

<p>To demonstrate the MDL-enhanced K-Medoids in action, I will illustrate its performance on some data sets that are amenable to graphic representation.  The code I used to generate these results is <a href="https://github.com/erikerlandson/silex/blob/blog/xmedoids/src/main/scala/com/redhat/et/silex/cluster/KMedoids.scala#L629">here</a>.</p>

<p>Consider this synthetic data set of points in 2D space.  You can see that I&#8217;ve generated the data to have two latent clusters:</p>

<p><img src="/assets/images/xmedoids/k2_raw.png" alt="K2-Raw" /></p>

<p>I collected the description-length values for candidate K-Medoids models having 1 up to 10 clusters, and plotted them.  This plot shows that the clustering with minimal description length had 2 clusters:</p>

<p><img src="/assets/images/xmedoids/k2_mdl.png" alt="K2-MDL" /></p>

<p>When I plot that optimal clustering at K=2 (with cluster medoids marked in black-and-yellow), the clustering looks good:</p>

<p><img src="/assets/images/xmedoids/k2_clusters.png" alt="K2-Clusters" /></p>

<p>To show the behavior for a different optimal value, the following plots demonstrate the MDL K-Medoids results on data where the number of latent clusters is 4:</p>

<p><img src="/assets/images/xmedoids/k4_raw.png" alt="K4-Raw" />
<img src="/assets/images/xmedoids/k4_mdl.png" alt="K4-MDL" />
<img src="/assets/images/xmedoids/k4_clusters.png" alt="K4-Clusters" /></p>

<p>A final comment on Minimum Description Length approaches to clustering &#8211; although I focused on K-Medoids models in this post, the basic approach (and I suspect even the same description length formulation) would apply equally well to K-Means, and possibly other clustering models.
Any clustering model that involves a distance function from elements to some kind of cluster center should be a good candidate.
I intend to keep an eye out for applications of MDL to <em>other</em> learning models, as well.</p>

<h5>References</h5>

<p><a name="cite1"</a>
[1] <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1338&amp;rep=rep1&amp;type=pdf">&#8220;Novelty Detection Using Extreme Value Statistics&#8221;</a>; Stephen J. Roberts; Feb 23, 1999
<a name="cite2"</a>
[2] <a href="http://papers.nips.cc/paper/2526-learning-the-k-in-k-means.pdf">&#8220;Learning the k in k-means. Advances in neural information processing systems&#8221;</a>; Hamerly, G., &amp; Elkan, C.; 2004</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution/">Approximating a PDF of Distances With a Gamma Distribution</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-07-09T11:25:00-07:00" pubdate data-updated="true">Jul 9<span>th</span>, 2016</time>
        
        
         | <a href="/blog/2016/07/09/approximating-a-pdf-of-distances-with-a-gamma-distribution/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In a <a href="http://erikerlandson.github.io/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/">previous post</a> I discussed some unintuitive aspects of the distribution of distances as spatial dimension changes.  To help explain this to myself I derived a formula for this distribution, assuming a unit multivariate Gaussian.  For distance (aka radius) r, and spatial dimension d, the PDF of distances is:</p>

<p><img src="/assets/images/dist_dist/gwwv5a5.png" alt="Figure 1" /></p>

<p>Recall that the form of this PDF is the <a href="https://en.wikipedia.org/wiki/Generalized_gamma_distribution">generalized gamma distribution</a>, with scale parameter <nobr>a=sqrt(2),</nobr> shape parameter p=2, and free shape parameter (d) representing the dimensionality.</p>

<p>I was interested in fitting parameters to such a distribution, using some distance data from a clustering algorithm.  <a href="https://www.scipy.org/">SciPy</a> comes with a predefined method for fitting generalized gamma parameters, however I wished to implement something similar using <a href="http://commons.apache.org/proper/commons-math/">Apache Commons Math</a>, which does not have native support for fitting a generalized gamma PDF.  I even went so far as to start working out <a href="http://erikerlandson.github.io/blog/2016/06/15/computing-derivatives-of-the-gamma-function/">some of the math</a> needed to augment the Commons Math <a href="http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/differentiation/package-summary.html">Automatic Differentiation libraries</a> with Gamma function differentiation needed to numerically fit my parameters.</p>

<p>Meanwhile, I have been fitting a <em>non generalized</em> <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> to the distance data, as a sort of rough cut, using a fast <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Maximum_likelihood_estimation">non-iterative approximation</a> to the parameter optimization.  Consistent with my habit of asking the obvious question last, I tried plotting this gamma approximation against distance data, to see how well it compared against the PDF that I derived.</p>

<p>Surprisingly (at least to me), my approximation using the gamma distribution is a very effective fit for spatial dimensionalities <nobr> >= 2 </nobr>:</p>

<p><img src="/assets/images/gamma_approx/approx_plot.png" alt="Figure 2" /></p>

<p>As the plot shows, only for the 1-dimension case is the gamma approximation substiantially deviating.  In fact, the fit appears to get better as dimensionality increases.  To address the 1D case, I can easily test the fit of a half-gaussian as a possible model.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/06/15/computing-derivatives-of-the-gamma-function/">Computing Derivatives of the Gamma Function</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-06-15T16:37:00-07:00" pubdate data-updated="true">Jun 15<span>th</span>, 2016</time>
        
        
         | <a href="/blog/2016/06/15/computing-derivatives-of-the-gamma-function/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In this post I&#8217;ll describe a simple algorithm to compute the kth derivatives of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a>.</p>

<p>I&#8217;ll start by showing a simple recursion relation for these derivatives, and then gives its derivation.  The kth derivative of Gamma(x) can be computed as follows:</p>

<p><img src="/assets/images/dgamma/hvqtl52.png" alt="Equation 1" /></p>

<p>The recursive formula for the D<sub>k</sub> functions has an easy inductive proof:</p>

<p><img src="/assets/images/dgamma/h79ued9.png" alt="Equation 2" /></p>

<p>Computing the next value D<sub>k</sub> requires knowledge of D<sub>k-1</sub> but also derivative D&#8217;<sub>k-1</sub>.  If we start expanding terms, we see the following:</p>

<p><img src="/assets/images/dgamma/hhvonpa.png" alt="Equation 3" /></p>

<p>Continuing the process above it is not hard to see that we can continue expanding until we are left only with terms of <nobr>D<sub>1</sub><sup>(*)</sup>(x);</nobr> that is, various derivatives of <nobr>D<sub>1</sub>(x)</nobr>.  Furthermore, each layer of substitutions adds an order to the derivatives, so that we will eventually be left with terms involving the derivatives of <nobr>D<sub>1</sub>(x)</nobr> up to the (k-1)th derivative. Note that these will all be successive orders of the <a href="https://en.wikipedia.org/wiki/Polygamma_function">polygamma function</a>.</p>

<p>What we want, to do these computations systematically, is a formula for computing the nth derivative of a term <nobr>D<sub>k</sub>(x)</nobr>.  Examining the first few such derivatives suggests a pattern:</p>

<p><img src="/assets/images/dgamma/jqwqpzy.png" alt="Equation 4" /></p>

<p>Generalizing from the above, we see that the formula for the nth derivative is:</p>

<p><img src="/assets/images/dgamma/jamccnh.png" alt="Equation 5" /></p>

<p>We are now in a position to fill in the triangular table of values, culminating in the value of <nobr>D<sub>k</sub>(x):</nobr></p>

<p><img src="/assets/images/dgamma/jj9ph5l.png" alt="Equation 6" /></p>

<p>As previously mentioned, the basis row of values <nobr>D<sub>1</sub><sup>(*)</sup>(x)</nobr> are the <a href="https://en.wikipedia.org/wiki/Polygamma_function">polygamma functions</a> where <nobr>D<sub>1</sub><sup>(n)</sup>(x) = polygamma<sup>(n)</sup>(x)</nobr>.  The first two polygammas, order 0 and 1, are simply the digamma and trigamma functions, respectively, and are available with most numeric libraries.  Computing the general polygamma is a project, and blog post, for another time, but the standard polynomial approximation for the digamma function can of course be differentiated&#8230;  Happy Computing!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/">Exploring the Effects of Dimensionality on a PDF of Distances</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-06-08T20:56:00-07:00" pubdate data-updated="true">Jun 8<span>th</span>, 2016</time>
        
        
         | <a href="/blog/2016/06/08/exploring-the-effects-of-dimensionality-on-a-pdf-of-distances/#feedback">Feedback</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Every so often I&#8217;m reminded that the effects of changing dimensionality on objects and processes can be surprisingly counterintuitive.  Recently I ran across a great example of this, while I working on a model for the distribution of distances in spaces of varying dimension.</p>

<p>Suppose that I draw some values from a classic one-dimensional Gaussian, with zero mean and unit variance, but that I am actually interested in their corresponding distances from center.  Knowing that my Gaussian is centered on the origin, I can rephrase that as: the distribution of magnitudes of values drawn from that Gaussian.  I can simulate this process by actually samping Gaussian values and taking their absolute value.  When I do, I get the following result:</p>

<p><img src="/assets/images/dist_dist/figure1.png" alt="Figure 1" /></p>

<p>It&#8217;s easy to see &#8211; and intuitive &#8211; that the resulting distribution is a <a href="https://en.wikipedia.org/wiki/Half-normal_distribution">half-Gaussian</a>, as I confirmed by overlaying the histogrammed samples above with a half-Gaussian PDF (displayed in green).</p>

<p>I wanted to generalize this basic idea into some arbitrary dimensionality, (d), where I draw d-vectors from an <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">d-dimensional Gaussian</a> (again, centered on the origin with unit variances). When I take the magnitudes of these sampled d-vectors, what will the probability distribution of <em>their</em> magnitudes look like?</p>

<p>My intuitive assumption was that these magnitudes would <em>also</em> follow a half-Gaussian distribution.  After all, every multivariate Gaussian is densest at its mean, just like the univariate case I examined above.  In fact I was so confident in this assumption that I built my initial modeling around it.  Great confusion ensued, when I saw how poorly my models were working on my higher-dimensional data!</p>

<p>Eventually it occurred to me to do the obvious thing and generate some visualizations from higher dimensional data.  For example, here is the correponding plot generated from a bivariate Gaussian (d=2):</p>

<p><a name="figure2"></a>
<img src="/assets/images/dist_dist/figure2.png" alt="Figure 2" /></p>

<p>Surprise &#8211; the distribution at d=2 is <em>not even close to half-Gaussian!</em>.  My intuitions couldn&#8217;t have been more misleading!</p>

<p>Where did I go wrong?</p>

<p>I&#8217;ll start by observing what happens when I take a multi-dimensional PDF of vectors in (d) dimensions and project it down to a one-dimensional PDF of the corresponding vector magnitudes. To keep things simple, I will be assuming a multi-dimensional PDF <nobr>f<sub>r</sub>(<strong>x</strong><sub>d</sub>)</nobr> that is (1) centered on the origin, and (2) is radially symmetric; the pdf value is the same for all points at a given distance from the origin.  For example, any multivariate Gaussian with <strong>0</strong><sub>d</sub> mean and <strong>I</strong><sub>d</sub> for a covariance matrix satisfies these two assumptions.  With this in mind, you can see that the process of projecting from vectors in <strong>R</strong><sub>d</sub> to their distance from <strong>0</strong><sub>d</sub> (their magnitude) is equivalent to summing all densities <nobr>f<sub>r</sub>(<strong>x</strong><sub>d</sub>)</nobr> along the surface of &#8220;d-sphere&#8221; radius (r) to obtain a pdf f(r) in distance space.  With assumption (2) we can simplify that integration to just <nobr>f(r)=A<sub>d</sub>(r)f&#8217;(r)</nobr>, where f&#8217;(r) defines the value of <nobr>f<sub>r</sub>(<strong>x</strong>)</nobr> for all <strong>x</strong> with magnitude of (r), and A<sub>d</sub>(r) is the surface area of a d-sphere with radius (r):</p>

<p><img src="/assets/images/dist_dist/ztrlusa.png" alt="Figure 3" /></p>

<p>The key observation is that this term is a <em>polynomial</em> function of radius (r), with degree (d-1).  When d=1, it is simply a constant multiplier and so we get the half-Gaussian distribution we expect, but when <nobr>d >= 2</nobr>, the term is zero at r=0, and grows with radius.  Hence we see the in the <a href="#figure2">d=2 plot above</a> that the density begins at zero, then grows with radius until the decreasing gaussian density gradually drives it back toward zero again.</p>

<p>The above ideas can be expressed compactly as follows:</p>

<p><img src="/assets/images/dist_dist/jukgy85.png" alt="Figure 4" /></p>

<p>In my experiments, I am using multivariate Gaussians of mean <strong>0</strong><sub>d</sub> and unit covariance matrix <strong>I</strong><sub>d</sub>, and so the form for f(r;d) becomes:</p>

<p><img src="/assets/images/dist_dist/gwwv5a5.png" alt="Figure 4" /></p>

<p>This form is in fact the <a href="https://en.wikipedia.org/wiki/Generalized_gamma_distribution">generalized gamma distribution</a>, with scale parameter <nobr>a=2<sup>1/2</sup>,</nobr> shape parameter p=2, and free shape parameter (d) representing the dimensionality in this context.</p>

<p>I can verify that this PDF is correct by plotting it against randomly sampled data at differing dimensions:</p>

<p><img src="/assets/images/dist_dist/figure3.png" alt="Figure 5" /></p>

<p>This plot demonstrates both that the PDF expression is correct for varying dimensionalities and also illustrates how the shape of the PDF evolves as dimensionality changes.  For me, it was a great example of challenging my intuitions and learning something completely unexpected about the interplay of distances and dimension.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About The Author</h1>
  <p>Erik is a software engineer at <a href="http://www.redhat.com">Red Hat</a> where he contributes to the <a href="https://radanalytics.io/">radanalytics.io</a> upstream community.
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/05/28/computing-smooth-max-and-its-gradients-without-over-and-underflow/">Computing Smooth Max and its Gradients Without Over- and Underflow</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/05/27/the-gradient-and-hessian-of-the-smooth-max-over-functions/">The Gradient and Hessian of the Smooth Max Over Functions</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/08/23/generalizing-the-concept-of-release-versioning/">Rethinking the Concept of Release Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/19/converging-monoid-addition-for-t-digest/">Converging Monoid Addition for T-Digest</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/05/expressing-map-reduce-as-a-left-folding-monoid/">Encoding Map-Reduce As A Monoid With Left Folding</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/erikerlandson">@erikerlandson</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'erikerlandson',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("manyangled", 4, true);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/manyangled" class="twitter-follow-button" data-show-count="false">Follow @manyangled</a>
  
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Erik Erlandson -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
